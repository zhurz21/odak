{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting started \u00b6 Odak (pronounced \"O-dac\") is the fundamental library for scientific computing in optical sciences, computer graphics and visual perception. We designed this page to help first time users, new contributors, and existing users to get an idea about where to go within this documentation when they need help about certain aspects of Odak. New Users \u00b6 What is Odak? Installation Use cases \u00b6 Computer-generated holography Computer graphics Computational imaging General toolkit Optical design Optimization Visual perception New contributors \u00b6 Contributing to Odak Additional information \u00b6 Citing Odak in a scientific publication License of Odak Reporting bugs or requesting a feature","title":"Getting started"},{"location":"#getting-started","text":"Odak (pronounced \"O-dac\") is the fundamental library for scientific computing in optical sciences, computer graphics and visual perception. We designed this page to help first time users, new contributors, and existing users to get an idea about where to go within this documentation when they need help about certain aspects of Odak.","title":"Getting started"},{"location":"#new-users","text":"What is Odak? Installation","title":"New Users"},{"location":"#use-cases","text":"Computer-generated holography Computer graphics Computational imaging General toolkit Optical design Optimization Visual perception","title":"Use cases"},{"location":"#new-contributors","text":"Contributing to Odak","title":"New contributors"},{"location":"#additional-information","text":"Citing Odak in a scientific publication License of Odak Reporting bugs or requesting a feature","title":"Additional information"},{"location":"cgh/","text":"Computer-Generated Holography \u00b6 Odak contains essential ingredients for research and development targeting Computer-Generated Holography. We consult the beginners in this matter to Goodman's Introduction to Fourier Optics book (ISBN-13: 978-0974707723) and Principles of optics: electromagnetic theory of propagation, interference and diffraction of light from Max Born and Emil Wolf (ISBN 0-08-26482-4). In the rest of this document, you will find engineering notes and relevant functions in Odak that helps you describing complex nature of light on a computer. Note that, the creators of this documentation are from Computational Displays domain, however the provided submodules can potentially aid other lines of research as well, such as Computational Imaging or Computational Microscopy . Engineering notes \u00b6 Note Description Holographic light transport This engineering note will give you an idea about how coherent light propagates in free space. Optimizing phase-only single plane holograms using Odak This engineering note will give you an idea about how to calculate phase-only holograms using Odak. Learning the model of a holographic display This link navigates to a project website that provides a codebase that can learn the model of a holographic display using a single complex kernel. odak.learn.wave \u00b6 This submodule is based on torch , therefore the functions provided here are differentiable and can be used with provided optimizers in torch . Function Description odak.learn.wave.adjust_phase_only_slm_range Adjust the range of a spatial light modulator. odak.learn.wave.band_limited_angular_spectrum Optical beam propagation with bandlimited angular spectrum. odak.learn.wave.calculate_amplitude Calculate amplitude of a complex field. odak.learn.wave.calculate_phase Calculate phase of a complex field. odak.learn.wave.custom Optical beam propagation with a custom complex kernel. odak.learn.wave.generate_complex_field Generate a complex field from an amplitude and a phase value. odak.learn.wave.gerchberg_saxton Phase-only hologram optimization using Gerchberg-Saxton algorithm. odak.learn.wave.impulse_response_fresnel Optical beam propagation with impulse response fresnel. odak.learn.wave.linear_grating One or two axis linear grating. odak.learn.wave.point_wise Phase-only hologram optimization using point wise integration method. odak.learn.wave.prism_phase_function Prism phase function, a prism. odak.learn.wave.produce_phase_only_slm_pattern Produce phase-only hologram for a spatial light modulator with given phase range. odak.learn.wave.propagate_beam General function for optical beam propagation. odak.learn.wave.quadratic_phase_function Quadratic phase function, a lens. odak.learn.wave.set_amplitude Set amplitude of a complex field. odak.learn.wave.stochastic_gradient_descent Phase-only hologram optimization using Stochastic Gradient Descent optimization. odak.learn.wave.transfer_function_kernel Optical beam propagation with transfer function kernel. odak.learn.wave.wavenumber Wave number. odak.wave \u00b6 This submodule is based on Numpy . This submodule existed much before odak.learn.wave . Though this submodule contains more functions than odak.learn.wave , it does not provide the flexibility in optimization provided by differentiation support of torch . Function Description odak.wave.adaptive_sampling_angular_spectrum Propagate coherent beams with adaptive sampling angular spectrum method. odak.wave.angular_spectrum Optical beam propagation with angular spectrum. odak.wave.add_phase Add a given phase to a given complex field. odak.wave.add_random_phase Add a random phase to a given complex field. odak.wave.adjust_phase_only_slm_range Adjust the range of a spatial light modulator. odak.wave.band_extended_angular_spectrum Optical beam propagation with band extended angular spectrum. odak.wave.band_limited_angular_spectrum Optical beam propagation with bandlimited angular spectrum. odak.wave.calculate_amplitude Calculate amplitude of a complex field. odak.wave.calculate_intensity Calculate intensity of a complex field. odak.wave.calculate_phase Calculate phase of a complex field. odak.wave.double_convergence Provides an initial phase for beam shaping. odak.wave.electric_field_per_plane_wave Return the state of a plane wave at a particular distance and time. odak.wave.fraunhofer_equal_size_adjust Match physical size of field with simulated. odak.wave.fraunhofer_inverse Adjoint model for Fraunhofer beam propagation. odak.wave.generate_complex_field Generate a complex field from an amplitude and a phase value. odak.wave.gerchberg_saxton Phase-only hologram optimization using Gerchberg-Saxton algorithm. odak.wave.gerchberg_saxton_3d Phase-only hologram optimization using Gerchberg-Saxton algorithm for three-dimensional reconstructions. odak.wave.impulse_response_fresnel Optical beam propagation with impulse response fresnel. odak.wave.linear_grating One or two axis linear grating. odak.wave.prism_phase_function Prism phase function, a prism. odak.wave.produce_phase_only_slm_pattern Produce phase-only hologram for a spatial light modulator with given phase range. odak.wave.propagate_beam General function for optical beam propagation. odak.wave.propagate_field Propagate a given array of spherical sources to given set of points in space. odak.wave.propagate_plane_waves Propagate a given plane wave in space and time. odak.wave.quadratic_phase_function Quadratic phase function, a lens. odak.wave.rayleigh_resolution Calculate Rayleigh resolution limit. odak.wave.rayleigh_sommerfeld Optical beam propagation with Rayleigh-Sommerfeld integral. odak.wave.rotationspeed Calculate rotation speed of a wave. odak.wave.set_amplitude Set amplitude of a complex field. odak.wave.transfer_function_kernel Optical beam propagation with transfer function kernel. odak.wave.wavenumber Wave number.","title":"Computer-Generated Holography"},{"location":"cgh/#computer-generated-holography","text":"Odak contains essential ingredients for research and development targeting Computer-Generated Holography. We consult the beginners in this matter to Goodman's Introduction to Fourier Optics book (ISBN-13: 978-0974707723) and Principles of optics: electromagnetic theory of propagation, interference and diffraction of light from Max Born and Emil Wolf (ISBN 0-08-26482-4). In the rest of this document, you will find engineering notes and relevant functions in Odak that helps you describing complex nature of light on a computer. Note that, the creators of this documentation are from Computational Displays domain, however the provided submodules can potentially aid other lines of research as well, such as Computational Imaging or Computational Microscopy .","title":"Computer-Generated Holography"},{"location":"cgh/#engineering-notes","text":"Note Description Holographic light transport This engineering note will give you an idea about how coherent light propagates in free space. Optimizing phase-only single plane holograms using Odak This engineering note will give you an idea about how to calculate phase-only holograms using Odak. Learning the model of a holographic display This link navigates to a project website that provides a codebase that can learn the model of a holographic display using a single complex kernel.","title":"Engineering notes"},{"location":"cgh/#odaklearnwave","text":"This submodule is based on torch , therefore the functions provided here are differentiable and can be used with provided optimizers in torch . Function Description odak.learn.wave.adjust_phase_only_slm_range Adjust the range of a spatial light modulator. odak.learn.wave.band_limited_angular_spectrum Optical beam propagation with bandlimited angular spectrum. odak.learn.wave.calculate_amplitude Calculate amplitude of a complex field. odak.learn.wave.calculate_phase Calculate phase of a complex field. odak.learn.wave.custom Optical beam propagation with a custom complex kernel. odak.learn.wave.generate_complex_field Generate a complex field from an amplitude and a phase value. odak.learn.wave.gerchberg_saxton Phase-only hologram optimization using Gerchberg-Saxton algorithm. odak.learn.wave.impulse_response_fresnel Optical beam propagation with impulse response fresnel. odak.learn.wave.linear_grating One or two axis linear grating. odak.learn.wave.point_wise Phase-only hologram optimization using point wise integration method. odak.learn.wave.prism_phase_function Prism phase function, a prism. odak.learn.wave.produce_phase_only_slm_pattern Produce phase-only hologram for a spatial light modulator with given phase range. odak.learn.wave.propagate_beam General function for optical beam propagation. odak.learn.wave.quadratic_phase_function Quadratic phase function, a lens. odak.learn.wave.set_amplitude Set amplitude of a complex field. odak.learn.wave.stochastic_gradient_descent Phase-only hologram optimization using Stochastic Gradient Descent optimization. odak.learn.wave.transfer_function_kernel Optical beam propagation with transfer function kernel. odak.learn.wave.wavenumber Wave number.","title":"odak.learn.wave"},{"location":"cgh/#odakwave","text":"This submodule is based on Numpy . This submodule existed much before odak.learn.wave . Though this submodule contains more functions than odak.learn.wave , it does not provide the flexibility in optimization provided by differentiation support of torch . Function Description odak.wave.adaptive_sampling_angular_spectrum Propagate coherent beams with adaptive sampling angular spectrum method. odak.wave.angular_spectrum Optical beam propagation with angular spectrum. odak.wave.add_phase Add a given phase to a given complex field. odak.wave.add_random_phase Add a random phase to a given complex field. odak.wave.adjust_phase_only_slm_range Adjust the range of a spatial light modulator. odak.wave.band_extended_angular_spectrum Optical beam propagation with band extended angular spectrum. odak.wave.band_limited_angular_spectrum Optical beam propagation with bandlimited angular spectrum. odak.wave.calculate_amplitude Calculate amplitude of a complex field. odak.wave.calculate_intensity Calculate intensity of a complex field. odak.wave.calculate_phase Calculate phase of a complex field. odak.wave.double_convergence Provides an initial phase for beam shaping. odak.wave.electric_field_per_plane_wave Return the state of a plane wave at a particular distance and time. odak.wave.fraunhofer_equal_size_adjust Match physical size of field with simulated. odak.wave.fraunhofer_inverse Adjoint model for Fraunhofer beam propagation. odak.wave.generate_complex_field Generate a complex field from an amplitude and a phase value. odak.wave.gerchberg_saxton Phase-only hologram optimization using Gerchberg-Saxton algorithm. odak.wave.gerchberg_saxton_3d Phase-only hologram optimization using Gerchberg-Saxton algorithm for three-dimensional reconstructions. odak.wave.impulse_response_fresnel Optical beam propagation with impulse response fresnel. odak.wave.linear_grating One or two axis linear grating. odak.wave.prism_phase_function Prism phase function, a prism. odak.wave.produce_phase_only_slm_pattern Produce phase-only hologram for a spatial light modulator with given phase range. odak.wave.propagate_beam General function for optical beam propagation. odak.wave.propagate_field Propagate a given array of spherical sources to given set of points in space. odak.wave.propagate_plane_waves Propagate a given plane wave in space and time. odak.wave.quadratic_phase_function Quadratic phase function, a lens. odak.wave.rayleigh_resolution Calculate Rayleigh resolution limit. odak.wave.rayleigh_sommerfeld Optical beam propagation with Rayleigh-Sommerfeld integral. odak.wave.rotationspeed Calculate rotation speed of a wave. odak.wave.set_amplitude Set amplitude of a complex field. odak.wave.transfer_function_kernel Optical beam propagation with transfer function kernel. odak.wave.wavenumber Wave number.","title":"odak.wave"},{"location":"contributing/","text":"Contributing to Odak \u00b6 Odak is in constant development. We shape Odak according to the most current needs in our scientific research. We welcome both users and developers in the open-source community as long as they have good intentions (e.g., scientific research). For the most recent description of Odak, please consult our description . If you are planning to use Odak for industrial purposes, please reach out to Kaan Ak\u015fit . All of the Odak contributors are listed in our THANKS.txt and added to CITATION.cff regardless of how much they contribute to the project. Their names are also included in our Digital Object Identifier (DOI) page . Contributing process \u00b6 Contributions to Odak can come in different forms. It can either be code or documentation related contributions. Historically, Odak has evolved through scientific collaboration, in which authors of Odak identified a collaborative project with a new potential contributor. You can always reach out to Kaan Ak\u015fit to query your idea for potential collaborations in the future. Another potential place to identify likely means to improve odak is to address outstanding issues of Odak . Code \u00b6 Odak's odak directory contains the source code. To add to it, please make sure that you can install and test Odak on your local computer. The installation documentation contains routines for installation and testing, please follow that page carefully. We typically work with pull requests . If you want to add new code to Odak, please do not hesitate to fork Odak's git repository and have your modifications on your fork at first. Once you test the modified version, please do not hesitate to initiate a pull request. We will revise your code, and if found suitable, it will be merged to the master branch. Remember to follow numpy convention while adding documentation to your newly added functions to Odak. Another thing to mention is regarding to the code quality and standard. Although it hasn't been strictly followed since the start of Odak, note that Odak follows code conventions of flake8 , which can be installed using: pip3 install flake8 You can always check for code standard violations in Odak by running these two commands: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics There are tools that can automatically fix code in terms of following standards. One primary tool that we are aware of is autopep8 , which can be installed using: pip3 install autopep8 Please once you are ready to have a pull request, make sure to add a unit test for your additions in test folder, and make sure to test all unit tests by running pytest . If your system do not have pytest installed, it can be installed using: pip3 install pytest Documentation \u00b6 Under Odak's source's root directory, you will find a folder named docs . This directory contains all the necessary information to generate the pages in this documentation. If you are interested in improving the documentation of Odak, this directory is the place where you will be adding things. Odak's documentation is built using mkdocs . At this point, I assume that you have successfully installed Odak on your system. If you haven't yet, please follow installation documentation . To be able to run documentation locally, make sure to have the correct dependencies installed properly: pip3 install plyfile pip3 install plyfile pip3 install Pillow pip3 install tqdm pip3 install mkdocs-material pip3 install mkdocstrings Once you have dependencies appropriately installed, navigate to the source directory of Odak in your hard drive and run a test server: cd odak mkdocs serve If all goes well, you should see a bunch of lines on your terminal, and the final lines should look similar to these: INFO - Documentation built in 4.45 seconds INFO - [22:15:22] Serving on http://127.0.0.1:8000/odak/ INFO - [22:15:23] Browser connected: http://127.0.0.1:8000/odak/ At this point, you can start your favourite browser and navigate to http://127.0.0.1:8000/odak to view documentation locally. This local viewing is essential as it can help you view your changes locally on the spot before actually committing. One last thing to mention here is the fact that Odak's docs folder's structure is self-explanatory. It follows markdown rules, and mkdocsstrings style is numpy .","title":"Contributing"},{"location":"contributing/#contributing-to-odak","text":"Odak is in constant development. We shape Odak according to the most current needs in our scientific research. We welcome both users and developers in the open-source community as long as they have good intentions (e.g., scientific research). For the most recent description of Odak, please consult our description . If you are planning to use Odak for industrial purposes, please reach out to Kaan Ak\u015fit . All of the Odak contributors are listed in our THANKS.txt and added to CITATION.cff regardless of how much they contribute to the project. Their names are also included in our Digital Object Identifier (DOI) page .","title":"Contributing to Odak"},{"location":"contributing/#contributing-process","text":"Contributions to Odak can come in different forms. It can either be code or documentation related contributions. Historically, Odak has evolved through scientific collaboration, in which authors of Odak identified a collaborative project with a new potential contributor. You can always reach out to Kaan Ak\u015fit to query your idea for potential collaborations in the future. Another potential place to identify likely means to improve odak is to address outstanding issues of Odak .","title":"Contributing process"},{"location":"contributing/#code","text":"Odak's odak directory contains the source code. To add to it, please make sure that you can install and test Odak on your local computer. The installation documentation contains routines for installation and testing, please follow that page carefully. We typically work with pull requests . If you want to add new code to Odak, please do not hesitate to fork Odak's git repository and have your modifications on your fork at first. Once you test the modified version, please do not hesitate to initiate a pull request. We will revise your code, and if found suitable, it will be merged to the master branch. Remember to follow numpy convention while adding documentation to your newly added functions to Odak. Another thing to mention is regarding to the code quality and standard. Although it hasn't been strictly followed since the start of Odak, note that Odak follows code conventions of flake8 , which can be installed using: pip3 install flake8 You can always check for code standard violations in Odak by running these two commands: flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics There are tools that can automatically fix code in terms of following standards. One primary tool that we are aware of is autopep8 , which can be installed using: pip3 install autopep8 Please once you are ready to have a pull request, make sure to add a unit test for your additions in test folder, and make sure to test all unit tests by running pytest . If your system do not have pytest installed, it can be installed using: pip3 install pytest","title":"Code"},{"location":"contributing/#documentation","text":"Under Odak's source's root directory, you will find a folder named docs . This directory contains all the necessary information to generate the pages in this documentation. If you are interested in improving the documentation of Odak, this directory is the place where you will be adding things. Odak's documentation is built using mkdocs . At this point, I assume that you have successfully installed Odak on your system. If you haven't yet, please follow installation documentation . To be able to run documentation locally, make sure to have the correct dependencies installed properly: pip3 install plyfile pip3 install plyfile pip3 install Pillow pip3 install tqdm pip3 install mkdocs-material pip3 install mkdocstrings Once you have dependencies appropriately installed, navigate to the source directory of Odak in your hard drive and run a test server: cd odak mkdocs serve If all goes well, you should see a bunch of lines on your terminal, and the final lines should look similar to these: INFO - Documentation built in 4.45 seconds INFO - [22:15:22] Serving on http://127.0.0.1:8000/odak/ INFO - [22:15:23] Browser connected: http://127.0.0.1:8000/odak/ At this point, you can start your favourite browser and navigate to http://127.0.0.1:8000/odak to view documentation locally. This local viewing is essential as it can help you view your changes locally on the spot before actually committing. One last thing to mention here is the fact that Odak's docs folder's structure is self-explanatory. It follows markdown rules, and mkdocsstrings style is numpy .","title":"Documentation"},{"location":"perception/","text":"Visual perception \u00b6 The perception module of odak focuses on visual perception, and in particular on gaze-contingent perceptual loss functions. It contains an implementation of a metameric loss function. When used in optimisation tasks, this loss function enforces the optimised image to be a ventral metamer to the ground truth image. This loss function is based on previous work on fast metamer generation . It uses the same statistical model and many of the same acceleration techniques (e.g. MIP map sampling) to enable the metameric loss to run efficiently. Engineering notes \u00b6 Note Description Using metameric loss in Odak This engineering note will give you an idea about how to use the metameric perceptual loss in Odak. odak.learn.perception \u00b6 Function Description odak.learn.perception.MetamericLoss() Metameric loss function odak.learn.perception.MetamericLossUniform() Metameric loss function (uniform, not foveated variant) odak.learn.perception.MetamerMSELoss() Metamer MSE loss function odak.learn.perception.BlurLoss() Blur function odak.learn.perception.RadiallyVaryingBlur() Radially varying blur odak.learn.perception.SpatialSteerablePyramid() Spatial implementation of the real-valued steerable pyramid odak.learn.perception.make_3d_location_map() Foveation method: make a map of 3D locations for each pixel odak.learn.perception.make_eccentricity_distance_maps() Foveation method: make maps of eccentricity and distance to the image plane for each pixel odak.learn.perception.make_pooling_size_map_pixels() Foveation method: make a map of pooling sizes (in pixels) odak.learn.perception.make_pooling_size_map_lod() Foveation method: make a map of pooling sizes (in LoD levels) odak.learn.perception.make_radial_map() Foveation method: make a map of distances from a gaze point in pixels odak.learn.perception.ycrcb_2_rgb() Colorspace conversion from YCrCb to RGB odak.learn.perception.rgb_2_ycrcb() Colorspace conversion from RGB to YCrCb","title":"Visual perception"},{"location":"perception/#visual-perception","text":"The perception module of odak focuses on visual perception, and in particular on gaze-contingent perceptual loss functions. It contains an implementation of a metameric loss function. When used in optimisation tasks, this loss function enforces the optimised image to be a ventral metamer to the ground truth image. This loss function is based on previous work on fast metamer generation . It uses the same statistical model and many of the same acceleration techniques (e.g. MIP map sampling) to enable the metameric loss to run efficiently.","title":"Visual perception"},{"location":"perception/#engineering-notes","text":"Note Description Using metameric loss in Odak This engineering note will give you an idea about how to use the metameric perceptual loss in Odak.","title":"Engineering notes"},{"location":"perception/#odaklearnperception","text":"Function Description odak.learn.perception.MetamericLoss() Metameric loss function odak.learn.perception.MetamericLossUniform() Metameric loss function (uniform, not foveated variant) odak.learn.perception.MetamerMSELoss() Metamer MSE loss function odak.learn.perception.BlurLoss() Blur function odak.learn.perception.RadiallyVaryingBlur() Radially varying blur odak.learn.perception.SpatialSteerablePyramid() Spatial implementation of the real-valued steerable pyramid odak.learn.perception.make_3d_location_map() Foveation method: make a map of 3D locations for each pixel odak.learn.perception.make_eccentricity_distance_maps() Foveation method: make maps of eccentricity and distance to the image plane for each pixel odak.learn.perception.make_pooling_size_map_pixels() Foveation method: make a map of pooling sizes (in pixels) odak.learn.perception.make_pooling_size_map_lod() Foveation method: make a map of pooling sizes (in LoD levels) odak.learn.perception.make_radial_map() Foveation method: make a map of distances from a gaze point in pixels odak.learn.perception.ycrcb_2_rgb() Colorspace conversion from YCrCb to RGB odak.learn.perception.rgb_2_ycrcb() Colorspace conversion from RGB to YCrCb","title":"odak.learn.perception"},{"location":"toolkit/","text":"General toolkit. \u00b6 Odak provides a set of functions that can be used for general purpose work, such as saving an image file or loading a three-dimensional point cloud of an object. These functions are helpful for general use and provide consistency across routine works in loading and saving routines. When working with odak, we strongly suggest sticking to the general toolkit to provide a coherent solution to your task. Engineering notes \u00b6 Note Description Working with images This engineering note will give you an idea about how read and write images using odak. Working with dictionaries This engineering note will give you an idea about how read and write dictionaries using odak. odak.tools \u00b6 This submodule is based on numpy . If you are using functions outside of odak.learn submodule, we recommend you to use this specific set of tools. Note that there are also corner case like loading a dictionary when using odak.learn , which does not necessarily require to work with odak.learn.tools as odak.tools.load_dictionary works with numpy , cupy or torch . In such cases, odak.tools is the go to submodule. Function Description odak.tools.check_directory Check if directory exist, if not create it. odak.tools.convert_bytes Convert bytes to other units (e.g., MB). odak.tools.load_image Load an image. odak.tools.load_dictionary Load a dictionary. odak.tools.list_files List files in a given path. odak.tools.resize_image Resize a given image. odak.tools.save_image Save as an image. odak.tools.save_dictionary Save a dictionary. odak.tools.size_of_a_file Geet size of a file in units (e.g., MB). odak.tools.shell_command Trigger a shell command.","title":"General toolkit"},{"location":"toolkit/#general-toolkit","text":"Odak provides a set of functions that can be used for general purpose work, such as saving an image file or loading a three-dimensional point cloud of an object. These functions are helpful for general use and provide consistency across routine works in loading and saving routines. When working with odak, we strongly suggest sticking to the general toolkit to provide a coherent solution to your task.","title":"General toolkit."},{"location":"toolkit/#engineering-notes","text":"Note Description Working with images This engineering note will give you an idea about how read and write images using odak. Working with dictionaries This engineering note will give you an idea about how read and write dictionaries using odak.","title":"Engineering notes"},{"location":"toolkit/#odaktools","text":"This submodule is based on numpy . If you are using functions outside of odak.learn submodule, we recommend you to use this specific set of tools. Note that there are also corner case like loading a dictionary when using odak.learn , which does not necessarily require to work with odak.learn.tools as odak.tools.load_dictionary works with numpy , cupy or torch . In such cases, odak.tools is the go to submodule. Function Description odak.tools.check_directory Check if directory exist, if not create it. odak.tools.convert_bytes Convert bytes to other units (e.g., MB). odak.tools.load_image Load an image. odak.tools.load_dictionary Load a dictionary. odak.tools.list_files List files in a given path. odak.tools.resize_image Resize a given image. odak.tools.save_image Save as an image. odak.tools.save_dictionary Save a dictionary. odak.tools.size_of_a_file Geet size of a file in units (e.g., MB). odak.tools.shell_command Trigger a shell command.","title":"odak.tools"},{"location":"notes/holographic_light_transport/","text":"Holographic light transport \u00b6 Odak contains essential ingredients for research and development targeting Computer-Generated Holography. We consult the beginners in this matter to Goodman's Introduction to Fourier Optics book (ISBN-13: 978-0974707723) and Principles of optics: electromagnetic theory of propagation, interference and diffraction of light from Max Born and Emil Wolf (ISBN 0-08-26482-4). This engineering note will provide a crash course on how light travels from a phase-only hologram to an image plane. Holographic image reconstruction. A collimated beam with a homogenous amplitude distribution (A=1) illuminates a phase-only hologram \\(u_0(x,y)\\) . Light from this hologram diffracts and arrive at an image plane \\(u(x,y)\\) at a distance of z. Diffracted beams from each hologram pixel interfere at the image plane and, finally, reconstruct a target image. As depicted in above figure, when such holograms are illuminated with a collimated coherent light (e.g. laser), these holograms can reconstruct an intended optical field at target depth levels. How light travels from a hologram to a parallel image plane is commonly described using Rayleigh-Sommerfeld diffraction integrals (For more, consult Heurtley, J. C. (1973). Scalar Rayleigh\u2013Sommerfeld and Kirchhoff diffraction integrals: a comparison of exact evaluations for axial points. JOSA, 63(8), 1003-1008. ). The first solution of the Rayleigh-Sommerfeld integral, also known as the Huygens-Fresnel principle, is expressed as follows: \\(u(x,y)=\\frac{1}{j\\lambda} \\int\\!\\!\\!\\!\\int u_0(x,y)\\frac{e^{jkr}}{r}cos(\\theta)dxdy,\\) where field at a target image plane, \\(u(x,y)\\) , is calculated by integrating over every point of hologram's field, \\(u_0(x,y)\\) . Note that, for the above equation, \\(r\\) represents the optical path between a selected point over a hologram and a selected point in the image plane, theta represents the angle between these two points, k represents the wavenumber ( \\(\\frac{2\\pi}{\\lambda}\\) ) and \\(\\lambda\\) represents the wavelength of light. In this described light transport model, optical fields, \\(u_0(x,y)\\) and \\(u(x,y)\\) , are represented with a complex value, \\(u_0(x,y)=A(x,y)e^{j\\phi(x,y)},\\) where A represents the spatial distribution of amplitude and \\(\\phi\\) represents the spatial distribution of phase across a hologram plane. The described holographic light transport model is often simplified into a single convolution with a fixed spatially invariant complex kernel, \\(h(x,y)\\) ( Sypek, Maciej. \"Light propagation in the Fresnel region. New numerical approach.\" Optics communications 116.1-3 (1995): 43-48. ). \\(u(x,y)=u_0(x,y) * h(x,y) =\\mathcal{F}^{-1}(\\mathcal{F}(u_0(x,y)) \\mathcal{F}(h(x,y)))\\) There are multiple variants of this simplified approach: Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673. , Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics letters 45.6 (2020): 1543-1546. , Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419. In many cases, people choose to use the most common form of h described as \\(h(x,y)=\\frac{e^{jkz}}{j\\lambda z} e^{\\frac{jk}{2z} (x^2+y^2)},\\) where z represents the distance between a hologram plane and a target image plane. Note that beam propagation can also be learned for physical setups to avoid imperfections in a setup and to improve the image quality at an image plane: Peng, Yifan, et al. \"Neural holography with camera-in-the-loop training.\" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-14. , Chakravarthula, Praneeth, et al. \"Learned hardware-in-the-loop phase retrieval for holographic near-eye displays.\" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-18. , Kavakl\u0131, Koray, Hakan Urey, and Kaan Ak\u015fit. \"Learned holographic light transport.\" Applied Optics (2021). . See also \u00b6 For more engineering notes, follow: Computer Generated-Holography","title":"Holographic light transport"},{"location":"notes/holographic_light_transport/#holographic-light-transport","text":"Odak contains essential ingredients for research and development targeting Computer-Generated Holography. We consult the beginners in this matter to Goodman's Introduction to Fourier Optics book (ISBN-13: 978-0974707723) and Principles of optics: electromagnetic theory of propagation, interference and diffraction of light from Max Born and Emil Wolf (ISBN 0-08-26482-4). This engineering note will provide a crash course on how light travels from a phase-only hologram to an image plane. Holographic image reconstruction. A collimated beam with a homogenous amplitude distribution (A=1) illuminates a phase-only hologram \\(u_0(x,y)\\) . Light from this hologram diffracts and arrive at an image plane \\(u(x,y)\\) at a distance of z. Diffracted beams from each hologram pixel interfere at the image plane and, finally, reconstruct a target image. As depicted in above figure, when such holograms are illuminated with a collimated coherent light (e.g. laser), these holograms can reconstruct an intended optical field at target depth levels. How light travels from a hologram to a parallel image plane is commonly described using Rayleigh-Sommerfeld diffraction integrals (For more, consult Heurtley, J. C. (1973). Scalar Rayleigh\u2013Sommerfeld and Kirchhoff diffraction integrals: a comparison of exact evaluations for axial points. JOSA, 63(8), 1003-1008. ). The first solution of the Rayleigh-Sommerfeld integral, also known as the Huygens-Fresnel principle, is expressed as follows: \\(u(x,y)=\\frac{1}{j\\lambda} \\int\\!\\!\\!\\!\\int u_0(x,y)\\frac{e^{jkr}}{r}cos(\\theta)dxdy,\\) where field at a target image plane, \\(u(x,y)\\) , is calculated by integrating over every point of hologram's field, \\(u_0(x,y)\\) . Note that, for the above equation, \\(r\\) represents the optical path between a selected point over a hologram and a selected point in the image plane, theta represents the angle between these two points, k represents the wavenumber ( \\(\\frac{2\\pi}{\\lambda}\\) ) and \\(\\lambda\\) represents the wavelength of light. In this described light transport model, optical fields, \\(u_0(x,y)\\) and \\(u(x,y)\\) , are represented with a complex value, \\(u_0(x,y)=A(x,y)e^{j\\phi(x,y)},\\) where A represents the spatial distribution of amplitude and \\(\\phi\\) represents the spatial distribution of phase across a hologram plane. The described holographic light transport model is often simplified into a single convolution with a fixed spatially invariant complex kernel, \\(h(x,y)\\) ( Sypek, Maciej. \"Light propagation in the Fresnel region. New numerical approach.\" Optics communications 116.1-3 (1995): 43-48. ). \\(u(x,y)=u_0(x,y) * h(x,y) =\\mathcal{F}^{-1}(\\mathcal{F}(u_0(x,y)) \\mathcal{F}(h(x,y)))\\) There are multiple variants of this simplified approach: Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673. , Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics letters 45.6 (2020): 1543-1546. , Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419. In many cases, people choose to use the most common form of h described as \\(h(x,y)=\\frac{e^{jkz}}{j\\lambda z} e^{\\frac{jk}{2z} (x^2+y^2)},\\) where z represents the distance between a hologram plane and a target image plane. Note that beam propagation can also be learned for physical setups to avoid imperfections in a setup and to improve the image quality at an image plane: Peng, Yifan, et al. \"Neural holography with camera-in-the-loop training.\" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-14. , Chakravarthula, Praneeth, et al. \"Learned hardware-in-the-loop phase retrieval for holographic near-eye displays.\" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-18. , Kavakl\u0131, Koray, Hakan Urey, and Kaan Ak\u015fit. \"Learned holographic light transport.\" Applied Optics (2021). .","title":"Holographic light transport"},{"location":"notes/holographic_light_transport/#see-also","text":"For more engineering notes, follow: Computer Generated-Holography","title":"See also"},{"location":"notes/optimizing_holograms_using_odak/","text":"Optimizing holograms using Odak \u00b6 This engineering note will give you an idea about how to optimize phase-only holograms using Odak. We consult the beginners in this matter to Goodman's Introduction to Fourier Optics (ISBN-13: 978-0974707723) and Principles of optics: electromagnetic theory of propagation, interference and diffraction of light from Max Born and Emil Wolf (ISBN 0-08-26482-4). Note that the creators of this documentation are from the Computational Displays domain. However, the provided submodules can potentially aid other lines of research as well, such as Computational Imaging or Computational Microscopy . The optimization that is referred to in this document is the one that generates a phase-only hologram that can reconstruct a target image. There are multiple ways in the literature to optimize a phase-only hologram for a single plane, and these include: Gerchberg-Saxton and Yang-Yu algorithms: - Yang, G. Z., Dong, B. Z., Gu, B. Y., Zhuang, J. Y., & Ersoy, O. K. (1994). Gerchberg\u2013Saxton and Yang\u2013Gu algorithms for phase retrieval in a nonunitary transform system: a comparison. Applied optics, 33(2), 209-218. Stochastic Gradient Descent based optimization: - Chen, Y., Chi, Y., Fan, J., & Ma, C. (2019). Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming, 176(1), 5-37. Odak provides functions to optimize phase-only holograms using Gerchberg-Saxton algorithm or the Stochastic Gradient Descent based approach. The relevant functions here are odak.learn.wave.stochastic_gradient_descent and odak.learn.wave.gerchberg_saxton . We will review both of these definitions in this document. But first, let's get prepared. Preparation \u00b6 We first start with imports, here is all you need: from odak.learn.wave import stochastic_gradient_descent , calculate_amplitude , calculate_phase import torch We will also be needing some variables that defines the wavelength of light that we work with: wavelength = 0.000000532 Pixel pitch and resolution of the phase-only hologram or a phase-only spatial light modulator that we are simulating: dx = 0.0000064 resolution = [ 1080 , 1920 ] Define the distance that the light will travel from optimized hologram. distance = 0.15 We have to set a target image. You can either load a sample image here or paint a white rectangle on a white background like in this example. target = torch . zeros ( resolution [ 0 ], resolution [ 1 ]) target [ 500 : 600 , 400 : 450 ] = 1. Surely, we also have to set the number of iterations and learning rate for our optimizations. If you want the GPU support, you also have to set the cuda as True . Propagation type has to be defined as well. In this example, we will use transfer function Fresnel approach. For more on propagation types, curious readers can consult Computational Fourier Optics David Vuelz (ISBN13:9780819482044). iteration_number = 100 learning_rate = 0.1 cuda = True propagation_type = 'TR Fresnel' This step concludes our preparations. Let's dive into optimizing our phase-only holograms. Depending on your choice, you can either optimize using Gerchberg-Saxton approach or the Stochastic Gradient Descent approach. This document will only show you Stochastic Gradient Descent approach as it is the state of art. However, optimizing a phase-only hologram is as importing: from odak.learn.wave import gerchberg_saxton and almost as easy as replacing stochastic_gradient_descent with gerchberg_saxton in the upcoming described hologram routine. For greater details, consult to documentation of odak.learn.wave.gerchberg_saxton . Stochastic Gradient Descent approach \u00b6 We have prepared a function for you to avoid compiling a differentiable hologram optimizer from scratch. hologram , reconstructed = stochastic_gradient_descent ( target , wavelength , distance , dx , resolution , 'TR Fresnel' , iteration_number , learning_rate = learning_rate , cuda = cuda ) Iteration: 99 loss:0.0003 Congratulations! You have just optimized a phase-only hologram that reconstruct your target image at the target depth. Surely, you want to see what kind of image is being reconstructed with this newly optimized hologram. You can save the outcome to an image file easily. Odak provides tools to save and load images. First, you have to import: from odak.learn.tools import save_image , load_image As you can recall, we have created a target image earlier that is normalized between zero and one. The same is true for our result, reconstructed . Therefore, we have to save it correctly by taking that into account. Note that reconstructed is the complex field generated by our optimized hologram variable. So, we need to save the reconstructed intensity as humans and cameras capture intensity but not a complex field with phase and amplitude. reconstructed_intensity = calculate_amplitude ( reconstructed ) ** 2 save_image ( 'reconstructed_image.png' , reconstructed_intensity , cmin = 0. , cmax = 1. ) True To save our hologram as an image so that we can load it to a spatial light modulator, we have to normalize it between zero and 255 (dynamic range of a typical image on a computer). P.S. Depending on your SLM's calibration and dynamic range things may vary. slm_range = 2 * 3.14 dynamic_range = 255 phase_hologram = calculate_phase ( hologram ) phase_only_hologram = ( phase_hologram % slm_range ) / ( slm_range ) * dynamic_range It is now time for saving our hologram: save_image ( 'phase_only_hologram.png' , phase_only_hologram ) True In some cases, you may want to add a grating term to your hologram as you will display it on a spatial light modulator. There are various reasons for that, but the most obvious is getting rid of zeroth-order reflections that are not modulated by your hologram. In case you need it is as simple as below: from odak.learn.wave import linear_grating grating = linear_grating ( resolution [ 0 ], resolution [ 1 ], axis = 'y' ) . to ( phase_hologram . device ) phase_only_hologram_w_grating = phase_hologram + calculate_phase ( grating ) And let's save what we got from this step: phase_only_hologram_w_grating = ( phase_only_hologram_w_grating % slm_range ) / ( slm_range ) * dynamic_range save_image ( 'phase_only_hologram_w_grating.png' , phase_only_hologram_w_grating ) True See also \u00b6 For more engineering notes, follow: Computer Generated-Holography","title":"Optimizing holograms using Odak"},{"location":"notes/optimizing_holograms_using_odak/#optimizing-holograms-using-odak","text":"This engineering note will give you an idea about how to optimize phase-only holograms using Odak. We consult the beginners in this matter to Goodman's Introduction to Fourier Optics (ISBN-13: 978-0974707723) and Principles of optics: electromagnetic theory of propagation, interference and diffraction of light from Max Born and Emil Wolf (ISBN 0-08-26482-4). Note that the creators of this documentation are from the Computational Displays domain. However, the provided submodules can potentially aid other lines of research as well, such as Computational Imaging or Computational Microscopy . The optimization that is referred to in this document is the one that generates a phase-only hologram that can reconstruct a target image. There are multiple ways in the literature to optimize a phase-only hologram for a single plane, and these include: Gerchberg-Saxton and Yang-Yu algorithms: - Yang, G. Z., Dong, B. Z., Gu, B. Y., Zhuang, J. Y., & Ersoy, O. K. (1994). Gerchberg\u2013Saxton and Yang\u2013Gu algorithms for phase retrieval in a nonunitary transform system: a comparison. Applied optics, 33(2), 209-218. Stochastic Gradient Descent based optimization: - Chen, Y., Chi, Y., Fan, J., & Ma, C. (2019). Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming, 176(1), 5-37. Odak provides functions to optimize phase-only holograms using Gerchberg-Saxton algorithm or the Stochastic Gradient Descent based approach. The relevant functions here are odak.learn.wave.stochastic_gradient_descent and odak.learn.wave.gerchberg_saxton . We will review both of these definitions in this document. But first, let's get prepared.","title":"Optimizing holograms using Odak"},{"location":"notes/optimizing_holograms_using_odak/#preparation","text":"We first start with imports, here is all you need: from odak.learn.wave import stochastic_gradient_descent , calculate_amplitude , calculate_phase import torch We will also be needing some variables that defines the wavelength of light that we work with: wavelength = 0.000000532 Pixel pitch and resolution of the phase-only hologram or a phase-only spatial light modulator that we are simulating: dx = 0.0000064 resolution = [ 1080 , 1920 ] Define the distance that the light will travel from optimized hologram. distance = 0.15 We have to set a target image. You can either load a sample image here or paint a white rectangle on a white background like in this example. target = torch . zeros ( resolution [ 0 ], resolution [ 1 ]) target [ 500 : 600 , 400 : 450 ] = 1. Surely, we also have to set the number of iterations and learning rate for our optimizations. If you want the GPU support, you also have to set the cuda as True . Propagation type has to be defined as well. In this example, we will use transfer function Fresnel approach. For more on propagation types, curious readers can consult Computational Fourier Optics David Vuelz (ISBN13:9780819482044). iteration_number = 100 learning_rate = 0.1 cuda = True propagation_type = 'TR Fresnel' This step concludes our preparations. Let's dive into optimizing our phase-only holograms. Depending on your choice, you can either optimize using Gerchberg-Saxton approach or the Stochastic Gradient Descent approach. This document will only show you Stochastic Gradient Descent approach as it is the state of art. However, optimizing a phase-only hologram is as importing: from odak.learn.wave import gerchberg_saxton and almost as easy as replacing stochastic_gradient_descent with gerchberg_saxton in the upcoming described hologram routine. For greater details, consult to documentation of odak.learn.wave.gerchberg_saxton .","title":"Preparation"},{"location":"notes/optimizing_holograms_using_odak/#stochastic-gradient-descent-approach","text":"We have prepared a function for you to avoid compiling a differentiable hologram optimizer from scratch. hologram , reconstructed = stochastic_gradient_descent ( target , wavelength , distance , dx , resolution , 'TR Fresnel' , iteration_number , learning_rate = learning_rate , cuda = cuda ) Iteration: 99 loss:0.0003 Congratulations! You have just optimized a phase-only hologram that reconstruct your target image at the target depth. Surely, you want to see what kind of image is being reconstructed with this newly optimized hologram. You can save the outcome to an image file easily. Odak provides tools to save and load images. First, you have to import: from odak.learn.tools import save_image , load_image As you can recall, we have created a target image earlier that is normalized between zero and one. The same is true for our result, reconstructed . Therefore, we have to save it correctly by taking that into account. Note that reconstructed is the complex field generated by our optimized hologram variable. So, we need to save the reconstructed intensity as humans and cameras capture intensity but not a complex field with phase and amplitude. reconstructed_intensity = calculate_amplitude ( reconstructed ) ** 2 save_image ( 'reconstructed_image.png' , reconstructed_intensity , cmin = 0. , cmax = 1. ) True To save our hologram as an image so that we can load it to a spatial light modulator, we have to normalize it between zero and 255 (dynamic range of a typical image on a computer). P.S. Depending on your SLM's calibration and dynamic range things may vary. slm_range = 2 * 3.14 dynamic_range = 255 phase_hologram = calculate_phase ( hologram ) phase_only_hologram = ( phase_hologram % slm_range ) / ( slm_range ) * dynamic_range It is now time for saving our hologram: save_image ( 'phase_only_hologram.png' , phase_only_hologram ) True In some cases, you may want to add a grating term to your hologram as you will display it on a spatial light modulator. There are various reasons for that, but the most obvious is getting rid of zeroth-order reflections that are not modulated by your hologram. In case you need it is as simple as below: from odak.learn.wave import linear_grating grating = linear_grating ( resolution [ 0 ], resolution [ 1 ], axis = 'y' ) . to ( phase_hologram . device ) phase_only_hologram_w_grating = phase_hologram + calculate_phase ( grating ) And let's save what we got from this step: phase_only_hologram_w_grating = ( phase_only_hologram_w_grating % slm_range ) / ( slm_range ) * dynamic_range save_image ( 'phase_only_hologram_w_grating.png' , phase_only_hologram_w_grating ) True","title":"Stochastic Gradient Descent approach"},{"location":"notes/optimizing_holograms_using_odak/#see-also","text":"For more engineering notes, follow: Computer Generated-Holography","title":"See also"},{"location":"notes/using_metameric_loss/","text":"This engineering note will give you an idea about using the metameric perceptual loss in odak . This note is compiled by David Walton . If you have further questions regarding this note, please email David at david.walton.13@ucl.ac.uk . Our metameric loss function works in a very similar way to built in loss functions in pytorch , such as torch.nn.MSELoss() . However, it has a number of parameters which can be adjusted on creation (see the documentation ). Additionally, when calculating the loss a gaze location must be specified. For example: loss_func = odak.learn.perception.MetamericLoss() loss = loss_func(my_image, gt_image, gaze=[0.7, 0.3]) The loss function caches some information, and performs most efficiently when repeatedly calculating losses for the same image size, with the same gaze location and foveation settings. We recommend adjusting the parameters of the loss function to match your application. Most importantly, please set the real_image_width and real_viewing_distance parameters to correspond to how your image will be displayed to the user. The alpha parameter controls the intensity of the foveation effect. You should only need to set alpha once - you can then adjust the width and viewing distance to achieve the same apparent foveation effect on a range of displays & viewing conditions. Note that we assume the pixels in the displayed image are square, and derive the height from the image dimensions. We also provide two baseline loss functions BlurLoss and MetamerMSELoss which function in much the same way. At the present time the loss functions are implemented only for images displayed to a user on a flat 2D display (e.g. an LCD computer monitor). Support for equirectangular 3D images is planned for the future. See also \u00b6 Visual perception","title":"Using metameric loss"},{"location":"notes/using_metameric_loss/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/beginning/","text":"What is Odak? \u00b6 Odak (pronounced \"O-dac\") is the fundamental library for scientific computing in optical sciences, computer graphics and visual perception. Why does it exist? \u00b6 This question has two answers. One of them is related to the history of Odak , which is partially answered in the next section. The other answer lies in what kind of submodules Odak has in it. Depending on a need of a scientist at all levels or a professional from the industry, these submodules can help the design processes in optics and visual perception. Odak includes modules for geometric 3D raytracing , Jones calculus , wave optics , and a set of tools to ease pain in measurement , exporting/importing CAD , and visualization during a design process. We have generated a set of recipes that go well with machine learning approaches compatible with the PyTorch learning framework as provided here . We have created many test scripts to inspire how you use Odak and helping your design process. Finally, we have created a distribution system to process tasks in parallel across multiple computing resources within the same network. Odak can either run using CPUs or automatically switch to NVIDIA GPUs . History \u00b6 In the summer of 2011, I, Kaan Ak\u015fit , was a PhD student. At the time, I had some understanding of the Python programming language, and I created my first Python based computer game using pygame , a fantastic library, over a weekend in 2009. I was actively using Python to deploy packages for the Linux distribution that I supported at the time, Pardus . Meantime, that summer, I didn't have any internship or any vital task that I had to complete. I was super curious about the internals of the optical design software that I used at the time, ZEMAX . All of this lead to an exciting never-ending excursion that I still enjoy to this day, which I named Odak. Odak means focus in Turkish, and pronounced as O-dac . The very first paper I read to build the pieces of Odak was General Ray tracing procedure\" from G.H. Spencer and M.V.R.K Murty , an article on routines for raytracing, published at the Journal of the Optical Society of America, Issue 6, Volume 52, Page 672 . It helped to add reflection and refraction functions required in a raytracing routine. I continuously add to Odak over my entire professional life. That little raytracing program I wrote in 2011 is now a vital library for my research, and much more than a raytracer. I can write pages and pages about what happened next. You can accurately estimate what happened next by checking my website and my cv . But I think the most critical part is always the beginning as it can inspire many other people to follow their thoughts and build their own thing! I used Odak in my all published papers. When I look back, I can only say that I am thankful to 2011 me spending a part of his summer in front of a computer to code a raytracer for optical design. Odak is now more than a raytracer, expanding on many other aspects of light, including vision science, polarization optics, computer-generated holography or machine learning routines for light sciences. Odak keeps on growing thanks to a body of people that contributed over time . I will keep it growing in the future and will continually transform into the tool that I need to innovate. All of it is free as in free-free, and all is sharable as I believe in people.","title":"What is Odak?"},{"location":"odak/beginning/#what-is-odak","text":"Odak (pronounced \"O-dac\") is the fundamental library for scientific computing in optical sciences, computer graphics and visual perception.","title":"What is Odak?"},{"location":"odak/beginning/#why-does-it-exist","text":"This question has two answers. One of them is related to the history of Odak , which is partially answered in the next section. The other answer lies in what kind of submodules Odak has in it. Depending on a need of a scientist at all levels or a professional from the industry, these submodules can help the design processes in optics and visual perception. Odak includes modules for geometric 3D raytracing , Jones calculus , wave optics , and a set of tools to ease pain in measurement , exporting/importing CAD , and visualization during a design process. We have generated a set of recipes that go well with machine learning approaches compatible with the PyTorch learning framework as provided here . We have created many test scripts to inspire how you use Odak and helping your design process. Finally, we have created a distribution system to process tasks in parallel across multiple computing resources within the same network. Odak can either run using CPUs or automatically switch to NVIDIA GPUs .","title":"Why does it exist?"},{"location":"odak/beginning/#history","text":"In the summer of 2011, I, Kaan Ak\u015fit , was a PhD student. At the time, I had some understanding of the Python programming language, and I created my first Python based computer game using pygame , a fantastic library, over a weekend in 2009. I was actively using Python to deploy packages for the Linux distribution that I supported at the time, Pardus . Meantime, that summer, I didn't have any internship or any vital task that I had to complete. I was super curious about the internals of the optical design software that I used at the time, ZEMAX . All of this lead to an exciting never-ending excursion that I still enjoy to this day, which I named Odak. Odak means focus in Turkish, and pronounced as O-dac . The very first paper I read to build the pieces of Odak was General Ray tracing procedure\" from G.H. Spencer and M.V.R.K Murty , an article on routines for raytracing, published at the Journal of the Optical Society of America, Issue 6, Volume 52, Page 672 . It helped to add reflection and refraction functions required in a raytracing routine. I continuously add to Odak over my entire professional life. That little raytracing program I wrote in 2011 is now a vital library for my research, and much more than a raytracer. I can write pages and pages about what happened next. You can accurately estimate what happened next by checking my website and my cv . But I think the most critical part is always the beginning as it can inspire many other people to follow their thoughts and build their own thing! I used Odak in my all published papers. When I look back, I can only say that I am thankful to 2011 me spending a part of his summer in front of a computer to code a raytracer for optical design. Odak is now more than a raytracer, expanding on many other aspects of light, including vision science, polarization optics, computer-generated holography or machine learning routines for light sciences. Odak keeps on growing thanks to a body of people that contributed over time . I will keep it growing in the future and will continually transform into the tool that I need to innovate. All of it is free as in free-free, and all is sharable as I believe in people.","title":"History"},{"location":"odak/installation/","text":"Installation \u00b6 We use odak with Linux operating systems. Therefore, we don't know if it can work with Windows or Mac operating systems. Odak can be installed in multiple ways. However, our recommended method for installing Odak is using pip distribution system. We update Odak within pip with each new version. Thus, the most straightforward way to install Odak is to use the below command in a Linux shell: pip3 install odak Note that Odak is in constant development. One may want to install the latest and greatest odak in the source repository for their reasons. In this case, our recommended method is to rely on pip for installing Odak from the source using: pip3 install git+https://github.com/kunguz/odak One can also install Odak without pip by first getting a local copy and installing using Python. Such an installation can be conducted using: git clone git@github.com:kunguz/odak.git cd odak pip3 install -r requirements.txt python3 setup.py install Notes before running \u00b6 Some notes should be highlighted to users, and these include: Odak installs Numpy during an installation but not Cupy . Therefore, anything that relies on approaches that use Numpy or Cupy can not be accelerated using GPU without installing Cupy : pip3 install cupy . Odak installs PyTorch that only uses CPU . To properly install PyTorch with GPU support, please consult PyTorch website . Testing an installation \u00b6 After installing Odak, one can test if Odak has been appropriately installed with its dependencies by running the unit tests. To be able to run unit tests, make sure to have pytest installed: pip3 install -U pytest Once pytest is installed, unit tests can be run by calling: cd odak pytest The tests should return no error. However, if an error is encountered, please start a new issue to help us be aware of the issue.","title":"Installation"},{"location":"odak/installation/#installation","text":"We use odak with Linux operating systems. Therefore, we don't know if it can work with Windows or Mac operating systems. Odak can be installed in multiple ways. However, our recommended method for installing Odak is using pip distribution system. We update Odak within pip with each new version. Thus, the most straightforward way to install Odak is to use the below command in a Linux shell: pip3 install odak Note that Odak is in constant development. One may want to install the latest and greatest odak in the source repository for their reasons. In this case, our recommended method is to rely on pip for installing Odak from the source using: pip3 install git+https://github.com/kunguz/odak One can also install Odak without pip by first getting a local copy and installing using Python. Such an installation can be conducted using: git clone git@github.com:kunguz/odak.git cd odak pip3 install -r requirements.txt python3 setup.py install","title":"Installation"},{"location":"odak/installation/#notes-before-running","text":"Some notes should be highlighted to users, and these include: Odak installs Numpy during an installation but not Cupy . Therefore, anything that relies on approaches that use Numpy or Cupy can not be accelerated using GPU without installing Cupy : pip3 install cupy . Odak installs PyTorch that only uses CPU . To properly install PyTorch with GPU support, please consult PyTorch website .","title":"Notes before running"},{"location":"odak/installation/#testing-an-installation","text":"After installing Odak, one can test if Odak has been appropriately installed with its dependencies by running the unit tests. To be able to run unit tests, make sure to have pytest installed: pip3 install -U pytest Once pytest is installed, unit tests can be run by calling: cd odak pytest The tests should return no error. However, if an error is encountered, please start a new issue to help us be aware of the issue.","title":"Testing an installation"},{"location":"odak/learn/perception/blur_loss/","text":"Blur Loss \u00b6 BlurLoss implements two different blur losses. When blur_source is set to False , it implements blur_match, trying to match the input image to the blurred target image. This tries to match the source input image to a blurred version of the target. When blur_source is set to True , it implements blur_lowpass, matching the blurred version of the input image to the blurred target image. This tries to match only the low frequencies of the source input image to the low frequencies of the target. The interface is similar to other pytorch loss functions, but note that the gaze location must be provided in addition to the source and target images. __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ]) special \u00b6 Calculates the Blur Loss. Parameters: Name Type Description Default image torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required target torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required gaze list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. [0.5, 0.5] Returns: Type Description torch.tensor The computed loss. Source code in odak/learn/perception/blur_loss.py def __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ]): \"\"\" Calculates the Blur Loss. Parameters ---------- image : torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) target : torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) gaze : list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. Returns ------- loss : torch.tensor The computed loss. \"\"\" blurred_target = self . blur_image ( target , gaze ) if self . blur_source : blurred_image = self . blur_image ( image , gaze ) return self . loss_func ( blurred_image , blurred_target ) else : return self . loss_func ( image , blurred_target ) __init__ ( self , device = device ( type = 'cpu' ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , mode = 'quadratic' , blur_source = False ) special \u00b6 Parameters: Name Type Description Default alpha float parameter controlling foveation - larger values mean bigger pooling regions. 0.08 real_image_width float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. 0.2 real_viewing_distance float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. 0.7 mode str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". 'quadratic' blur_source bool If true, blurs the source image as well as the target before computing the loss. False Source code in odak/learn/perception/blur_loss.py def __init__ ( self , device = torch . device ( \"cpu\" ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , mode = \"quadratic\" , blur_source = False ): \"\"\" Parameters ---------- alpha : float parameter controlling foveation - larger values mean bigger pooling regions. real_image_width : float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. real_viewing_distance : float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. mode : str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". blur_source : bool If true, blurs the source image as well as the target before computing the loss. \"\"\" self . target = None self . device = device self . alpha = alpha self . real_image_width = real_image_width self . real_viewing_distance = real_viewing_distance self . mode = mode self . blur = None self . loss_func = torch . nn . MSELoss () self . blur_source = blur_source See also \u00b6 Visual Perception","title":"Blur Loss"},{"location":"odak/learn/perception/blur_loss/#blur-loss","text":"BlurLoss implements two different blur losses. When blur_source is set to False , it implements blur_match, trying to match the input image to the blurred target image. This tries to match the source input image to a blurred version of the target. When blur_source is set to True , it implements blur_lowpass, matching the blurred version of the input image to the blurred target image. This tries to match only the low frequencies of the source input image to the low frequencies of the target. The interface is similar to other pytorch loss functions, but note that the gaze location must be provided in addition to the source and target images.","title":"Blur Loss"},{"location":"odak/learn/perception/blur_loss/#odak.learn.perception.blur_loss.BlurLoss.__call__","text":"Calculates the Blur Loss. Parameters: Name Type Description Default image torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required target torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required gaze list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. [0.5, 0.5] Returns: Type Description torch.tensor The computed loss. Source code in odak/learn/perception/blur_loss.py def __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ]): \"\"\" Calculates the Blur Loss. Parameters ---------- image : torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) target : torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) gaze : list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. Returns ------- loss : torch.tensor The computed loss. \"\"\" blurred_target = self . blur_image ( target , gaze ) if self . blur_source : blurred_image = self . blur_image ( image , gaze ) return self . loss_func ( blurred_image , blurred_target ) else : return self . loss_func ( image , blurred_target )","title":"__call__()"},{"location":"odak/learn/perception/blur_loss/#odak.learn.perception.blur_loss.BlurLoss.__init__","text":"Parameters: Name Type Description Default alpha float parameter controlling foveation - larger values mean bigger pooling regions. 0.08 real_image_width float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. 0.2 real_viewing_distance float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. 0.7 mode str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". 'quadratic' blur_source bool If true, blurs the source image as well as the target before computing the loss. False Source code in odak/learn/perception/blur_loss.py def __init__ ( self , device = torch . device ( \"cpu\" ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , mode = \"quadratic\" , blur_source = False ): \"\"\" Parameters ---------- alpha : float parameter controlling foveation - larger values mean bigger pooling regions. real_image_width : float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. real_viewing_distance : float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. mode : str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". blur_source : bool If true, blurs the source image as well as the target before computing the loss. \"\"\" self . target = None self . device = device self . alpha = alpha self . real_image_width = real_image_width self . real_viewing_distance = real_viewing_distance self . mode = mode self . blur = None self . loss_func = torch . nn . MSELoss () self . blur_source = blur_source","title":"__init__()"},{"location":"odak/learn/perception/blur_loss/#see-also","text":"Visual Perception","title":"See also"},{"location":"odak/learn/perception/make_3d_location_map/","text":"odak.learn.perception.make_3d_location_map \u00b6 Makes a map of the real 3D location that each pixel in an image corresponds to, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Parameters: Name Type Description Default image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed 3D location map, of size 3xWxH. Source code in odak/learn/perception/foveation.py def make_3d_location_map ( image_pixel_size , real_image_width = 0.3 , real_viewing_distance = 0.6 ): \"\"\" Makes a map of the real 3D location that each pixel in an image corresponds to, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Parameters ---------- image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- map : torch.tensor The computed 3D location map, of size 3xWxH. \"\"\" real_image_height = ( real_image_width / image_pixel_size [ - 1 ]) * image_pixel_size [ - 2 ] x_coords = torch . linspace ( - 0.5 , 0.5 , image_pixel_size [ - 1 ]) * real_image_width x_coords = x_coords [ None , None , :] . repeat ( 1 , image_pixel_size [ - 2 ], 1 ) y_coords = torch . linspace ( - 0.5 , 0.5 , image_pixel_size [ - 2 ]) * real_image_height y_coords = y_coords [ None , :, None ] . repeat ( 1 , 1 , image_pixel_size [ - 1 ]) z_coords = torch . ones ( ( 1 , image_pixel_size [ - 2 ], image_pixel_size [ - 1 ])) * real_viewing_distance return torch . cat ([ x_coords , y_coords , z_coords ], dim = 0 ) See also \u00b6 Visual perception","title":"odak.learn.perception.make_3d_location_map"},{"location":"odak/learn/perception/make_3d_location_map/#odaklearnperceptionmake_3d_location_map","text":"Makes a map of the real 3D location that each pixel in an image corresponds to, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Parameters: Name Type Description Default image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed 3D location map, of size 3xWxH. Source code in odak/learn/perception/foveation.py def make_3d_location_map ( image_pixel_size , real_image_width = 0.3 , real_viewing_distance = 0.6 ): \"\"\" Makes a map of the real 3D location that each pixel in an image corresponds to, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Parameters ---------- image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- map : torch.tensor The computed 3D location map, of size 3xWxH. \"\"\" real_image_height = ( real_image_width / image_pixel_size [ - 1 ]) * image_pixel_size [ - 2 ] x_coords = torch . linspace ( - 0.5 , 0.5 , image_pixel_size [ - 1 ]) * real_image_width x_coords = x_coords [ None , None , :] . repeat ( 1 , image_pixel_size [ - 2 ], 1 ) y_coords = torch . linspace ( - 0.5 , 0.5 , image_pixel_size [ - 2 ]) * real_image_height y_coords = y_coords [ None , :, None ] . repeat ( 1 , 1 , image_pixel_size [ - 1 ]) z_coords = torch . ones ( ( 1 , image_pixel_size [ - 2 ], image_pixel_size [ - 1 ])) * real_viewing_distance return torch . cat ([ x_coords , y_coords , z_coords ], dim = 0 )","title":"odak.learn.perception.make_3d_location_map"},{"location":"odak/learn/perception/make_3d_location_map/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/learn/perception/make_eccentricity_distance_maps/","text":"odak.learn.perception.make_eccentricity_distance_maps \u00b6 Makes a map of the eccentricity of each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output in radians. Parameters: Name Type Description Default gaze_location tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) required image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed eccentricity map, of size WxH. Source code in odak/learn/perception/foveation.py def make_eccentricity_distance_maps ( gaze_location , image_pixel_size , real_image_width = 0.3 , real_viewing_distance = 0.6 ): \"\"\" Makes a map of the eccentricity of each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output in radians. Parameters ---------- gaze_location : tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- eccentricity_map : torch.tensor The computed eccentricity map, of size WxH. distance_map : torch.tensor The computed distance map, of size WxH. \"\"\" real_image_height = ( real_image_width / image_pixel_size [ - 1 ]) * image_pixel_size [ - 2 ] location_map = make_3d_location_map ( image_pixel_size , real_image_width , real_viewing_distance ) distance_map = torch . sqrt ( torch . sum ( location_map * location_map , dim = 0 )) direction_map = location_map / distance_map gaze_location_3d = torch . tensor ([ ( gaze_location [ 0 ] * 2 - 1 ) * real_image_width * 0.5 , ( gaze_location [ 1 ] * 2 - 1 ) * real_image_height * 0.5 , real_viewing_distance ]) gaze_dir = gaze_location_3d / \\ torch . sqrt ( torch . sum ( gaze_location_3d * gaze_location_3d )) gaze_dir = gaze_dir [:, None , None ] dot_prod_map = torch . sum ( gaze_dir * direction_map , dim = 0 ) dot_prod_map = torch . clamp ( dot_prod_map , min =- 1.0 , max = 1.0 ) eccentricity_map = torch . acos ( dot_prod_map ) return eccentricity_map , distance_map See also \u00b6 Visual perception","title":"odak.learn.perception.make_eccentricity_distance_maps"},{"location":"odak/learn/perception/make_eccentricity_distance_maps/#odaklearnperceptionmake_eccentricity_distance_maps","text":"Makes a map of the eccentricity of each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output in radians. Parameters: Name Type Description Default gaze_location tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) required image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed eccentricity map, of size WxH. Source code in odak/learn/perception/foveation.py def make_eccentricity_distance_maps ( gaze_location , image_pixel_size , real_image_width = 0.3 , real_viewing_distance = 0.6 ): \"\"\" Makes a map of the eccentricity of each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output in radians. Parameters ---------- gaze_location : tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- eccentricity_map : torch.tensor The computed eccentricity map, of size WxH. distance_map : torch.tensor The computed distance map, of size WxH. \"\"\" real_image_height = ( real_image_width / image_pixel_size [ - 1 ]) * image_pixel_size [ - 2 ] location_map = make_3d_location_map ( image_pixel_size , real_image_width , real_viewing_distance ) distance_map = torch . sqrt ( torch . sum ( location_map * location_map , dim = 0 )) direction_map = location_map / distance_map gaze_location_3d = torch . tensor ([ ( gaze_location [ 0 ] * 2 - 1 ) * real_image_width * 0.5 , ( gaze_location [ 1 ] * 2 - 1 ) * real_image_height * 0.5 , real_viewing_distance ]) gaze_dir = gaze_location_3d / \\ torch . sqrt ( torch . sum ( gaze_location_3d * gaze_location_3d )) gaze_dir = gaze_dir [:, None , None ] dot_prod_map = torch . sum ( gaze_dir * direction_map , dim = 0 ) dot_prod_map = torch . clamp ( dot_prod_map , min =- 1.0 , max = 1.0 ) eccentricity_map = torch . acos ( dot_prod_map ) return eccentricity_map , distance_map","title":"odak.learn.perception.make_eccentricity_distance_maps"},{"location":"odak/learn/perception/make_eccentricity_distance_maps/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/learn/perception/make_pooling_size_map_lod/","text":"odak.learn.perception.make_pooling_size_map_lod \u00b6 This function is similar to make_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from to achieve the correct pooling region areas. Parameters: Name Type Description Default gaze_location tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) required image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed pooling size map, of size WxH. Source code in odak/learn/perception/foveation.py def make_pooling_size_map_lod ( gaze_location , image_pixel_size , alpha = 0.3 , real_image_width = 0.3 , real_viewing_distance = 0.6 , mode = \"quadratic\" ): \"\"\" This function is similar to make_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from to achieve the correct pooling region areas. Parameters ---------- gaze_location : tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- pooling_size_map : torch.tensor The computed pooling size map, of size WxH. \"\"\" pooling_pixel = make_pooling_size_map_pixels ( gaze_location , image_pixel_size , alpha , real_image_width , real_viewing_distance , mode ) pooling_lod = torch . log2 ( 1e-6 + pooling_pixel ) pooling_lod [ pooling_lod < 0 ] = 0 return pooling_lod See also \u00b6 Visual perception","title":"odak.learn.perception.make_pooling_size_map_lod"},{"location":"odak/learn/perception/make_pooling_size_map_lod/#odaklearnperceptionmake_pooling_size_map_lod","text":"This function is similar to make_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from to achieve the correct pooling region areas. Parameters: Name Type Description Default gaze_location tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) required image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed pooling size map, of size WxH. Source code in odak/learn/perception/foveation.py def make_pooling_size_map_lod ( gaze_location , image_pixel_size , alpha = 0.3 , real_image_width = 0.3 , real_viewing_distance = 0.6 , mode = \"quadratic\" ): \"\"\" This function is similar to make_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from to achieve the correct pooling region areas. Parameters ---------- gaze_location : tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- pooling_size_map : torch.tensor The computed pooling size map, of size WxH. \"\"\" pooling_pixel = make_pooling_size_map_pixels ( gaze_location , image_pixel_size , alpha , real_image_width , real_viewing_distance , mode ) pooling_lod = torch . log2 ( 1e-6 + pooling_pixel ) pooling_lod [ pooling_lod < 0 ] = 0 return pooling_lod","title":"odak.learn.perception.make_pooling_size_map_lod"},{"location":"odak/learn/perception/make_pooling_size_map_lod/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/learn/perception/make_pooling_size_map_pixels/","text":"odak.learn.perception.make_pooling_size_map_pizels \u00b6 Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output is the width of the pooling region in pixels. Parameters: Name Type Description Default gaze_location tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) required image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed pooling size map, of size WxH. Source code in odak/learn/perception/foveation.py def make_pooling_size_map_pixels ( gaze_location , image_pixel_size , alpha = 0.3 , real_image_width = 0.3 , real_viewing_distance = 0.6 , mode = \"quadratic\" ): \"\"\" Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output is the width of the pooling region in pixels. Parameters ---------- gaze_location : tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- pooling_size_map : torch.tensor The computed pooling size map, of size WxH. \"\"\" eccentricity , distance_to_pixel = make_eccentricity_distance_maps ( gaze_location , image_pixel_size , real_image_width , real_viewing_distance ) eccentricity_centre , _ = make_eccentricity_distance_maps ( [ 0.5 , 0.5 ], image_pixel_size , real_image_width , real_viewing_distance ) pooling_rad = alpha * eccentricity if mode == \"quadratic\" : pooling_rad *= eccentricity angle_min = eccentricity_centre - pooling_rad * 0.5 angle_max = eccentricity_centre + pooling_rad * 0.5 major_axis = ( torch . tan ( angle_max ) - torch . tan ( angle_min )) / \\ real_viewing_distance minor_axis = 2 * distance_to_pixel * torch . tan ( pooling_rad * 0.5 ) area = math . pi * major_axis * minor_axis # Should be +ve anyway, but check to ensure we don't take sqrt of negative number area = torch . abs ( area ) pooling_real = torch . sqrt ( area ) pooling_pixel = ( pooling_real / real_image_width ) * image_pixel_size [ 1 ] return pooling_pixel See also \u00b6 Visual perception","title":"odak.learn.perception.make_pooling_size_map_pizels"},{"location":"odak/learn/perception/make_pooling_size_map_pixels/#odaklearnperceptionmake_pooling_size_map_pizels","text":"Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output is the width of the pooling region in pixels. Parameters: Name Type Description Default gaze_location tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) required image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed pooling size map, of size WxH. Source code in odak/learn/perception/foveation.py def make_pooling_size_map_pixels ( gaze_location , image_pixel_size , alpha = 0.3 , real_image_width = 0.3 , real_viewing_distance = 0.6 , mode = \"quadratic\" ): \"\"\" Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output is the width of the pooling region in pixels. Parameters ---------- gaze_location : tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- pooling_size_map : torch.tensor The computed pooling size map, of size WxH. \"\"\" eccentricity , distance_to_pixel = make_eccentricity_distance_maps ( gaze_location , image_pixel_size , real_image_width , real_viewing_distance ) eccentricity_centre , _ = make_eccentricity_distance_maps ( [ 0.5 , 0.5 ], image_pixel_size , real_image_width , real_viewing_distance ) pooling_rad = alpha * eccentricity if mode == \"quadratic\" : pooling_rad *= eccentricity angle_min = eccentricity_centre - pooling_rad * 0.5 angle_max = eccentricity_centre + pooling_rad * 0.5 major_axis = ( torch . tan ( angle_max ) - torch . tan ( angle_min )) / \\ real_viewing_distance minor_axis = 2 * distance_to_pixel * torch . tan ( pooling_rad * 0.5 ) area = math . pi * major_axis * minor_axis # Should be +ve anyway, but check to ensure we don't take sqrt of negative number area = torch . abs ( area ) pooling_real = torch . sqrt ( area ) pooling_pixel = ( pooling_real / real_image_width ) * image_pixel_size [ 1 ] return pooling_pixel","title":"odak.learn.perception.make_pooling_size_map_pizels"},{"location":"odak/learn/perception/make_pooling_size_map_pixels/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/learn/perception/make_radial_map/","text":"odak.learn.perception.make_pooling_size_map_pizels \u00b6 Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output is the width of the pooling region in pixels. Parameters: Name Type Description Default gaze_location tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) required image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed pooling size map, of size WxH. Source code in odak/learn/perception/foveation.py def make_pooling_size_map_pixels ( gaze_location , image_pixel_size , alpha = 0.3 , real_image_width = 0.3 , real_viewing_distance = 0.6 , mode = \"quadratic\" ): \"\"\" Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output is the width of the pooling region in pixels. Parameters ---------- gaze_location : tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- pooling_size_map : torch.tensor The computed pooling size map, of size WxH. \"\"\" eccentricity , distance_to_pixel = make_eccentricity_distance_maps ( gaze_location , image_pixel_size , real_image_width , real_viewing_distance ) eccentricity_centre , _ = make_eccentricity_distance_maps ( [ 0.5 , 0.5 ], image_pixel_size , real_image_width , real_viewing_distance ) pooling_rad = alpha * eccentricity if mode == \"quadratic\" : pooling_rad *= eccentricity angle_min = eccentricity_centre - pooling_rad * 0.5 angle_max = eccentricity_centre + pooling_rad * 0.5 major_axis = ( torch . tan ( angle_max ) - torch . tan ( angle_min )) / \\ real_viewing_distance minor_axis = 2 * distance_to_pixel * torch . tan ( pooling_rad * 0.5 ) area = math . pi * major_axis * minor_axis # Should be +ve anyway, but check to ensure we don't take sqrt of negative number area = torch . abs ( area ) pooling_real = torch . sqrt ( area ) pooling_pixel = ( pooling_real / real_image_width ) * image_pixel_size [ 1 ] return pooling_pixel See also \u00b6 Visual perception","title":"odak.learn.perception.make_pooling_size_map_pizels"},{"location":"odak/learn/perception/make_radial_map/#odaklearnperceptionmake_pooling_size_map_pizels","text":"Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output is the width of the pooling region in pixels. Parameters: Name Type Description Default gaze_location tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) required image_pixel_size tuple of ints The size of the image in pixels, as a tuple of form (height, width) required real_image_width float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance 0.3 real_viewing_distance float The real distance from the user's viewpoint to the screen. 0.6 Returns: Type Description torch.tensor The computed pooling size map, of size WxH. Source code in odak/learn/perception/foveation.py def make_pooling_size_map_pixels ( gaze_location , image_pixel_size , alpha = 0.3 , real_image_width = 0.3 , real_viewing_distance = 0.6 , mode = \"quadratic\" ): \"\"\" Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). Assumes the viewpoint is located at the centre of the image, and the screen is perpendicular to the viewing direction. Output is the width of the pooling region in pixels. Parameters ---------- gaze_location : tuple of floats User's gaze (fixation point) in the image. Should be given as a tuple with normalized image coordinates (ranging from 0 to 1) image_pixel_size : tuple of ints The size of the image in pixels, as a tuple of form (height, width) real_image_width : float The real width of the image as displayed. Units not important, as long as they are the same as those used for real_viewing_distance real_viewing_distance : float The real distance from the user's viewpoint to the screen. Returns ------- pooling_size_map : torch.tensor The computed pooling size map, of size WxH. \"\"\" eccentricity , distance_to_pixel = make_eccentricity_distance_maps ( gaze_location , image_pixel_size , real_image_width , real_viewing_distance ) eccentricity_centre , _ = make_eccentricity_distance_maps ( [ 0.5 , 0.5 ], image_pixel_size , real_image_width , real_viewing_distance ) pooling_rad = alpha * eccentricity if mode == \"quadratic\" : pooling_rad *= eccentricity angle_min = eccentricity_centre - pooling_rad * 0.5 angle_max = eccentricity_centre + pooling_rad * 0.5 major_axis = ( torch . tan ( angle_max ) - torch . tan ( angle_min )) / \\ real_viewing_distance minor_axis = 2 * distance_to_pixel * torch . tan ( pooling_rad * 0.5 ) area = math . pi * major_axis * minor_axis # Should be +ve anyway, but check to ensure we don't take sqrt of negative number area = torch . abs ( area ) pooling_real = torch . sqrt ( area ) pooling_pixel = ( pooling_real / real_image_width ) * image_pixel_size [ 1 ] return pooling_pixel","title":"odak.learn.perception.make_pooling_size_map_pizels"},{"location":"odak/learn/perception/make_radial_map/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/learn/perception/metamer_mse_loss/","text":"Metameric Loss \u00b6 Metameric Loss \u00b6 The MetamerMSELoss class provides a perceptual loss function. This generates a metamer for the target image, and then optimises the source image to be the same as this target image metamer. Please note this is different to MetamericLoss which optimises the source image to be any metamer of the target image. Its interface is similar to other pytorch loss functions, but note that the gaze location must be provided in addition to the source and target images. __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ]) special \u00b6 Calculates the Metamer MSE Loss. Parameters: Name Type Description Default image torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required target torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required gaze list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. [0.5, 0.5] Returns: Type Description torch.tensor The computed loss. Source code in odak/learn/perception/metamer_mse_loss.py def __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ]): \"\"\" Calculates the Metamer MSE Loss. Parameters ---------- image : torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) target : torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) gaze : list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. Returns ------- loss : torch.tensor The computed loss. \"\"\" # Pad image and target if necessary min_divisor = 2 ** self . metameric_loss . n_pyramid_levels height = image . size ( 2 ) width = image . size ( 3 ) required_height = math . ceil ( height / min_divisor ) * min_divisor required_width = math . ceil ( width / min_divisor ) * min_divisor if required_height > height or required_width > width : # We need to pad! pad = torch . nn . ReflectionPad2d ( ( 0 , 0 , required_height - height , required_width - width )) image = pad ( image ) target = pad ( target ) if target is not self . target or self . target is None : self . target_metamer = self . gen_metamer ( target , gaze ) self . target = target return self . loss_func ( image , self . target_metamer ) __init__ ( self , device = device ( type = 'cpu' ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , mode = 'quadratic' , n_pyramid_levels = 5 , n_orientations = 2 ) special \u00b6 Parameters: Name Type Description Default alpha float parameter controlling foveation - larger values mean bigger pooling regions. 0.08 real_image_width float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. 0.2 real_viewing_distance float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. 0.7 n_pyramid_levels int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. 5 mode str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". 'quadratic' n_orientations int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. 2 Source code in odak/learn/perception/metamer_mse_loss.py def __init__ ( self , device = torch . device ( \"cpu\" ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , mode = \"quadratic\" , n_pyramid_levels = 5 , n_orientations = 2 ): \"\"\" Parameters ---------- alpha : float parameter controlling foveation - larger values mean bigger pooling regions. real_image_width : float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. real_viewing_distance : float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. n_pyramid_levels : int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. mode : str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". n_orientations : int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. \"\"\" self . target = None self . target_metamer = None self . metameric_loss = MetamericLoss ( device = device , alpha = alpha , real_image_width = real_image_width , real_viewing_distance = real_viewing_distance , n_pyramid_levels = n_pyramid_levels , n_orientations = n_orientations , use_l2_foveal_loss = False ) self . loss_func = torch . nn . MSELoss () gen_metamer ( self , image , gaze ) \u00b6 Generates a metamer for an image, following the method in this paper This function can be used on its own to generate a metamer for a desired image. Parameters: Name Type Description Default image torch.tensor Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions) required gaze list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. required Returns: Type Description torch.tensor The generated metamer image Source code in odak/learn/perception/metamer_mse_loss.py def gen_metamer ( self , image , gaze ): \"\"\" Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943) This function can be used on its own to generate a metamer for a desired image. Parameters ---------- image : torch.tensor Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions) gaze : list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. Returns ------- metamer : torch.tensor The generated metamer image \"\"\" image = rgb_2_ycrcb ( image ) target_stats = self . metameric_loss . calc_statsmaps ( image , gaze = gaze , alpha = self . metameric_loss . alpha ) target_means = target_stats [:: 2 ] target_stdevs = target_stats [ 1 :: 2 ] torch . manual_seed ( 0 ) noise_image = torch . rand_like ( image ) noise_pyramid = self . metameric_loss . pyramid_maker . construct_pyramid ( noise_image , self . metameric_loss . n_pyramid_levels ) input_pyramid = self . metameric_loss . pyramid_maker . construct_pyramid ( image , self . metameric_loss . n_pyramid_levels ) def match_level ( input_level , target_mean , target_std ): level = input_level . clone () level -= torch . mean ( level ) input_std = torch . sqrt ( torch . mean ( level * level )) eps = 1e-6 # Safeguard against divide by zero input_std [ input_std < eps ] = eps level /= input_std level *= target_std level += target_mean return level nbands = len ( noise_pyramid [ 0 ][ \"b\" ]) noise_pyramid [ 0 ][ \"h\" ] = match_level ( noise_pyramid [ 0 ][ \"h\" ], target_means [ 0 ], target_stdevs [ 0 ]) for l in range ( len ( noise_pyramid ) - 1 ): for b in range ( nbands ): noise_pyramid [ l ][ \"b\" ][ b ] = match_level ( noise_pyramid [ l ][ \"b\" ][ b ], target_means [ 1 + l * nbands + b ], target_stdevs [ 1 + l * nbands + b ]) noise_pyramid [ - 1 ][ \"l\" ] = input_pyramid [ - 1 ][ \"l\" ] metamer = self . metameric_loss . pyramid_maker . reconstruct_from_pyramid ( noise_pyramid ) metamer = ycrcb_2_rgb ( metamer ) return metamer See also \u00b6 Visual perception","title":"Metameric Loss"},{"location":"odak/learn/perception/metamer_mse_loss/#metameric-loss","text":"","title":"Metameric Loss"},{"location":"odak/learn/perception/metamer_mse_loss/#metameric-loss_1","text":"The MetamerMSELoss class provides a perceptual loss function. This generates a metamer for the target image, and then optimises the source image to be the same as this target image metamer. Please note this is different to MetamericLoss which optimises the source image to be any metamer of the target image. Its interface is similar to other pytorch loss functions, but note that the gaze location must be provided in addition to the source and target images.","title":"Metameric Loss"},{"location":"odak/learn/perception/metamer_mse_loss/#odak.learn.perception.metamer_mse_loss.MetamerMSELoss.__call__","text":"Calculates the Metamer MSE Loss. Parameters: Name Type Description Default image torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required target torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required gaze list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. [0.5, 0.5] Returns: Type Description torch.tensor The computed loss. Source code in odak/learn/perception/metamer_mse_loss.py def __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ]): \"\"\" Calculates the Metamer MSE Loss. Parameters ---------- image : torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) target : torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) gaze : list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. Returns ------- loss : torch.tensor The computed loss. \"\"\" # Pad image and target if necessary min_divisor = 2 ** self . metameric_loss . n_pyramid_levels height = image . size ( 2 ) width = image . size ( 3 ) required_height = math . ceil ( height / min_divisor ) * min_divisor required_width = math . ceil ( width / min_divisor ) * min_divisor if required_height > height or required_width > width : # We need to pad! pad = torch . nn . ReflectionPad2d ( ( 0 , 0 , required_height - height , required_width - width )) image = pad ( image ) target = pad ( target ) if target is not self . target or self . target is None : self . target_metamer = self . gen_metamer ( target , gaze ) self . target = target return self . loss_func ( image , self . target_metamer )","title":"__call__()"},{"location":"odak/learn/perception/metamer_mse_loss/#odak.learn.perception.metamer_mse_loss.MetamerMSELoss.__init__","text":"Parameters: Name Type Description Default alpha float parameter controlling foveation - larger values mean bigger pooling regions. 0.08 real_image_width float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. 0.2 real_viewing_distance float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. 0.7 n_pyramid_levels int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. 5 mode str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". 'quadratic' n_orientations int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. 2 Source code in odak/learn/perception/metamer_mse_loss.py def __init__ ( self , device = torch . device ( \"cpu\" ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , mode = \"quadratic\" , n_pyramid_levels = 5 , n_orientations = 2 ): \"\"\" Parameters ---------- alpha : float parameter controlling foveation - larger values mean bigger pooling regions. real_image_width : float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. real_viewing_distance : float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. n_pyramid_levels : int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. mode : str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". n_orientations : int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. \"\"\" self . target = None self . target_metamer = None self . metameric_loss = MetamericLoss ( device = device , alpha = alpha , real_image_width = real_image_width , real_viewing_distance = real_viewing_distance , n_pyramid_levels = n_pyramid_levels , n_orientations = n_orientations , use_l2_foveal_loss = False ) self . loss_func = torch . nn . MSELoss ()","title":"__init__()"},{"location":"odak/learn/perception/metamer_mse_loss/#odak.learn.perception.metamer_mse_loss.MetamerMSELoss.gen_metamer","text":"Generates a metamer for an image, following the method in this paper This function can be used on its own to generate a metamer for a desired image. Parameters: Name Type Description Default image torch.tensor Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions) required gaze list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. required Returns: Type Description torch.tensor The generated metamer image Source code in odak/learn/perception/metamer_mse_loss.py def gen_metamer ( self , image , gaze ): \"\"\" Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943) This function can be used on its own to generate a metamer for a desired image. Parameters ---------- image : torch.tensor Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions) gaze : list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. Returns ------- metamer : torch.tensor The generated metamer image \"\"\" image = rgb_2_ycrcb ( image ) target_stats = self . metameric_loss . calc_statsmaps ( image , gaze = gaze , alpha = self . metameric_loss . alpha ) target_means = target_stats [:: 2 ] target_stdevs = target_stats [ 1 :: 2 ] torch . manual_seed ( 0 ) noise_image = torch . rand_like ( image ) noise_pyramid = self . metameric_loss . pyramid_maker . construct_pyramid ( noise_image , self . metameric_loss . n_pyramid_levels ) input_pyramid = self . metameric_loss . pyramid_maker . construct_pyramid ( image , self . metameric_loss . n_pyramid_levels ) def match_level ( input_level , target_mean , target_std ): level = input_level . clone () level -= torch . mean ( level ) input_std = torch . sqrt ( torch . mean ( level * level )) eps = 1e-6 # Safeguard against divide by zero input_std [ input_std < eps ] = eps level /= input_std level *= target_std level += target_mean return level nbands = len ( noise_pyramid [ 0 ][ \"b\" ]) noise_pyramid [ 0 ][ \"h\" ] = match_level ( noise_pyramid [ 0 ][ \"h\" ], target_means [ 0 ], target_stdevs [ 0 ]) for l in range ( len ( noise_pyramid ) - 1 ): for b in range ( nbands ): noise_pyramid [ l ][ \"b\" ][ b ] = match_level ( noise_pyramid [ l ][ \"b\" ][ b ], target_means [ 1 + l * nbands + b ], target_stdevs [ 1 + l * nbands + b ]) noise_pyramid [ - 1 ][ \"l\" ] = input_pyramid [ - 1 ][ \"l\" ] metamer = self . metameric_loss . pyramid_maker . reconstruct_from_pyramid ( noise_pyramid ) metamer = ycrcb_2_rgb ( metamer ) return metamer","title":"gen_metamer()"},{"location":"odak/learn/perception/metamer_mse_loss/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/learn/perception/metameric_loss/","text":"Metameric Loss \u00b6 The MetamericLoss class provides a perceptual loss function. Rather than exactly match the source image to the target, it tries to ensure the source is a metamer to the target image. Its interface is similar to other pytorch loss functions, but note that the gaze location must be provided in addition to the source and target images. __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ], image_colorspace = 'RGB' , visualise_loss = False ) special \u00b6 Calculates the Metameric Loss. Parameters: Name Type Description Default image torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required target torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required image_colorspace str The current colorspace of your image and target. Ignored if input does not have 3 channels. accepted values: RGB, YCrCb. 'RGB' gaze list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. [0.5, 0.5] visualise_loss bool Shows a heatmap indicating which parts of the image contributed most to the loss. False Returns: Type Description torch.tensor The computed loss. Source code in odak/learn/perception/metameric_loss.py def __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ], image_colorspace = \"RGB\" , visualise_loss = False ): \"\"\" Calculates the Metameric Loss. Parameters ---------- image : torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) target : torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) image_colorspace : str The current colorspace of your image and target. Ignored if input does not have 3 channels. accepted values: RGB, YCrCb. gaze : list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. visualise_loss : bool Shows a heatmap indicating which parts of the image contributed most to the loss. Returns ------- loss : torch.tensor The computed loss. \"\"\" # TODO if input is RGB, convert to YCrCb. if image . size ( 1 ) != target . size ( 1 ): raise Exception ( \"MetamericLoss ERROR: Input and target must have same number of channels.\" ) # Pad image and target if necessary min_divisor = 2 ** self . n_pyramid_levels height = image . size ( 2 ) width = image . size ( 3 ) required_height = math . ceil ( height / min_divisor ) * min_divisor required_width = math . ceil ( width / min_divisor ) * min_divisor if required_height > height or required_width > width : # We need to pad! pad = torch . nn . ReflectionPad2d ( ( 0 , 0 , required_height - height , required_width - width )) image = pad ( image ) target = pad ( target ) if image . size ( 1 ) == 3 and image_colorspace == \"RGB\" : image = rgb_2_ycrcb ( image ) target = rgb_2_ycrcb ( target ) if self . target is None : self . target = torch . zeros ( target . shape ) . to ( target . device ) if type ( target ) == type ( self . target ): if not torch . all ( torch . eq ( target , self . target )): self . target = target . detach () . clone () self . target_stats = self . calc_statsmaps ( self . target , gaze = gaze , alpha = self . alpha , real_image_width = self . real_image_width , real_viewing_distance = self . real_viewing_distance , mode = self . mode ) self . target = target . detach () . clone () image_stats = self . calc_statsmaps ( image , gaze = gaze , alpha = self . alpha , real_image_width = self . real_image_width , real_viewing_distance = self . real_viewing_distance , mode = self . mode ) if visualise_loss : self . visualise_loss_map ( image_stats ) if self . use_l2_foveal_loss : peripheral_loss = self . metameric_loss_stats ( image_stats , self . target_stats , gaze ) foveal_loss = torch . nn . MSELoss ()( self . fovea_mask * image , self . fovea_mask * target ) # New weighting - evenly weight fovea and periphery. loss = peripheral_loss + self . fovea_weight * foveal_loss else : loss = self . metameric_loss_stats ( image_stats , self . target_stats , gaze ) return loss else : raise Exception ( \"Target of incorrect type\" ) __init__ ( self , device = device ( type = 'cpu' ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , n_pyramid_levels = 5 , mode = 'quadratic' , n_orientations = 2 , use_l2_foveal_loss = True , fovea_weight = 20.0 , use_radial_weight = False , use_fullres_l0 = False ) special \u00b6 Parameters: Name Type Description Default alpha float parameter controlling foveation - larger values mean bigger pooling regions. 0.08 real_image_width float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. 0.2 real_viewing_distance float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. 0.7 n_pyramid_levels int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. 5 mode str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". 'quadratic' n_orientations int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. 2 use_l2_foveal_loss bool If true, for all the pixels that have pooling size 1 pixel in the largest scale will use direct L2 against target rather than pooling over pyramid levels. In practice this gives better results when the loss is used for holography. True fovea_weight float A weight to apply to the foveal region if use_l2_foveal_loss is set to True. 20.0 use_radial_weight bool If True, will apply a radial weighting when calculating the difference between the source and target stats maps. This weights stats closer to the fovea more than those further away. False use_fullres_l0 bool If true, stats for the lowpass residual are replaced with blurred versions of the full-resolution source and target images. False Source code in odak/learn/perception/metameric_loss.py def __init__ ( self , device = torch . device ( 'cpu' ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , n_pyramid_levels = 5 , mode = \"quadratic\" , n_orientations = 2 , use_l2_foveal_loss = True , fovea_weight = 20.0 , use_radial_weight = False , use_fullres_l0 = False ): \"\"\" Parameters ---------- alpha : float parameter controlling foveation - larger values mean bigger pooling regions. real_image_width : float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. real_viewing_distance : float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. n_pyramid_levels : int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. mode : str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". n_orientations : int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. use_l2_foveal_loss : bool If true, for all the pixels that have pooling size 1 pixel in the largest scale will use direct L2 against target rather than pooling over pyramid levels. In practice this gives better results when the loss is used for holography. fovea_weight : float A weight to apply to the foveal region if use_l2_foveal_loss is set to True. use_radial_weight : bool If True, will apply a radial weighting when calculating the difference between the source and target stats maps. This weights stats closer to the fovea more than those further away. use_fullres_l0 : bool If true, stats for the lowpass residual are replaced with blurred versions of the full-resolution source and target images. \"\"\" self . target = None self . device = device self . pyramid_maker = None self . alpha = alpha self . real_image_width = real_image_width self . real_viewing_distance = real_viewing_distance self . blurs = None self . n_pyramid_levels = n_pyramid_levels self . n_orientations = n_orientations self . mode = mode self . use_l2_foveal_loss = use_l2_foveal_loss self . fovea_weight = fovea_weight self . use_radial_weight = use_radial_weight self . use_fullres_l0 = use_fullres_l0 if self . use_fullres_l0 and self . use_l2_foveal_loss : raise Exception ( \"Can't use use_fullres_l0 and use_l2_foveal_loss options together in MetamericLoss!\" ) See also \u00b6 Visual perception","title":"Metameric Loss"},{"location":"odak/learn/perception/metameric_loss/#metameric-loss","text":"The MetamericLoss class provides a perceptual loss function. Rather than exactly match the source image to the target, it tries to ensure the source is a metamer to the target image. Its interface is similar to other pytorch loss functions, but note that the gaze location must be provided in addition to the source and target images.","title":"Metameric Loss"},{"location":"odak/learn/perception/metameric_loss/#odak.learn.perception.metameric_loss.MetamericLoss.__call__","text":"Calculates the Metameric Loss. Parameters: Name Type Description Default image torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required target torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required image_colorspace str The current colorspace of your image and target. Ignored if input does not have 3 channels. accepted values: RGB, YCrCb. 'RGB' gaze list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. [0.5, 0.5] visualise_loss bool Shows a heatmap indicating which parts of the image contributed most to the loss. False Returns: Type Description torch.tensor The computed loss. Source code in odak/learn/perception/metameric_loss.py def __call__ ( self , image , target , gaze = [ 0.5 , 0.5 ], image_colorspace = \"RGB\" , visualise_loss = False ): \"\"\" Calculates the Metameric Loss. Parameters ---------- image : torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) target : torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) image_colorspace : str The current colorspace of your image and target. Ignored if input does not have 3 channels. accepted values: RGB, YCrCb. gaze : list Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image. visualise_loss : bool Shows a heatmap indicating which parts of the image contributed most to the loss. Returns ------- loss : torch.tensor The computed loss. \"\"\" # TODO if input is RGB, convert to YCrCb. if image . size ( 1 ) != target . size ( 1 ): raise Exception ( \"MetamericLoss ERROR: Input and target must have same number of channels.\" ) # Pad image and target if necessary min_divisor = 2 ** self . n_pyramid_levels height = image . size ( 2 ) width = image . size ( 3 ) required_height = math . ceil ( height / min_divisor ) * min_divisor required_width = math . ceil ( width / min_divisor ) * min_divisor if required_height > height or required_width > width : # We need to pad! pad = torch . nn . ReflectionPad2d ( ( 0 , 0 , required_height - height , required_width - width )) image = pad ( image ) target = pad ( target ) if image . size ( 1 ) == 3 and image_colorspace == \"RGB\" : image = rgb_2_ycrcb ( image ) target = rgb_2_ycrcb ( target ) if self . target is None : self . target = torch . zeros ( target . shape ) . to ( target . device ) if type ( target ) == type ( self . target ): if not torch . all ( torch . eq ( target , self . target )): self . target = target . detach () . clone () self . target_stats = self . calc_statsmaps ( self . target , gaze = gaze , alpha = self . alpha , real_image_width = self . real_image_width , real_viewing_distance = self . real_viewing_distance , mode = self . mode ) self . target = target . detach () . clone () image_stats = self . calc_statsmaps ( image , gaze = gaze , alpha = self . alpha , real_image_width = self . real_image_width , real_viewing_distance = self . real_viewing_distance , mode = self . mode ) if visualise_loss : self . visualise_loss_map ( image_stats ) if self . use_l2_foveal_loss : peripheral_loss = self . metameric_loss_stats ( image_stats , self . target_stats , gaze ) foveal_loss = torch . nn . MSELoss ()( self . fovea_mask * image , self . fovea_mask * target ) # New weighting - evenly weight fovea and periphery. loss = peripheral_loss + self . fovea_weight * foveal_loss else : loss = self . metameric_loss_stats ( image_stats , self . target_stats , gaze ) return loss else : raise Exception ( \"Target of incorrect type\" )","title":"__call__()"},{"location":"odak/learn/perception/metameric_loss/#odak.learn.perception.metameric_loss.MetamericLoss.__init__","text":"Parameters: Name Type Description Default alpha float parameter controlling foveation - larger values mean bigger pooling regions. 0.08 real_image_width float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. 0.2 real_viewing_distance float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. 0.7 n_pyramid_levels int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. 5 mode str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". 'quadratic' n_orientations int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. 2 use_l2_foveal_loss bool If true, for all the pixels that have pooling size 1 pixel in the largest scale will use direct L2 against target rather than pooling over pyramid levels. In practice this gives better results when the loss is used for holography. True fovea_weight float A weight to apply to the foveal region if use_l2_foveal_loss is set to True. 20.0 use_radial_weight bool If True, will apply a radial weighting when calculating the difference between the source and target stats maps. This weights stats closer to the fovea more than those further away. False use_fullres_l0 bool If true, stats for the lowpass residual are replaced with blurred versions of the full-resolution source and target images. False Source code in odak/learn/perception/metameric_loss.py def __init__ ( self , device = torch . device ( 'cpu' ), alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , n_pyramid_levels = 5 , mode = \"quadratic\" , n_orientations = 2 , use_l2_foveal_loss = True , fovea_weight = 20.0 , use_radial_weight = False , use_fullres_l0 = False ): \"\"\" Parameters ---------- alpha : float parameter controlling foveation - larger values mean bigger pooling regions. real_image_width : float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. real_viewing_distance : float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. n_pyramid_levels : int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. mode : str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". n_orientations : int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. use_l2_foveal_loss : bool If true, for all the pixels that have pooling size 1 pixel in the largest scale will use direct L2 against target rather than pooling over pyramid levels. In practice this gives better results when the loss is used for holography. fovea_weight : float A weight to apply to the foveal region if use_l2_foveal_loss is set to True. use_radial_weight : bool If True, will apply a radial weighting when calculating the difference between the source and target stats maps. This weights stats closer to the fovea more than those further away. use_fullres_l0 : bool If true, stats for the lowpass residual are replaced with blurred versions of the full-resolution source and target images. \"\"\" self . target = None self . device = device self . pyramid_maker = None self . alpha = alpha self . real_image_width = real_image_width self . real_viewing_distance = real_viewing_distance self . blurs = None self . n_pyramid_levels = n_pyramid_levels self . n_orientations = n_orientations self . mode = mode self . use_l2_foveal_loss = use_l2_foveal_loss self . fovea_weight = fovea_weight self . use_radial_weight = use_radial_weight self . use_fullres_l0 = use_fullres_l0 if self . use_fullres_l0 and self . use_l2_foveal_loss : raise Exception ( \"Can't use use_fullres_l0 and use_l2_foveal_loss options together in MetamericLoss!\" )","title":"__init__()"},{"location":"odak/learn/perception/metameric_loss/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/learn/perception/metameric_loss_uniform/","text":"Metameric Loss (Uniform) \u00b6 Measures metameric loss between a given image and a metamer of the given target image. This variant of the metameric loss is not foveated - it applies uniform pooling sizes to the whole input image. __call__ ( self , image , target , image_colorspace = 'RGB' , visualise_loss = False ) special \u00b6 Calculates the Metameric Loss. Parameters: Name Type Description Default image torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required target torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required image_colorspace str The current colorspace of your image and target. Ignored if input does not have 3 channels. accepted values: RGB, YCrCb. 'RGB' visualise_loss bool Shows a heatmap indicating which parts of the image contributed most to the loss. False Returns: Type Description torch.tensor The computed loss. Source code in odak/learn/perception/metameric_loss_uniform.py def __call__ ( self , image , target , image_colorspace = \"RGB\" , visualise_loss = False ): \"\"\" Calculates the Metameric Loss. Parameters ---------- image : torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) target : torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) image_colorspace : str The current colorspace of your image and target. Ignored if input does not have 3 channels. accepted values: RGB, YCrCb. visualise_loss : bool Shows a heatmap indicating which parts of the image contributed most to the loss. Returns ------- loss : torch.tensor The computed loss. \"\"\" # TODO if input is RGB, convert to YCrCb. if image . size ( 1 ) != target . size ( 1 ): raise Exception ( \"MetamericLoss ERROR: Input and target must have same number of channels.\" ) # Pad image and target if necessary min_divisor = 2 ** self . n_pyramid_levels height = image . size ( 2 ) width = image . size ( 3 ) required_height = math . ceil ( height / min_divisor ) * min_divisor required_width = math . ceil ( width / min_divisor ) * min_divisor if required_height > height or required_width > width : # We need to pad! pad = torch . nn . ReflectionPad2d ( ( 0 , 0 , required_height - height , required_width - width )) image = pad ( image ) target = pad ( target ) if image . size ( 1 ) == 3 and image_colorspace == \"RGB\" : image = rgb_2_ycrcb ( image ) target = rgb_2_ycrcb ( target ) if self . target is None : self . target = torch . zeros ( target . shape ) . to ( target . device ) if type ( target ) == type ( self . target ): if not torch . all ( torch . eq ( target , self . target )): self . target = target . detach () . clone () self . target_stats = self . calc_statsmaps ( self . target , self . pooling_size ) self . target = target . detach () . clone () image_stats = self . calc_statsmaps ( image , self . pooling_size ) if visualise_loss : self . visualise_loss_map ( image_stats ) loss = self . metameric_loss_stats ( image_stats , self . target_stats ) return loss else : raise Exception ( \"Target of incorrect type\" ) __init__ ( self , device = device ( type = 'cpu' ), pooling_size = 32 , n_pyramid_levels = 5 , n_orientations = 2 ) special \u00b6 Parameters: Name Type Description Default pooling_size int Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image. 32 n_pyramid_levels int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. 5 n_orientations int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. 2 Source code in odak/learn/perception/metameric_loss_uniform.py def __init__ ( self , device = torch . device ( 'cpu' ), pooling_size = 32 , n_pyramid_levels = 5 , n_orientations = 2 ): \"\"\" Parameters ---------- pooling_size : int Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image. n_pyramid_levels : int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. n_orientations : int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. \"\"\" self . target = None self . device = device self . pyramid_maker = None self . pooling_size = pooling_size self . n_pyramid_levels = n_pyramid_levels self . n_orientations = n_orientations See also \u00b6 Visual perception","title":"Metameric Loss (Uniform)"},{"location":"odak/learn/perception/metameric_loss_uniform/#metameric-loss-uniform","text":"Measures metameric loss between a given image and a metamer of the given target image. This variant of the metameric loss is not foveated - it applies uniform pooling sizes to the whole input image.","title":"Metameric Loss (Uniform)"},{"location":"odak/learn/perception/metameric_loss_uniform/#odak.learn.perception.metameric_loss_uniform.MetamericLossUniform.__call__","text":"Calculates the Metameric Loss. Parameters: Name Type Description Default image torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required target torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) required image_colorspace str The current colorspace of your image and target. Ignored if input does not have 3 channels. accepted values: RGB, YCrCb. 'RGB' visualise_loss bool Shows a heatmap indicating which parts of the image contributed most to the loss. False Returns: Type Description torch.tensor The computed loss. Source code in odak/learn/perception/metameric_loss_uniform.py def __call__ ( self , image , target , image_colorspace = \"RGB\" , visualise_loss = False ): \"\"\" Calculates the Metameric Loss. Parameters ---------- image : torch.tensor Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) target : torch.tensor Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions) image_colorspace : str The current colorspace of your image and target. Ignored if input does not have 3 channels. accepted values: RGB, YCrCb. visualise_loss : bool Shows a heatmap indicating which parts of the image contributed most to the loss. Returns ------- loss : torch.tensor The computed loss. \"\"\" # TODO if input is RGB, convert to YCrCb. if image . size ( 1 ) != target . size ( 1 ): raise Exception ( \"MetamericLoss ERROR: Input and target must have same number of channels.\" ) # Pad image and target if necessary min_divisor = 2 ** self . n_pyramid_levels height = image . size ( 2 ) width = image . size ( 3 ) required_height = math . ceil ( height / min_divisor ) * min_divisor required_width = math . ceil ( width / min_divisor ) * min_divisor if required_height > height or required_width > width : # We need to pad! pad = torch . nn . ReflectionPad2d ( ( 0 , 0 , required_height - height , required_width - width )) image = pad ( image ) target = pad ( target ) if image . size ( 1 ) == 3 and image_colorspace == \"RGB\" : image = rgb_2_ycrcb ( image ) target = rgb_2_ycrcb ( target ) if self . target is None : self . target = torch . zeros ( target . shape ) . to ( target . device ) if type ( target ) == type ( self . target ): if not torch . all ( torch . eq ( target , self . target )): self . target = target . detach () . clone () self . target_stats = self . calc_statsmaps ( self . target , self . pooling_size ) self . target = target . detach () . clone () image_stats = self . calc_statsmaps ( image , self . pooling_size ) if visualise_loss : self . visualise_loss_map ( image_stats ) loss = self . metameric_loss_stats ( image_stats , self . target_stats ) return loss else : raise Exception ( \"Target of incorrect type\" )","title":"__call__()"},{"location":"odak/learn/perception/metameric_loss_uniform/#odak.learn.perception.metameric_loss_uniform.MetamericLossUniform.__init__","text":"Parameters: Name Type Description Default pooling_size int Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image. 32 n_pyramid_levels int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. 5 n_orientations int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. 2 Source code in odak/learn/perception/metameric_loss_uniform.py def __init__ ( self , device = torch . device ( 'cpu' ), pooling_size = 32 , n_pyramid_levels = 5 , n_orientations = 2 ): \"\"\" Parameters ---------- pooling_size : int Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image. n_pyramid_levels : int Number of levels of the steerable pyramid. Note that the image is padded so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value too high will slow down the calculation a lot. n_orientations : int Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6. Increasing this will increase runtime. \"\"\" self . target = None self . device = device self . pyramid_maker = None self . pooling_size = pooling_size self . n_pyramid_levels = n_pyramid_levels self . n_orientations = n_orientations","title":"__init__()"},{"location":"odak/learn/perception/metameric_loss_uniform/#see-also","text":"Visual perception","title":"See also"},{"location":"odak/learn/perception/radially_varying_blur/","text":"Radially Varying Blur \u00b6 The RadiallyVaryingBlur class provides a way to apply a radially varying blur to an image. Given a gaze location and information about the image and foveation, it applies a blur that will achieve the proper pooling size. The pooling size is chosen to appear the same at a range of display sizes and viewing distances, for a given alpha parameter value. For more information on how the pooling sizes are computed, please see link coming soon . The blur is accelerated by generating and sampling from MIP maps of the input image. This class caches the foveation information. This means that if it is run repeatedly with the same foveation parameters, gaze location and image size (e.g. in an optimisation loop) it won't recalculate the pooling maps. If you are repeatedly applying blur to images of different sizes (e.g. a pyramid) for best performance use one instance of this class per image size. blur ( self , image , alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , centre = None , mode = 'quadratic' ) \u00b6 Apply the radially varying blur to an image. Parameters: Name Type Description Default image torch.tensor The image to blur, in NCHW format. required alpha float parameter controlling foveation - larger values mean bigger pooling regions. 0.08 real_image_width float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. 0.2 real_viewing_distance float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. 0.7 centre tuple of floats The centre of the radially varying blur (the gaze location). Should be a tuple of floats containing normalised image coordinates in range [0,1] None mode str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". 'quadratic' Returns: Type Description torch.tensor The blurred image Source code in odak/learn/perception/radially_varying_blur.py def blur ( self , image , alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , centre = None , mode = \"quadratic\" ): \"\"\" Apply the radially varying blur to an image. Parameters ---------- image : torch.tensor The image to blur, in NCHW format. alpha : float parameter controlling foveation - larger values mean bigger pooling regions. real_image_width : float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. real_viewing_distance : float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. centre : tuple of floats The centre of the radially varying blur (the gaze location). Should be a tuple of floats containing normalised image coordinates in range [0,1] mode : str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". Returns ------- output : torch.tensor The blurred image \"\"\" size = ( image . size ( - 2 ), image . size ( - 1 )) # LOD map caching if self . lod_map is None or \\ self . size != size or \\ self . n_channels != image . size ( 1 ) or \\ self . alpha != alpha or \\ self . real_image_width != real_image_width or \\ self . real_viewing_distance != real_viewing_distance or \\ self . centre != centre or \\ self . mode != mode : self . lod_map = make_pooling_size_map_lod ( centre , ( image . size ( - 2 ), image . size ( - 1 )), alpha , real_image_width , real_viewing_distance , mode ) self . size = size self . n_channels = image . size ( 1 ) self . alpha = alpha self . real_image_width = real_image_width self . real_viewing_distance = real_viewing_distance self . centre = centre self . lod_map = self . lod_map . to ( image . device ) self . lod_fraction = torch . fmod ( self . lod_map , 1.0 ) self . lod_fraction = self . lod_fraction [ None , None , ... ] . repeat ( 1 , image . size ( 1 ), 1 , 1 ) self . mode = mode if self . lod_map . device != image . device : self . lod_map = self . lod_map . to ( image . device ) if self . lod_fraction . device != image . device : self . lod_fraction = self . lod_fraction . to ( image . device ) mipmap = [ image ] while mipmap [ - 1 ] . size ( - 1 ) > 1 and mipmap [ - 1 ] . size ( - 2 ) > 1 : mipmap . append ( torch . nn . functional . interpolate ( mipmap [ - 1 ], scale_factor = 0.5 , mode = \"area\" , recompute_scale_factor = False )) if mipmap [ - 1 ] . size ( - 1 ) == 2 : final_mip = torch . mean ( mipmap [ - 1 ], axis =- 1 )[ ... , None ] mipmap . append ( final_mip ) if mipmap [ - 1 ] . size ( - 2 ) == 2 : final_mip = torch . mean ( mipmap [ - 2 ], axis =- 2 )[ ... , None , :] mipmap . append ( final_mip ) for l in range ( len ( mipmap )): if l == len ( mipmap ) - 1 : mipmap [ l ] = mipmap [ l ] * \\ torch . ones ( image . size (), device = image . device ) else : for l2 in range ( l - 1 , - 1 , - 1 ): mipmap [ l ] = torch . nn . functional . interpolate ( mipmap [ l ], size = ( image . size ( - 2 ), image . size ( - 1 )), mode = \"bilinear\" , align_corners = False , recompute_scale_factor = False ) output = torch . zeros ( image . size (), device = image . device ) for l in range ( len ( mipmap )): if l == 0 : mask = self . lod_map < ( l + 1 ) elif l == len ( mipmap ) - 1 : mask = self . lod_map >= l else : mask = torch . logical_and ( self . lod_map >= l , self . lod_map < ( l + 1 )) if l == len ( mipmap ) - 1 : blended_levels = mipmap [ l ] else : blended_levels = ( 1 - self . lod_fraction ) * \\ mipmap [ l ] + self . lod_fraction * mipmap [ l + 1 ] mask = mask [ None , None , ... ] mask = mask . repeat ( 1 , image . size ( 1 ), 1 , 1 ) output [ mask ] = blended_levels [ mask ] return output See also \u00b6 Perception","title":"Radially Varying Blur"},{"location":"odak/learn/perception/radially_varying_blur/#radially-varying-blur","text":"The RadiallyVaryingBlur class provides a way to apply a radially varying blur to an image. Given a gaze location and information about the image and foveation, it applies a blur that will achieve the proper pooling size. The pooling size is chosen to appear the same at a range of display sizes and viewing distances, for a given alpha parameter value. For more information on how the pooling sizes are computed, please see link coming soon . The blur is accelerated by generating and sampling from MIP maps of the input image. This class caches the foveation information. This means that if it is run repeatedly with the same foveation parameters, gaze location and image size (e.g. in an optimisation loop) it won't recalculate the pooling maps. If you are repeatedly applying blur to images of different sizes (e.g. a pyramid) for best performance use one instance of this class per image size.","title":"Radially Varying Blur"},{"location":"odak/learn/perception/radially_varying_blur/#odak.learn.perception.radially_varying_blur.RadiallyVaryingBlur.blur","text":"Apply the radially varying blur to an image. Parameters: Name Type Description Default image torch.tensor The image to blur, in NCHW format. required alpha float parameter controlling foveation - larger values mean bigger pooling regions. 0.08 real_image_width float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. 0.2 real_viewing_distance float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. 0.7 centre tuple of floats The centre of the radially varying blur (the gaze location). Should be a tuple of floats containing normalised image coordinates in range [0,1] None mode str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". 'quadratic' Returns: Type Description torch.tensor The blurred image Source code in odak/learn/perception/radially_varying_blur.py def blur ( self , image , alpha = 0.08 , real_image_width = 0.2 , real_viewing_distance = 0.7 , centre = None , mode = \"quadratic\" ): \"\"\" Apply the radially varying blur to an image. Parameters ---------- image : torch.tensor The image to blur, in NCHW format. alpha : float parameter controlling foveation - larger values mean bigger pooling regions. real_image_width : float The real width of the image as displayed to the user. Units don't matter as long as they are the same as for real_viewing_distance. real_viewing_distance : float The real distance of the observer's eyes to the image plane. Units don't matter as long as they are the same as for real_image_width. centre : tuple of floats The centre of the radially varying blur (the gaze location). Should be a tuple of floats containing normalised image coordinates in range [0,1] mode : str Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow as you move away from the fovea. We got best results with \"quadratic\". Returns ------- output : torch.tensor The blurred image \"\"\" size = ( image . size ( - 2 ), image . size ( - 1 )) # LOD map caching if self . lod_map is None or \\ self . size != size or \\ self . n_channels != image . size ( 1 ) or \\ self . alpha != alpha or \\ self . real_image_width != real_image_width or \\ self . real_viewing_distance != real_viewing_distance or \\ self . centre != centre or \\ self . mode != mode : self . lod_map = make_pooling_size_map_lod ( centre , ( image . size ( - 2 ), image . size ( - 1 )), alpha , real_image_width , real_viewing_distance , mode ) self . size = size self . n_channels = image . size ( 1 ) self . alpha = alpha self . real_image_width = real_image_width self . real_viewing_distance = real_viewing_distance self . centre = centre self . lod_map = self . lod_map . to ( image . device ) self . lod_fraction = torch . fmod ( self . lod_map , 1.0 ) self . lod_fraction = self . lod_fraction [ None , None , ... ] . repeat ( 1 , image . size ( 1 ), 1 , 1 ) self . mode = mode if self . lod_map . device != image . device : self . lod_map = self . lod_map . to ( image . device ) if self . lod_fraction . device != image . device : self . lod_fraction = self . lod_fraction . to ( image . device ) mipmap = [ image ] while mipmap [ - 1 ] . size ( - 1 ) > 1 and mipmap [ - 1 ] . size ( - 2 ) > 1 : mipmap . append ( torch . nn . functional . interpolate ( mipmap [ - 1 ], scale_factor = 0.5 , mode = \"area\" , recompute_scale_factor = False )) if mipmap [ - 1 ] . size ( - 1 ) == 2 : final_mip = torch . mean ( mipmap [ - 1 ], axis =- 1 )[ ... , None ] mipmap . append ( final_mip ) if mipmap [ - 1 ] . size ( - 2 ) == 2 : final_mip = torch . mean ( mipmap [ - 2 ], axis =- 2 )[ ... , None , :] mipmap . append ( final_mip ) for l in range ( len ( mipmap )): if l == len ( mipmap ) - 1 : mipmap [ l ] = mipmap [ l ] * \\ torch . ones ( image . size (), device = image . device ) else : for l2 in range ( l - 1 , - 1 , - 1 ): mipmap [ l ] = torch . nn . functional . interpolate ( mipmap [ l ], size = ( image . size ( - 2 ), image . size ( - 1 )), mode = \"bilinear\" , align_corners = False , recompute_scale_factor = False ) output = torch . zeros ( image . size (), device = image . device ) for l in range ( len ( mipmap )): if l == 0 : mask = self . lod_map < ( l + 1 ) elif l == len ( mipmap ) - 1 : mask = self . lod_map >= l else : mask = torch . logical_and ( self . lod_map >= l , self . lod_map < ( l + 1 )) if l == len ( mipmap ) - 1 : blended_levels = mipmap [ l ] else : blended_levels = ( 1 - self . lod_fraction ) * \\ mipmap [ l ] + self . lod_fraction * mipmap [ l + 1 ] mask = mask [ None , None , ... ] mask = mask . repeat ( 1 , image . size ( 1 ), 1 , 1 ) output [ mask ] = blended_levels [ mask ] return output","title":"blur()"},{"location":"odak/learn/perception/radially_varying_blur/#see-also","text":"Perception","title":"See also"},{"location":"odak/learn/perception/rgb_2_ycrcb/","text":"odak.learn.perception.rgb_2_ycrcb \u00b6 Converts an image from RGB colourspace to YCrCb colourspace. Parameters: Name Type Description Default image torch.tensor Input image. Should be an RGB floating-point image with values in the range [0, 1] Should be in NCHW format. required Returns: Type Description torch.tensor Image converted to YCrCb colourspace. Source code in odak/learn/perception/color_conversion.py def rgb_2_ycrcb ( image ): \"\"\" Converts an image from RGB colourspace to YCrCb colourspace. Parameters ---------- image : torch.tensor Input image. Should be an RGB floating-point image with values in the range [0, 1] Should be in NCHW format. Returns ------- ycrcb : torch.tensor Image converted to YCrCb colourspace. \"\"\" ycrcb = torch . zeros ( image . size ()) . to ( image . device ) ycrcb [:, 0 , :, :] = 0.299 * image [:, 0 , :, :] + 0.587 * \\ image [:, 1 , :, :] + 0.114 * image [:, 2 , :, :] ycrcb [:, 1 , :, :] = 0.5 + 0.713 * ( image [:, 0 , :, :] - ycrcb [:, 0 , :, :]) ycrcb [:, 2 , :, :] = 0.5 + 0.564 * ( image [:, 2 , :, :] - ycrcb [:, 0 , :, :]) return ycrcb See also \u00b6 ycrcb_2_rgb()","title":"odak.learn.perception.rgb_2_ycrcb"},{"location":"odak/learn/perception/rgb_2_ycrcb/#odaklearnperceptionrgb_2_ycrcb","text":"Converts an image from RGB colourspace to YCrCb colourspace. Parameters: Name Type Description Default image torch.tensor Input image. Should be an RGB floating-point image with values in the range [0, 1] Should be in NCHW format. required Returns: Type Description torch.tensor Image converted to YCrCb colourspace. Source code in odak/learn/perception/color_conversion.py def rgb_2_ycrcb ( image ): \"\"\" Converts an image from RGB colourspace to YCrCb colourspace. Parameters ---------- image : torch.tensor Input image. Should be an RGB floating-point image with values in the range [0, 1] Should be in NCHW format. Returns ------- ycrcb : torch.tensor Image converted to YCrCb colourspace. \"\"\" ycrcb = torch . zeros ( image . size ()) . to ( image . device ) ycrcb [:, 0 , :, :] = 0.299 * image [:, 0 , :, :] + 0.587 * \\ image [:, 1 , :, :] + 0.114 * image [:, 2 , :, :] ycrcb [:, 1 , :, :] = 0.5 + 0.713 * ( image [:, 0 , :, :] - ycrcb [:, 0 , :, :]) ycrcb [:, 2 , :, :] = 0.5 + 0.564 * ( image [:, 2 , :, :] - ycrcb [:, 0 , :, :]) return ycrcb","title":"odak.learn.perception.rgb_2_ycrcb"},{"location":"odak/learn/perception/rgb_2_ycrcb/#see-also","text":"ycrcb_2_rgb()","title":"See also"},{"location":"odak/learn/perception/spatial_steerable_pyramid/","text":"odak.learn.perception.SpatialSteerablePyramid \u00b6 This implements a real-valued steerable pyramid where the filtering is carried out spatially (using convolution) as opposed to multiplication in the Fourier domain. This has a number of optimisations over previous implementations that increase efficiency, but introduce some reconstruction error. __init__ ( self , use_bilinear_downup = True , n_channels = 1 , filter_size = 9 , n_orientations = 6 , filter_type = 'full' , device = device ( type = 'cpu' )) special \u00b6 Parameters: Name Type Description Default use_bilinear_downup bool This uses bilinear filtering when upsampling/downsampling, rather than the original approach of applying a large lowpass kernel and sampling even rows/columns True n_channels int Number of channels in the input images (e.g. 3 for RGB input) 1 filter_size int Desired size of filters (e.g. 3 will use 3x3 filters). 9 n_orientations int Number of oriented bands in each level of the pyramid. 6 filter_type str This can be used to select smaller filters than the original ones if desired. full: Original filter sizes cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate. trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels. 'full' device torch.device torch device the input images will be supplied from. device(type='cpu') Source code in odak/learn/perception/spatial_steerable_pyramid.py def __init__ ( self , use_bilinear_downup = True , n_channels = 1 , filter_size = 9 , n_orientations = 6 , filter_type = \"full\" , device = torch . device ( 'cpu' )): \"\"\" Parameters ---------- use_bilinear_downup : bool This uses bilinear filtering when upsampling/downsampling, rather than the original approach of applying a large lowpass kernel and sampling even rows/columns n_channels : int Number of channels in the input images (e.g. 3 for RGB input) filter_size : int Desired size of filters (e.g. 3 will use 3x3 filters). n_orientations : int Number of oriented bands in each level of the pyramid. filter_type : str This can be used to select smaller filters than the original ones if desired. full: Original filter sizes cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate. trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels. device : torch.device torch device the input images will be supplied from. \"\"\" self . use_bilinear_downup = use_bilinear_downup self . device = device filters = get_steerable_pyramid_filters ( filter_size , n_orientations , filter_type ) def make_pad ( filter ): filter_size = filter . size ( - 1 ) pad_amt = ( filter_size - 1 ) // 2 return torch . nn . ReflectionPad2d (( pad_amt , pad_amt , pad_amt , pad_amt )) if not self . use_bilinear_downup : self . filt_l = filters [ \"l\" ] . to ( device ) self . pad_l = make_pad ( self . filt_l ) self . filt_l0 = filters [ \"l0\" ] . to ( device ) self . pad_l0 = make_pad ( self . filt_l0 ) self . filt_h0 = filters [ \"h0\" ] . to ( device ) self . pad_h0 = make_pad ( self . filt_h0 ) for b in range ( len ( filters [ \"b\" ])): filters [ \"b\" ][ b ] = filters [ \"b\" ][ b ] . to ( device ) self . band_filters = filters [ \"b\" ] self . pad_b = make_pad ( self . band_filters [ 0 ]) if n_channels != 1 : def add_channels_to_filter ( filter ): padded = torch . zeros ( n_channels , n_channels , filter . size ()[ 2 ], filter . size ()[ 3 ]) . to ( device ) for channel in range ( n_channels ): padded [ channel , channel , :, :] = filter return padded self . filt_h0 = add_channels_to_filter ( self . filt_h0 ) for b in range ( len ( self . band_filters )): self . band_filters [ b ] = add_channels_to_filter ( self . band_filters [ b ]) self . filt_l0 = add_channels_to_filter ( self . filt_l0 ) if not self . use_bilinear_downup : self . filt_l = add_channels_to_filter ( self . filt_l ) construct_pyramid ( self , image , n_levels , multiple_highpass = False ) \u00b6 Constructs and returns a steerable pyramid for the provided image. Parameters: Name Type Description Default image torch.tensor The input image, in NCHW format. The number of channels C should match num_channels when the pyramid maker was created. required n_levels int Number of levels in the constructed steerable pyramid. required multiple_highpass bool If true, computes a highpass for each level of the pyramid. These extra levels are redundant (not used for reconstruction). False Returns: Type Description list of dicts of torch.tensor The computed steerable pyramid. Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels. Each level is stored as a dict, with the following keys: \"h\" Highpass residual \"l\" Lowpass residual \"b\" Oriented bands (a list of torch.tensor) Source code in odak/learn/perception/spatial_steerable_pyramid.py def construct_pyramid ( self , image , n_levels , multiple_highpass = False ): \"\"\" Constructs and returns a steerable pyramid for the provided image. Parameters ---------- image : torch.tensor The input image, in NCHW format. The number of channels C should match num_channels when the pyramid maker was created. n_levels : int Number of levels in the constructed steerable pyramid. multiple_highpass : bool If true, computes a highpass for each level of the pyramid. These extra levels are redundant (not used for reconstruction). Returns ------- pyramid : list of dicts of torch.tensor The computed steerable pyramid. Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels. Each level is stored as a dict, with the following keys: \"h\" Highpass residual \"l\" Lowpass residual \"b\" Oriented bands (a list of torch.tensor) \"\"\" pyramid = [] # Make level 0, containing highpass, lowpass and the bands level0 = {} level0 [ 'h' ] = torch . nn . functional . conv2d ( self . pad_h0 ( image ), self . filt_h0 ) #plt.imshow(level0['h'][0,0,...], cmap=\"gray\", vmin=0, vmax=1) # plt.show() lowpass = torch . nn . functional . conv2d ( self . pad_l0 ( image ), self . filt_l0 ) level0 [ 'l' ] = lowpass . clone () #np.save(\"lowpass_filtered.npy\", level0['l'][0,...].permute(1,2,0).numpy()) # quit() bands = [] for filt_b in self . band_filters : bands . append ( torch . nn . functional . conv2d ( self . pad_b ( lowpass ), filt_b )) level0 [ 'b' ] = bands pyramid . append ( level0 ) # Make intermediate levels for l in range ( n_levels - 2 ): level = {} if self . use_bilinear_downup : lowpass = torch . nn . functional . interpolate ( lowpass , scale_factor = 0.5 , mode = \"area\" , recompute_scale_factor = False ) else : lowpass = torch . nn . functional . conv2d ( self . pad_l ( lowpass ), self . filt_l ) lowpass = lowpass [:, :, :: 2 , :: 2 ] level [ 'l' ] = lowpass . clone () bands = [] for filt_b in self . band_filters : bands . append ( torch . nn . functional . conv2d ( self . pad_b ( lowpass ), filt_b )) level [ 'b' ] = bands if multiple_highpass : #downsampled = torch.nn.functional.interpolate(image, scale_factor=0.5, mode=\"area\") #level['h'] = torch.nn.functional.conv2d(self.pad_h0(downsampled), self.filt_h0) level [ 'h' ] = torch . nn . functional . conv2d ( self . pad_h0 ( lowpass ), self . filt_h0 ) pyramid . append ( level ) # Make final level (lowpass residual) level = {} if self . use_bilinear_downup : lowpass = torch . nn . functional . interpolate ( lowpass , scale_factor = 0.5 , mode = \"area\" , recompute_scale_factor = False ) else : lowpass = torch . nn . functional . conv2d ( self . pad_l ( lowpass ), self . filt_l ) lowpass = lowpass [:, :, :: 2 , :: 2 ] level [ 'l' ] = lowpass pyramid . append ( level ) return pyramid reconstruct_from_pyramid ( self , pyramid ) \u00b6 Reconstructs an input image from a steerable pyramid. Parameters: Name Type Description Default pyramid list of dicts of torch.tensor The steerable pyramid. Should be in the same format as output by construct_steerable_pyramid(). The number of channels should match num_channels when the pyramid maker was created. required Returns: Type Description torch.tensor The reconstructed image, in NCHW format. Source code in odak/learn/perception/spatial_steerable_pyramid.py def reconstruct_from_pyramid ( self , pyramid ): \"\"\" Reconstructs an input image from a steerable pyramid. Parameters ---------- pyramid : list of dicts of torch.tensor The steerable pyramid. Should be in the same format as output by construct_steerable_pyramid(). The number of channels should match num_channels when the pyramid maker was created. Returns ------- image : torch.tensor The reconstructed image, in NCHW format. \"\"\" def upsample ( image , size ): if self . use_bilinear_downup : return torch . nn . functional . interpolate ( image , size = size , mode = \"bilinear\" , align_corners = False , recompute_scale_factor = False ) else : zeros = torch . zeros (( image . size ()[ 0 ], image . size ()[ 1 ], image . size ()[ 2 ] * 2 , image . size ()[ 3 ] * 2 )) . to ( self . device ) zeros [:, :, :: 2 , :: 2 ] = image zeros = torch . nn . functional . conv2d ( self . pad_l ( zeros ), self . filt_l ) return zeros image = pyramid [ - 1 ][ 'l' ] for level in reversed ( pyramid [: - 1 ]): image = upsample ( image , level [ 'b' ][ 0 ] . size ()[ 2 :]) for b in range ( len ( level [ 'b' ])): b_filtered = torch . nn . functional . conv2d ( self . pad_b ( level [ 'b' ][ b ]), - self . band_filters [ b ]) image += b_filtered image = torch . nn . functional . conv2d ( self . pad_l0 ( image ), self . filt_l0 ) image += torch . nn . functional . conv2d ( self . pad_h0 ( pyramid [ 0 ][ 'h' ]), self . filt_h0 ) return image See also \u00b6 Perception","title":"odak.learn.perception.SpatialSteerablePyramid"},{"location":"odak/learn/perception/spatial_steerable_pyramid/#odaklearnperceptionspatialsteerablepyramid","text":"This implements a real-valued steerable pyramid where the filtering is carried out spatially (using convolution) as opposed to multiplication in the Fourier domain. This has a number of optimisations over previous implementations that increase efficiency, but introduce some reconstruction error.","title":"odak.learn.perception.SpatialSteerablePyramid"},{"location":"odak/learn/perception/spatial_steerable_pyramid/#odak.learn.perception.spatial_steerable_pyramid.SpatialSteerablePyramid.__init__","text":"Parameters: Name Type Description Default use_bilinear_downup bool This uses bilinear filtering when upsampling/downsampling, rather than the original approach of applying a large lowpass kernel and sampling even rows/columns True n_channels int Number of channels in the input images (e.g. 3 for RGB input) 1 filter_size int Desired size of filters (e.g. 3 will use 3x3 filters). 9 n_orientations int Number of oriented bands in each level of the pyramid. 6 filter_type str This can be used to select smaller filters than the original ones if desired. full: Original filter sizes cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate. trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels. 'full' device torch.device torch device the input images will be supplied from. device(type='cpu') Source code in odak/learn/perception/spatial_steerable_pyramid.py def __init__ ( self , use_bilinear_downup = True , n_channels = 1 , filter_size = 9 , n_orientations = 6 , filter_type = \"full\" , device = torch . device ( 'cpu' )): \"\"\" Parameters ---------- use_bilinear_downup : bool This uses bilinear filtering when upsampling/downsampling, rather than the original approach of applying a large lowpass kernel and sampling even rows/columns n_channels : int Number of channels in the input images (e.g. 3 for RGB input) filter_size : int Desired size of filters (e.g. 3 will use 3x3 filters). n_orientations : int Number of oriented bands in each level of the pyramid. filter_type : str This can be used to select smaller filters than the original ones if desired. full: Original filter sizes cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate. trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels. device : torch.device torch device the input images will be supplied from. \"\"\" self . use_bilinear_downup = use_bilinear_downup self . device = device filters = get_steerable_pyramid_filters ( filter_size , n_orientations , filter_type ) def make_pad ( filter ): filter_size = filter . size ( - 1 ) pad_amt = ( filter_size - 1 ) // 2 return torch . nn . ReflectionPad2d (( pad_amt , pad_amt , pad_amt , pad_amt )) if not self . use_bilinear_downup : self . filt_l = filters [ \"l\" ] . to ( device ) self . pad_l = make_pad ( self . filt_l ) self . filt_l0 = filters [ \"l0\" ] . to ( device ) self . pad_l0 = make_pad ( self . filt_l0 ) self . filt_h0 = filters [ \"h0\" ] . to ( device ) self . pad_h0 = make_pad ( self . filt_h0 ) for b in range ( len ( filters [ \"b\" ])): filters [ \"b\" ][ b ] = filters [ \"b\" ][ b ] . to ( device ) self . band_filters = filters [ \"b\" ] self . pad_b = make_pad ( self . band_filters [ 0 ]) if n_channels != 1 : def add_channels_to_filter ( filter ): padded = torch . zeros ( n_channels , n_channels , filter . size ()[ 2 ], filter . size ()[ 3 ]) . to ( device ) for channel in range ( n_channels ): padded [ channel , channel , :, :] = filter return padded self . filt_h0 = add_channels_to_filter ( self . filt_h0 ) for b in range ( len ( self . band_filters )): self . band_filters [ b ] = add_channels_to_filter ( self . band_filters [ b ]) self . filt_l0 = add_channels_to_filter ( self . filt_l0 ) if not self . use_bilinear_downup : self . filt_l = add_channels_to_filter ( self . filt_l )","title":"__init__()"},{"location":"odak/learn/perception/spatial_steerable_pyramid/#odak.learn.perception.spatial_steerable_pyramid.SpatialSteerablePyramid.construct_pyramid","text":"Constructs and returns a steerable pyramid for the provided image. Parameters: Name Type Description Default image torch.tensor The input image, in NCHW format. The number of channels C should match num_channels when the pyramid maker was created. required n_levels int Number of levels in the constructed steerable pyramid. required multiple_highpass bool If true, computes a highpass for each level of the pyramid. These extra levels are redundant (not used for reconstruction). False Returns: Type Description list of dicts of torch.tensor The computed steerable pyramid. Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels. Each level is stored as a dict, with the following keys: \"h\" Highpass residual \"l\" Lowpass residual \"b\" Oriented bands (a list of torch.tensor) Source code in odak/learn/perception/spatial_steerable_pyramid.py def construct_pyramid ( self , image , n_levels , multiple_highpass = False ): \"\"\" Constructs and returns a steerable pyramid for the provided image. Parameters ---------- image : torch.tensor The input image, in NCHW format. The number of channels C should match num_channels when the pyramid maker was created. n_levels : int Number of levels in the constructed steerable pyramid. multiple_highpass : bool If true, computes a highpass for each level of the pyramid. These extra levels are redundant (not used for reconstruction). Returns ------- pyramid : list of dicts of torch.tensor The computed steerable pyramid. Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels. Each level is stored as a dict, with the following keys: \"h\" Highpass residual \"l\" Lowpass residual \"b\" Oriented bands (a list of torch.tensor) \"\"\" pyramid = [] # Make level 0, containing highpass, lowpass and the bands level0 = {} level0 [ 'h' ] = torch . nn . functional . conv2d ( self . pad_h0 ( image ), self . filt_h0 ) #plt.imshow(level0['h'][0,0,...], cmap=\"gray\", vmin=0, vmax=1) # plt.show() lowpass = torch . nn . functional . conv2d ( self . pad_l0 ( image ), self . filt_l0 ) level0 [ 'l' ] = lowpass . clone () #np.save(\"lowpass_filtered.npy\", level0['l'][0,...].permute(1,2,0).numpy()) # quit() bands = [] for filt_b in self . band_filters : bands . append ( torch . nn . functional . conv2d ( self . pad_b ( lowpass ), filt_b )) level0 [ 'b' ] = bands pyramid . append ( level0 ) # Make intermediate levels for l in range ( n_levels - 2 ): level = {} if self . use_bilinear_downup : lowpass = torch . nn . functional . interpolate ( lowpass , scale_factor = 0.5 , mode = \"area\" , recompute_scale_factor = False ) else : lowpass = torch . nn . functional . conv2d ( self . pad_l ( lowpass ), self . filt_l ) lowpass = lowpass [:, :, :: 2 , :: 2 ] level [ 'l' ] = lowpass . clone () bands = [] for filt_b in self . band_filters : bands . append ( torch . nn . functional . conv2d ( self . pad_b ( lowpass ), filt_b )) level [ 'b' ] = bands if multiple_highpass : #downsampled = torch.nn.functional.interpolate(image, scale_factor=0.5, mode=\"area\") #level['h'] = torch.nn.functional.conv2d(self.pad_h0(downsampled), self.filt_h0) level [ 'h' ] = torch . nn . functional . conv2d ( self . pad_h0 ( lowpass ), self . filt_h0 ) pyramid . append ( level ) # Make final level (lowpass residual) level = {} if self . use_bilinear_downup : lowpass = torch . nn . functional . interpolate ( lowpass , scale_factor = 0.5 , mode = \"area\" , recompute_scale_factor = False ) else : lowpass = torch . nn . functional . conv2d ( self . pad_l ( lowpass ), self . filt_l ) lowpass = lowpass [:, :, :: 2 , :: 2 ] level [ 'l' ] = lowpass pyramid . append ( level ) return pyramid","title":"construct_pyramid()"},{"location":"odak/learn/perception/spatial_steerable_pyramid/#odak.learn.perception.spatial_steerable_pyramid.SpatialSteerablePyramid.reconstruct_from_pyramid","text":"Reconstructs an input image from a steerable pyramid. Parameters: Name Type Description Default pyramid list of dicts of torch.tensor The steerable pyramid. Should be in the same format as output by construct_steerable_pyramid(). The number of channels should match num_channels when the pyramid maker was created. required Returns: Type Description torch.tensor The reconstructed image, in NCHW format. Source code in odak/learn/perception/spatial_steerable_pyramid.py def reconstruct_from_pyramid ( self , pyramid ): \"\"\" Reconstructs an input image from a steerable pyramid. Parameters ---------- pyramid : list of dicts of torch.tensor The steerable pyramid. Should be in the same format as output by construct_steerable_pyramid(). The number of channels should match num_channels when the pyramid maker was created. Returns ------- image : torch.tensor The reconstructed image, in NCHW format. \"\"\" def upsample ( image , size ): if self . use_bilinear_downup : return torch . nn . functional . interpolate ( image , size = size , mode = \"bilinear\" , align_corners = False , recompute_scale_factor = False ) else : zeros = torch . zeros (( image . size ()[ 0 ], image . size ()[ 1 ], image . size ()[ 2 ] * 2 , image . size ()[ 3 ] * 2 )) . to ( self . device ) zeros [:, :, :: 2 , :: 2 ] = image zeros = torch . nn . functional . conv2d ( self . pad_l ( zeros ), self . filt_l ) return zeros image = pyramid [ - 1 ][ 'l' ] for level in reversed ( pyramid [: - 1 ]): image = upsample ( image , level [ 'b' ][ 0 ] . size ()[ 2 :]) for b in range ( len ( level [ 'b' ])): b_filtered = torch . nn . functional . conv2d ( self . pad_b ( level [ 'b' ][ b ]), - self . band_filters [ b ]) image += b_filtered image = torch . nn . functional . conv2d ( self . pad_l0 ( image ), self . filt_l0 ) image += torch . nn . functional . conv2d ( self . pad_h0 ( pyramid [ 0 ][ 'h' ]), self . filt_h0 ) return image","title":"reconstruct_from_pyramid()"},{"location":"odak/learn/perception/spatial_steerable_pyramid/#see-also","text":"Perception","title":"See also"},{"location":"odak/learn/perception/ycrcb_2_rgb/","text":"odak.learn.perception.ycrcb_2_rgb \u00b6 Converts an image from YCrCb colourspace to RGB colourspace. Parameters: Name Type Description Default image torch.tensor Input image. Should be a YCrCb floating-point image with values in the range [0, 1] Should be in NCHW format. required Returns: Type Description torch.tensor Image converted to RGB colourspace. Source code in odak/learn/perception/color_conversion.py def ycrcb_2_rgb ( image ): \"\"\" Converts an image from YCrCb colourspace to RGB colourspace. Parameters ---------- image : torch.tensor Input image. Should be a YCrCb floating-point image with values in the range [0, 1] Should be in NCHW format. Returns ------- rgb : torch.tensor Image converted to RGB colourspace. \"\"\" rgb = torch . zeros ( image . size (), device = image . device ) rgb [:, 0 , :, :] = image [:, 0 , :, :] + 1.403 * ( image [:, 1 , :, :] - 0.5 ) rgb [:, 1 , :, :] = image [:, 0 , :, :] - 0.714 * \\ ( image [:, 1 , :, :] - 0.5 ) - 0.344 * ( image [:, 2 , :, :] - 0.5 ) rgb [:, 2 , :, :] = image [:, 0 , :, :] + 1.773 * ( image [:, 2 , :, :] - 0.5 ) return rgb See also \u00b6 rgb_2_ycrcb()","title":"odak.learn.perception.ycrcb_2_rgb"},{"location":"odak/learn/perception/ycrcb_2_rgb/#odaklearnperceptionycrcb_2_rgb","text":"Converts an image from YCrCb colourspace to RGB colourspace. Parameters: Name Type Description Default image torch.tensor Input image. Should be a YCrCb floating-point image with values in the range [0, 1] Should be in NCHW format. required Returns: Type Description torch.tensor Image converted to RGB colourspace. Source code in odak/learn/perception/color_conversion.py def ycrcb_2_rgb ( image ): \"\"\" Converts an image from YCrCb colourspace to RGB colourspace. Parameters ---------- image : torch.tensor Input image. Should be a YCrCb floating-point image with values in the range [0, 1] Should be in NCHW format. Returns ------- rgb : torch.tensor Image converted to RGB colourspace. \"\"\" rgb = torch . zeros ( image . size (), device = image . device ) rgb [:, 0 , :, :] = image [:, 0 , :, :] + 1.403 * ( image [:, 1 , :, :] - 0.5 ) rgb [:, 1 , :, :] = image [:, 0 , :, :] - 0.714 * \\ ( image [:, 1 , :, :] - 0.5 ) - 0.344 * ( image [:, 2 , :, :] - 0.5 ) rgb [:, 2 , :, :] = image [:, 0 , :, :] + 1.773 * ( image [:, 2 , :, :] - 0.5 ) return rgb","title":"odak.learn.perception.ycrcb_2_rgb"},{"location":"odak/learn/perception/ycrcb_2_rgb/#see-also","text":"rgb_2_ycrcb()","title":"See also"},{"location":"odak/learn/wave/adjust_phase_only_slm_range/","text":"odak.learn.wave.adjust_phase_only_slm_range \u00b6 Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well. Parameters: Name Type Description Default native_range float Native range of the phase only SLM in radians (i.e. two pi). required working_wavelength float Wavelength of the illumination source or some working wavelength. required native_wavelength float Wavelength which the SLM is designed for. required Returns: Type Description float Calculated phase range in radians. Source code in odak/learn/wave/__init__.py def adjust_phase_only_slm_range ( native_range , working_wavelength , native_wavelength ): \"\"\" Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well. Parameters ---------- native_range : float Native range of the phase only SLM in radians (i.e. two pi). working_wavelength : float Wavelength of the illumination source or some working wavelength. native_wavelength : float Wavelength which the SLM is designed for. Returns ------- new_range : float Calculated phase range in radians. \"\"\" new_range = native_range / working_wavelength * native_wavelength return new_range Notes \u00b6 Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak . See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.adjust_phase_only_slm_range"},{"location":"odak/learn/wave/adjust_phase_only_slm_range/#odaklearnwaveadjust_phase_only_slm_range","text":"Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well. Parameters: Name Type Description Default native_range float Native range of the phase only SLM in radians (i.e. two pi). required working_wavelength float Wavelength of the illumination source or some working wavelength. required native_wavelength float Wavelength which the SLM is designed for. required Returns: Type Description float Calculated phase range in radians. Source code in odak/learn/wave/__init__.py def adjust_phase_only_slm_range ( native_range , working_wavelength , native_wavelength ): \"\"\" Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well. Parameters ---------- native_range : float Native range of the phase only SLM in radians (i.e. two pi). working_wavelength : float Wavelength of the illumination source or some working wavelength. native_wavelength : float Wavelength which the SLM is designed for. Returns ------- new_range : float Calculated phase range in radians. \"\"\" new_range = native_range / working_wavelength * native_wavelength return new_range","title":"odak.learn.wave.adjust_phase_only_slm_range"},{"location":"odak/learn/wave/adjust_phase_only_slm_range/#notes","text":"Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak .","title":"Notes"},{"location":"odak/learn/wave/adjust_phase_only_slm_range/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/band_limited_angular_spectrum/","text":"odak.learn.wave.band_limited_angular_spectrum \u00b6 A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673 . Parameters: Name Type Description Default field torch.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description torch.complex Final complex field (MxN). Source code in odak/learn/wave/classical.py def band_limited_angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate bandlimited angular spectrum based beam propagation. For more `Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673`. Parameters ---------- field : torch.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : torch.complex Final complex field (MxN). \"\"\" assert True == False , \"Refer to Issue 19 for more. This definition is unreliable.\" nv , nu = field . shape [ - 1 ], field . shape [ - 2 ] x = torch . linspace ( - nv * dx / 2 , nv * dx / 2 , nv , dtype = torch . float32 ) y = torch . linspace ( - nu * dx / 2 , nu * dx / 2 , nu , dtype = torch . float32 ) Y , X = torch . meshgrid ( y , x ) Z = torch . pow ( X , 2 ) + torch . pow ( Y , 2 ) distance = torch . FloatTensor ([ distance ]) h = 1. / ( 1 j * wavelength * distance ) * torch . exp ( 1 j * k * ( distance + Z / 2 / distance )) h = torch . fft . fft2 ( torch . fft . fftshift ( h )) * pow ( dx , 2 ) h = h . to ( field . device ) flimx = torch . ceil ( 1 / ((( 2 * distance * ( 1. / ( nv ))) ** 2 + 1 ) ** 0.5 * wavelength )) flimy = torch . ceil ( 1 / ((( 2 * distance * ( 1. / ( nu ))) ** 2 + 1 ) ** 0.5 * wavelength )) mask = torch . zeros (( nu , nv ), dtype = torch . cfloat ) . to ( field . device ) mask [ ... ] = torch . logical_and ( torch . lt ( torch . abs ( X ), flimx ), torch . lt ( torch . abs ( Y ), flimy )) mask = set_amplitude ( h , mask ) U1 = torch . fft . fft2 ( torch . fft . fftshift ( field )) U2 = mask * U1 result = torch . fft . ifftshift ( torch . fft . ifft2 ( U2 )) return result Notes \u00b6 Unless you know what you are doing, we do not suggest you to use this function directly. Rather stick to odak.learn.wave.propagate_beam for your beam propagation code. See also \u00b6 Computer Generated-Holography odak.learn.wave.propagate_beam","title":"odak.learn.wave.band_limited_angular_spectrum"},{"location":"odak/learn/wave/band_limited_angular_spectrum/#odaklearnwaveband_limited_angular_spectrum","text":"A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673 . Parameters: Name Type Description Default field torch.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description torch.complex Final complex field (MxN). Source code in odak/learn/wave/classical.py def band_limited_angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate bandlimited angular spectrum based beam propagation. For more `Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673`. Parameters ---------- field : torch.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : torch.complex Final complex field (MxN). \"\"\" assert True == False , \"Refer to Issue 19 for more. This definition is unreliable.\" nv , nu = field . shape [ - 1 ], field . shape [ - 2 ] x = torch . linspace ( - nv * dx / 2 , nv * dx / 2 , nv , dtype = torch . float32 ) y = torch . linspace ( - nu * dx / 2 , nu * dx / 2 , nu , dtype = torch . float32 ) Y , X = torch . meshgrid ( y , x ) Z = torch . pow ( X , 2 ) + torch . pow ( Y , 2 ) distance = torch . FloatTensor ([ distance ]) h = 1. / ( 1 j * wavelength * distance ) * torch . exp ( 1 j * k * ( distance + Z / 2 / distance )) h = torch . fft . fft2 ( torch . fft . fftshift ( h )) * pow ( dx , 2 ) h = h . to ( field . device ) flimx = torch . ceil ( 1 / ((( 2 * distance * ( 1. / ( nv ))) ** 2 + 1 ) ** 0.5 * wavelength )) flimy = torch . ceil ( 1 / ((( 2 * distance * ( 1. / ( nu ))) ** 2 + 1 ) ** 0.5 * wavelength )) mask = torch . zeros (( nu , nv ), dtype = torch . cfloat ) . to ( field . device ) mask [ ... ] = torch . logical_and ( torch . lt ( torch . abs ( X ), flimx ), torch . lt ( torch . abs ( Y ), flimy )) mask = set_amplitude ( h , mask ) U1 = torch . fft . fft2 ( torch . fft . fftshift ( field )) U2 = mask * U1 result = torch . fft . ifftshift ( torch . fft . ifft2 ( U2 )) return result","title":"odak.learn.wave.band_limited_angular_spectrum"},{"location":"odak/learn/wave/band_limited_angular_spectrum/#notes","text":"Unless you know what you are doing, we do not suggest you to use this function directly. Rather stick to odak.learn.wave.propagate_beam for your beam propagation code.","title":"Notes"},{"location":"odak/learn/wave/band_limited_angular_spectrum/#see-also","text":"Computer Generated-Holography odak.learn.wave.propagate_beam","title":"See also"},{"location":"odak/learn/wave/calculate_amplitude/","text":"odak.learn.wave.calculate_amplitude \u00b6 Definition to calculate amplitude of a single or multiple given electric field(s). Parameters: Name Type Description Default field torch.cfloat Electric fields or an electric field. required Returns: Type Description torch.float Amplitude or amplitudes of electric field(s). Source code in odak/learn/wave/__init__.py def calculate_amplitude ( field ): \"\"\" Definition to calculate amplitude of a single or multiple given electric field(s). Parameters ---------- field : torch.cfloat Electric fields or an electric field. Returns ------- amplitude : torch.float Amplitude or amplitudes of electric field(s). \"\"\" amplitude = torch . abs ( field ) return amplitude Notes \u00b6 Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak . See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.calculate_amplitude"},{"location":"odak/learn/wave/calculate_amplitude/#odaklearnwavecalculate_amplitude","text":"Definition to calculate amplitude of a single or multiple given electric field(s). Parameters: Name Type Description Default field torch.cfloat Electric fields or an electric field. required Returns: Type Description torch.float Amplitude or amplitudes of electric field(s). Source code in odak/learn/wave/__init__.py def calculate_amplitude ( field ): \"\"\" Definition to calculate amplitude of a single or multiple given electric field(s). Parameters ---------- field : torch.cfloat Electric fields or an electric field. Returns ------- amplitude : torch.float Amplitude or amplitudes of electric field(s). \"\"\" amplitude = torch . abs ( field ) return amplitude","title":"odak.learn.wave.calculate_amplitude"},{"location":"odak/learn/wave/calculate_amplitude/#notes","text":"Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak .","title":"Notes"},{"location":"odak/learn/wave/calculate_amplitude/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/calculate_phase/","text":"odak.learn.wave.calculate_phase \u00b6 i::: odak.learn.wave.calculate_phase selection: docstring_style: numpy Notes \u00b6 Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak . See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.calculate_phase"},{"location":"odak/learn/wave/calculate_phase/#odaklearnwavecalculate_phase","text":"i::: odak.learn.wave.calculate_phase selection: docstring_style: numpy","title":"odak.learn.wave.calculate_phase"},{"location":"odak/learn/wave/calculate_phase/#notes","text":"Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak .","title":"Notes"},{"location":"odak/learn/wave/calculate_phase/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/custom/","text":"odak.learn.wave.custom \u00b6 A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field torch.complex Complex field (MxN). required kernel torch.complex Custom complex kernel for beam propagation. required Returns: Type Description torch.complex Final complex field (MxN). Source code in odak/learn/wave/classical.py def custom ( field , kernel , zero_padding = False ): \"\"\" A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters ---------- field : torch.complex Complex field (MxN). kernel : torch.complex Custom complex kernel for beam propagation. Returns ------- result : torch.complex Final complex field (MxN). \"\"\" if type ( kernel ) == type ( None ): H = torch . zeros ( field . shape ) . to ( field . device ) else : H = kernel U1 = torch . fft . fftshift ( torch . fft . fft2 ( torch . fft . fftshift ( field ))) if zero_padding == False : U2 = H * U1 elif zero_padding == True : U2 = zero_pad ( H * U1 ) result = torch . fft . ifftshift ( torch . fft . ifft2 ( torch . fft . ifftshift ( U2 ))) return result Notes \u00b6 Unless you know what you are doing, we do not suggest you to use this function directly. Rather stick to odak.learn.wave.propagate_beam for your beam propagation code. Note that this function can also be used as convolution operation between two complex arrays. See also \u00b6 Computer Generated-Holography odak.learn.wave.propagate_beam","title":"odak.learn.wave.custom"},{"location":"odak/learn/wave/custom/#odaklearnwavecustom","text":"A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field torch.complex Complex field (MxN). required kernel torch.complex Custom complex kernel for beam propagation. required Returns: Type Description torch.complex Final complex field (MxN). Source code in odak/learn/wave/classical.py def custom ( field , kernel , zero_padding = False ): \"\"\" A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters ---------- field : torch.complex Complex field (MxN). kernel : torch.complex Custom complex kernel for beam propagation. Returns ------- result : torch.complex Final complex field (MxN). \"\"\" if type ( kernel ) == type ( None ): H = torch . zeros ( field . shape ) . to ( field . device ) else : H = kernel U1 = torch . fft . fftshift ( torch . fft . fft2 ( torch . fft . fftshift ( field ))) if zero_padding == False : U2 = H * U1 elif zero_padding == True : U2 = zero_pad ( H * U1 ) result = torch . fft . ifftshift ( torch . fft . ifft2 ( torch . fft . ifftshift ( U2 ))) return result","title":"odak.learn.wave.custom"},{"location":"odak/learn/wave/custom/#notes","text":"Unless you know what you are doing, we do not suggest you to use this function directly. Rather stick to odak.learn.wave.propagate_beam for your beam propagation code. Note that this function can also be used as convolution operation between two complex arrays.","title":"Notes"},{"location":"odak/learn/wave/custom/#see-also","text":"Computer Generated-Holography odak.learn.wave.propagate_beam","title":"See also"},{"location":"odak/learn/wave/generate_complex_field/","text":"odak.learn.wave.generate_complex_field \u00b6 Definition to generate a complex field with a given amplitude and phase. Parameters: Name Type Description Default amplitude ndarray or float Amplitude of the field. required phase ndarray or float Phase of the field. required Returns: Type Description ndarray Complex field. Source code in odak/learn/wave/__init__.py def generate_complex_field ( amplitude , phase ): \"\"\" Definition to generate a complex field with a given amplitude and phase. Parameters ---------- amplitude : ndarray or float Amplitude of the field. phase : ndarray or float Phase of the field. Returns ------- field : ndarray Complex field. \"\"\" if type ( phase ) == 'torch.Tensor' : phase = torch . tensor ( phase , requires_grad = True ) elif type ( phase ) == type ( 1. ): phase = torch . tensor ([ phase ], requires_grad = True ) if type ( amplitude ) == 'torch.Tensor' : amplitude = torch . tensor ( amplitude , requires_grad = True ) elif type ( amplitude ) == type ( 1. ): amplitude = torch . tensor ([ amplitude ], requires_grad = True ) field = amplitude * torch . cos ( phase ) + 1 j * amplitude * torch . sin ( phase ) return field Notes \u00b6 Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak . See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.generate_complex_field"},{"location":"odak/learn/wave/generate_complex_field/#odaklearnwavegenerate_complex_field","text":"Definition to generate a complex field with a given amplitude and phase. Parameters: Name Type Description Default amplitude ndarray or float Amplitude of the field. required phase ndarray or float Phase of the field. required Returns: Type Description ndarray Complex field. Source code in odak/learn/wave/__init__.py def generate_complex_field ( amplitude , phase ): \"\"\" Definition to generate a complex field with a given amplitude and phase. Parameters ---------- amplitude : ndarray or float Amplitude of the field. phase : ndarray or float Phase of the field. Returns ------- field : ndarray Complex field. \"\"\" if type ( phase ) == 'torch.Tensor' : phase = torch . tensor ( phase , requires_grad = True ) elif type ( phase ) == type ( 1. ): phase = torch . tensor ([ phase ], requires_grad = True ) if type ( amplitude ) == 'torch.Tensor' : amplitude = torch . tensor ( amplitude , requires_grad = True ) elif type ( amplitude ) == type ( 1. ): amplitude = torch . tensor ([ amplitude ], requires_grad = True ) field = amplitude * torch . cos ( phase ) + 1 j * amplitude * torch . sin ( phase ) return field","title":"odak.learn.wave.generate_complex_field"},{"location":"odak/learn/wave/generate_complex_field/#notes","text":"Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak .","title":"Notes"},{"location":"odak/learn/wave/generate_complex_field/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/gerchberg_saxton/","text":"odak.learn.wave.gerchberg_saxton \u00b6 Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246. Parameters: Name Type Description Default field torch.cfloat Complex field (MxN). required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required slm_range float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. 6.28 propagation_type str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). 'IR Fresnel' Returns: Type Description torch.cfloat Calculated complex hologram. Source code in odak/learn/wave/classical.py def gerchberg_saxton ( field , n_iterations , distance , dx , wavelength , slm_range = 6.28 , propagation_type = 'IR Fresnel' ): \"\"\" Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246. Parameters ---------- field : torch.cfloat Complex field (MxN). distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. slm_range : float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. propagation_type : str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). Returns ------- hologram : torch.cfloat Calculated complex hologram. reconstruction : torch.cfloat Calculated reconstruction using calculated hologram. \"\"\" k = wavenumber ( wavelength ) reconstruction = field for i in range ( n_iterations ): hologram = propagate_beam ( reconstruction , k , - distance , dx , wavelength , propagation_type ) hologram , _ = produce_phase_only_slm_pattern ( hologram , slm_range ) reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) reconstruction = set_amplitude ( hologram , field ) reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) return hologram , reconstruction Notes \u00b6 To optimize a phase-only hologram using Gerchberg-Saxton algorithm, please follow and observe the below example: import torch from odak.learn.wave import gerchberg_saxton from odak import np wavelength = 0.000000532 dx = 0.0000064 distance = 0.2 target_field = torch.zeros((500,500),dtype=torch.complex64) target_field[0::50,:] += 1 iteration_number = 3 hologram,reconstructed = gerchberg_saxton( target_field, iteration_number, distance, dx, wavelength, np.pi*2, 'TR Fresnel' ) See also \u00b6 Computer Generated-Holography odak.learn.wave.stochastic_gradient_descent odak.learn.wave.point_wise","title":"odak.learn.wave.gerchberg_saxton"},{"location":"odak/learn/wave/gerchberg_saxton/#odaklearnwavegerchberg_saxton","text":"Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246. Parameters: Name Type Description Default field torch.cfloat Complex field (MxN). required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required slm_range float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. 6.28 propagation_type str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). 'IR Fresnel' Returns: Type Description torch.cfloat Calculated complex hologram. Source code in odak/learn/wave/classical.py def gerchberg_saxton ( field , n_iterations , distance , dx , wavelength , slm_range = 6.28 , propagation_type = 'IR Fresnel' ): \"\"\" Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246. Parameters ---------- field : torch.cfloat Complex field (MxN). distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. slm_range : float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. propagation_type : str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). Returns ------- hologram : torch.cfloat Calculated complex hologram. reconstruction : torch.cfloat Calculated reconstruction using calculated hologram. \"\"\" k = wavenumber ( wavelength ) reconstruction = field for i in range ( n_iterations ): hologram = propagate_beam ( reconstruction , k , - distance , dx , wavelength , propagation_type ) hologram , _ = produce_phase_only_slm_pattern ( hologram , slm_range ) reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) reconstruction = set_amplitude ( hologram , field ) reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) return hologram , reconstruction","title":"odak.learn.wave.gerchberg_saxton"},{"location":"odak/learn/wave/gerchberg_saxton/#notes","text":"To optimize a phase-only hologram using Gerchberg-Saxton algorithm, please follow and observe the below example: import torch from odak.learn.wave import gerchberg_saxton from odak import np wavelength = 0.000000532 dx = 0.0000064 distance = 0.2 target_field = torch.zeros((500,500),dtype=torch.complex64) target_field[0::50,:] += 1 iteration_number = 3 hologram,reconstructed = gerchberg_saxton( target_field, iteration_number, distance, dx, wavelength, np.pi*2, 'TR Fresnel' )","title":"Notes"},{"location":"odak/learn/wave/gerchberg_saxton/#see-also","text":"Computer Generated-Holography odak.learn.wave.stochastic_gradient_descent odak.learn.wave.point_wise","title":"See also"},{"location":"odak/learn/wave/impulse_response_fresnel/","text":"odak.learn.wave.impulse_response_fresnel \u00b6 A definition to calculate impulse response based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/learn/wave/classical.py def impulse_response_fresnel ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate impulse response based Fresnel approximation for beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" assert True == False , \"Refer to Issue 19 for more. This definition is unreliable.\" nv , nu = field . shape [ - 1 ], field . shape [ - 2 ] x = torch . linspace ( - nu / 2 * dx , nu / 2 * dx , nu ) y = torch . linspace ( - nv / 2 * dx , nv / 2 * dx , nv ) X , Y = torch . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 distance = torch . tensor ([ distance ]) . to ( field . device ) h = torch . exp ( 1 j * k * distance ) / ( 1 j * wavelength * distance ) * \\ torch . exp ( 1 j * k / 2 / distance * Z ) h = torch . fft . fft2 ( torch . fft . fftshift ( h )) * dx ** 2 h = h . to ( field . device ) U1 = torch . fft . fft2 ( torch . fft . fftshift ( field )) U2 = h * U1 result = torch . fft . ifftshift ( torch . fft . ifft2 ( U2 )) return result Notes \u00b6 Unless you know what you are doing, we do not suggest you to use this function directly. Rather stick to odak.learn.wave.propagate_beam for your beam propagation code. See also \u00b6 Computer Generated-Holography odak.learn.wave.propagate_beam","title":"odak.learn.wave.impulse_response_fresnel"},{"location":"odak/learn/wave/impulse_response_fresnel/#odaklearnwaveimpulse_response_fresnel","text":"A definition to calculate impulse response based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/learn/wave/classical.py def impulse_response_fresnel ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate impulse response based Fresnel approximation for beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" assert True == False , \"Refer to Issue 19 for more. This definition is unreliable.\" nv , nu = field . shape [ - 1 ], field . shape [ - 2 ] x = torch . linspace ( - nu / 2 * dx , nu / 2 * dx , nu ) y = torch . linspace ( - nv / 2 * dx , nv / 2 * dx , nv ) X , Y = torch . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 distance = torch . tensor ([ distance ]) . to ( field . device ) h = torch . exp ( 1 j * k * distance ) / ( 1 j * wavelength * distance ) * \\ torch . exp ( 1 j * k / 2 / distance * Z ) h = torch . fft . fft2 ( torch . fft . fftshift ( h )) * dx ** 2 h = h . to ( field . device ) U1 = torch . fft . fft2 ( torch . fft . fftshift ( field )) U2 = h * U1 result = torch . fft . ifftshift ( torch . fft . ifft2 ( U2 )) return result","title":"odak.learn.wave.impulse_response_fresnel"},{"location":"odak/learn/wave/impulse_response_fresnel/#notes","text":"Unless you know what you are doing, we do not suggest you to use this function directly. Rather stick to odak.learn.wave.propagate_beam for your beam propagation code.","title":"Notes"},{"location":"odak/learn/wave/impulse_response_fresnel/#see-also","text":"Computer Generated-Holography odak.learn.wave.propagate_beam","title":"See also"},{"location":"odak/learn/wave/linear_grating/","text":"odak.learn.wave.linear_grating \u00b6 i::: odak.learn.wave.linear_grating selection: docstring_style: numpy Notes \u00b6 Here is a short example on how to use this function: from odak.learn.wave import wavenumber,quadratic_phase_function wavelength = 0.5*pow(10,-6) pixeltom = 6*pow(10,-6) distance = 10.0 resolution = [1080,1920] k = wavenumber(wavelength) plane_field = linear_grating( resolution[0], resolution[1], every=2, add=3.14, axis='x' ) See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.linear_grating"},{"location":"odak/learn/wave/linear_grating/#odaklearnwavelinear_grating","text":"i::: odak.learn.wave.linear_grating selection: docstring_style: numpy","title":"odak.learn.wave.linear_grating"},{"location":"odak/learn/wave/linear_grating/#notes","text":"Here is a short example on how to use this function: from odak.learn.wave import wavenumber,quadratic_phase_function wavelength = 0.5*pow(10,-6) pixeltom = 6*pow(10,-6) distance = 10.0 resolution = [1080,1920] k = wavenumber(wavelength) plane_field = linear_grating( resolution[0], resolution[1], every=2, add=3.14, axis='x' )","title":"Notes"},{"location":"odak/learn/wave/linear_grating/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/point_wise/","text":"odak.learn.wave.point_wise \u00b6 Naive point-wise hologram calculation method. For more information, refer to Maimone, Andrew, Andreas Georgiou, and Joel S. Kollin. \"Holographic near-eye displays for virtual and augmented reality.\" ACM Transactions on Graphics (TOG) 36.4 (2017): 1-16. Parameters: Name Type Description Default target torch.float float input target to be converted into a hologram (Target should be in range of 0 and 1). required wavelength float Wavelength of the electric field. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required device torch.device Device type (cuda or cpu)`. required lens_size int Size of lens for masking sub holograms(in pixels). 401 Returns: Type Description torch.cfloat Calculated complex hologram. Source code in odak/learn/wave/classical.py def point_wise ( target , wavelength , distance , dx , device , lens_size = 401 ): \"\"\" Naive point-wise hologram calculation method. For more information, refer to Maimone, Andrew, Andreas Georgiou, and Joel S. Kollin. \"Holographic near-eye displays for virtual and augmented reality.\" ACM Transactions on Graphics (TOG) 36.4 (2017): 1-16. Parameters ---------- target : torch.float float input target to be converted into a hologram (Target should be in range of 0 and 1). wavelength : float Wavelength of the electric field. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). device : torch.device Device type (cuda or cpu)`. lens_size : int Size of lens for masking sub holograms(in pixels). Returns ------- hologram : torch.cfloat Calculated complex hologram. \"\"\" target = zero_pad ( target ) nx , ny = target . shape k = wavenumber ( wavelength ) ones = torch . ones ( target . shape , requires_grad = False ) . to ( device ) x = torch . linspace ( - nx / 2 , nx / 2 , nx ) . to ( device ) y = torch . linspace ( - ny / 2 , ny / 2 , ny ) . to ( device ) X , Y = torch . meshgrid ( x , y ) Z = ( X ** 2 + Y ** 2 ) ** 0.5 mask = ( torch . abs ( Z ) <= lens_size ) mask [ mask > 1 ] = 1 fz = quadratic_phase_function ( nx , ny , k , focal =- distance , dx = dx ) . to ( device ) A = target ** 0.5 fz = mask * fz FA = torch . fft . fft2 ( torch . fft . fftshift ( A )) FFZ = torch . fft . fft2 ( torch . fft . fftshift ( fz )) H = torch . mul ( FA , FFZ ) hologram = torch . fft . ifftshift ( torch . fft . ifft2 ( H )) hologram_phase = calculate_phase ( hologram ) hologram = generate_complex_field ( ones , hologram_phase ) hologram = crop_center ( hologram ) return hologram Notes \u00b6 To optimize a phase-only hologram using point wise algorithm, please follow and observe the below example: import torch from odak.learn.wave import point_wise wavelength = 0.000000515 dx = 0.000008 distance = 0.15 resolution = [1080,1920] cuda = True device = torch.device(\"cuda\" if cuda else \"cpu\") target = torch.zeros(resolution[0],resolution[1]).to(device) target[540:600,960:1020] = 1 hologram = point_wise( target, wavelength, distance, dx, device, lens_size=401 ) See also \u00b6 Computer Generated-Holography odak.learn.wave.stochastic_gradient_descent odak.learn.wave.gerchberg_saxton","title":"odak.learn.wave.point_wise"},{"location":"odak/learn/wave/point_wise/#odaklearnwavepoint_wise","text":"Naive point-wise hologram calculation method. For more information, refer to Maimone, Andrew, Andreas Georgiou, and Joel S. Kollin. \"Holographic near-eye displays for virtual and augmented reality.\" ACM Transactions on Graphics (TOG) 36.4 (2017): 1-16. Parameters: Name Type Description Default target torch.float float input target to be converted into a hologram (Target should be in range of 0 and 1). required wavelength float Wavelength of the electric field. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required device torch.device Device type (cuda or cpu)`. required lens_size int Size of lens for masking sub holograms(in pixels). 401 Returns: Type Description torch.cfloat Calculated complex hologram. Source code in odak/learn/wave/classical.py def point_wise ( target , wavelength , distance , dx , device , lens_size = 401 ): \"\"\" Naive point-wise hologram calculation method. For more information, refer to Maimone, Andrew, Andreas Georgiou, and Joel S. Kollin. \"Holographic near-eye displays for virtual and augmented reality.\" ACM Transactions on Graphics (TOG) 36.4 (2017): 1-16. Parameters ---------- target : torch.float float input target to be converted into a hologram (Target should be in range of 0 and 1). wavelength : float Wavelength of the electric field. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). device : torch.device Device type (cuda or cpu)`. lens_size : int Size of lens for masking sub holograms(in pixels). Returns ------- hologram : torch.cfloat Calculated complex hologram. \"\"\" target = zero_pad ( target ) nx , ny = target . shape k = wavenumber ( wavelength ) ones = torch . ones ( target . shape , requires_grad = False ) . to ( device ) x = torch . linspace ( - nx / 2 , nx / 2 , nx ) . to ( device ) y = torch . linspace ( - ny / 2 , ny / 2 , ny ) . to ( device ) X , Y = torch . meshgrid ( x , y ) Z = ( X ** 2 + Y ** 2 ) ** 0.5 mask = ( torch . abs ( Z ) <= lens_size ) mask [ mask > 1 ] = 1 fz = quadratic_phase_function ( nx , ny , k , focal =- distance , dx = dx ) . to ( device ) A = target ** 0.5 fz = mask * fz FA = torch . fft . fft2 ( torch . fft . fftshift ( A )) FFZ = torch . fft . fft2 ( torch . fft . fftshift ( fz )) H = torch . mul ( FA , FFZ ) hologram = torch . fft . ifftshift ( torch . fft . ifft2 ( H )) hologram_phase = calculate_phase ( hologram ) hologram = generate_complex_field ( ones , hologram_phase ) hologram = crop_center ( hologram ) return hologram","title":"odak.learn.wave.point_wise"},{"location":"odak/learn/wave/point_wise/#notes","text":"To optimize a phase-only hologram using point wise algorithm, please follow and observe the below example: import torch from odak.learn.wave import point_wise wavelength = 0.000000515 dx = 0.000008 distance = 0.15 resolution = [1080,1920] cuda = True device = torch.device(\"cuda\" if cuda else \"cpu\") target = torch.zeros(resolution[0],resolution[1]).to(device) target[540:600,960:1020] = 1 hologram = point_wise( target, wavelength, distance, dx, device, lens_size=401 )","title":"Notes"},{"location":"odak/learn/wave/point_wise/#see-also","text":"Computer Generated-Holography odak.learn.wave.stochastic_gradient_descent odak.learn.wave.gerchberg_saxton","title":"See also"},{"location":"odak/learn/wave/prism_phase_function/","text":"odak.learn.wave.prism_phase_function \u00b6 i::: odak.learn.wave.prism_phase_function selection: docstring_style: numpy Notes \u00b6 Here is a short example on how to use this function: from odak.learn.wave import wavenumber,quadratic_phase_function wavelength = 0.5*pow(10,-6) pixeltom = 6*pow(10,-6) distance = 10.0 resolution = [1080,1920] k = wavenumber(wavelength) lens_field = prism_phase_function( resolution[0], resolution[1], k, fangle=0.1, dx=pixeltom, axis='x' ) See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.prism_phase_function"},{"location":"odak/learn/wave/prism_phase_function/#odaklearnwaveprism_phase_function","text":"i::: odak.learn.wave.prism_phase_function selection: docstring_style: numpy","title":"odak.learn.wave.prism_phase_function"},{"location":"odak/learn/wave/prism_phase_function/#notes","text":"Here is a short example on how to use this function: from odak.learn.wave import wavenumber,quadratic_phase_function wavelength = 0.5*pow(10,-6) pixeltom = 6*pow(10,-6) distance = 10.0 resolution = [1080,1920] k = wavenumber(wavelength) lens_field = prism_phase_function( resolution[0], resolution[1], k, fangle=0.1, dx=pixeltom, axis='x' )","title":"Notes"},{"location":"odak/learn/wave/prism_phase_function/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/produce_phase_only_slm_pattern/","text":"odak.learn.wave.produce_phase_only_slm_pattern \u00b6 Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field. Parameters: Name Type Description Default hologram torch.cfloat Input holographic field. required slm_range float Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more. required bits int Quantization bits. 8 default_range float Default range of phase only SLM. 6.28 illumination torch.tensor Spatial illumination distribution. None Returns: Type Description torch.cfloat Adjusted phase only pattern. Source code in odak/learn/wave/__init__.py def produce_phase_only_slm_pattern ( hologram , slm_range , bits = 8 , default_range = 6.28 , illumination = None ): \"\"\" Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field. Parameters ---------- hologram : torch.cfloat Input holographic field. slm_range : float Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more. bits : int Quantization bits. default_range : float Default range of phase only SLM. illumination : torch.tensor Spatial illumination distribution. Returns ------- pattern : torch.cfloat Adjusted phase only pattern. hologram_digital : np.int Digital representation of the hologram. \"\"\" # hologram_phase = calculate_phase(hologram) % default_range hologram_phase = calculate_phase ( hologram ) hologram_phase = hologram_phase % slm_range hologram_phase /= slm_range hologram_phase *= 2 ** bits hologram_digital = hologram_phase . detach () . clone () hologram_phase = torch . ceil ( hologram_phase ) hologram_phase *= slm_range / 2 ** bits if type ( illumination ) == type ( None ): A = torch . tensor ([ 1. ]) . to ( hologram_phase . device ) else : A = illumination return A * torch . cos ( hologram_phase ) + A * 1 j * torch . sin ( hologram_phase ), hologram_digital Notes \u00b6 Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak . See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.produce_phase_only_slm_pattern"},{"location":"odak/learn/wave/produce_phase_only_slm_pattern/#odaklearnwaveproduce_phase_only_slm_pattern","text":"Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field. Parameters: Name Type Description Default hologram torch.cfloat Input holographic field. required slm_range float Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more. required bits int Quantization bits. 8 default_range float Default range of phase only SLM. 6.28 illumination torch.tensor Spatial illumination distribution. None Returns: Type Description torch.cfloat Adjusted phase only pattern. Source code in odak/learn/wave/__init__.py def produce_phase_only_slm_pattern ( hologram , slm_range , bits = 8 , default_range = 6.28 , illumination = None ): \"\"\" Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field. Parameters ---------- hologram : torch.cfloat Input holographic field. slm_range : float Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more. bits : int Quantization bits. default_range : float Default range of phase only SLM. illumination : torch.tensor Spatial illumination distribution. Returns ------- pattern : torch.cfloat Adjusted phase only pattern. hologram_digital : np.int Digital representation of the hologram. \"\"\" # hologram_phase = calculate_phase(hologram) % default_range hologram_phase = calculate_phase ( hologram ) hologram_phase = hologram_phase % slm_range hologram_phase /= slm_range hologram_phase *= 2 ** bits hologram_digital = hologram_phase . detach () . clone () hologram_phase = torch . ceil ( hologram_phase ) hologram_phase *= slm_range / 2 ** bits if type ( illumination ) == type ( None ): A = torch . tensor ([ 1. ]) . to ( hologram_phase . device ) else : A = illumination return A * torch . cos ( hologram_phase ) + A * 1 j * torch . sin ( hologram_phase ), hologram_digital","title":"odak.learn.wave.produce_phase_only_slm_pattern"},{"location":"odak/learn/wave/produce_phase_only_slm_pattern/#notes","text":"Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak .","title":"Notes"},{"location":"odak/learn/wave/produce_phase_only_slm_pattern/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/propagate_beam/","text":"odak.learn.wave.propagate_beam \u00b6 Definitions for Fresnel impulse respone (IR), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. Parameters: Name Type Description Default field torch.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required propagation_type str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). 'IR Fresnel' kernel torch.complex Custom complex kernel. None Returns: Type Description torch.complex128 Final complex field (MxN). Source code in odak/learn/wave/classical.py def propagate_beam ( field , k , distance , dx , wavelength , propagation_type = 'IR Fresnel' , kernel = None , zero_padding = False ): \"\"\" Definitions for Fresnel impulse respone (IR), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. Parameters ---------- field : torch.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. propagation_type : str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). kernel : torch.complex Custom complex kernel. Returns ------- result : torch.complex128 Final complex field (MxN). \"\"\" if propagation_type == 'IR Fresnel' : result = impulse_response_fresnel ( field , k , distance , dx , wavelength ) elif propagation_type == 'Bandlimited Angular Spectrum' : result = band_limited_angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'TR Fresnel' : result = transfer_function_fresnel ( field , k , distance , dx , wavelength , zero_padding ) elif propagation_type == 'custom' : result = custom ( field , kernel , zero_padding ) elif propagation_type == 'Fraunhofer' : nv , nu = field . shape [ - 1 ], field . shape [ - 2 ] x = torch . linspace ( - nv * dx / 2 , nv * dx / 2 , nv , dtype = torch . float32 ) y = torch . linspace ( - nu * dx / 2 , nu * dx / 2 , nu , dtype = torch . float32 ) Y , X = torch . meshgrid ( y , x ) Z = torch . pow ( X , 2 ) + torch . pow ( Y , 2 ) c = 1. / ( 1 j * wavelength * distance ) * torch . exp ( 1 j * k * 0.5 / distance * Z ) c = c . to ( field . device ) result = c * \\ torch . fft . ifftshift ( torch . fft . fft2 ( torch . fft . fftshift ( field ))) * pow ( dx , 2 ) else : assert True == False , \"Propagation type not recognized.\" return result Notes \u00b6 We provide a sample usage of this function as below. from odak.learn.wave import propagate_beam,generate_complex_field,wavenumber from odak.learn.tools import zero_pad import torch wavelength = 0.5*pow(10,-6) pixeltom = 6*pow(10,-6) distance = 0.2 propagation_type = 'TR Fresnel' k = wavenumber(wavelength) sample_phase = torch.rand((500,500)) sample_amplitude = torch.zeros((500,500)) sample_amplitude[ 240:260, 240:260 ] = 1000 sample_field = generate_complex_field(sample_amplitude,sample_phase) sample_field = zero_pad(sample_field) reconstruction = propagate_beam( sample_field, k, distance, pixeltom, wavelength, propagation_type ) See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.propagate_beam"},{"location":"odak/learn/wave/propagate_beam/#odaklearnwavepropagate_beam","text":"Definitions for Fresnel impulse respone (IR), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. Parameters: Name Type Description Default field torch.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required propagation_type str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). 'IR Fresnel' kernel torch.complex Custom complex kernel. None Returns: Type Description torch.complex128 Final complex field (MxN). Source code in odak/learn/wave/classical.py def propagate_beam ( field , k , distance , dx , wavelength , propagation_type = 'IR Fresnel' , kernel = None , zero_padding = False ): \"\"\" Definitions for Fresnel impulse respone (IR), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. Parameters ---------- field : torch.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. propagation_type : str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). kernel : torch.complex Custom complex kernel. Returns ------- result : torch.complex128 Final complex field (MxN). \"\"\" if propagation_type == 'IR Fresnel' : result = impulse_response_fresnel ( field , k , distance , dx , wavelength ) elif propagation_type == 'Bandlimited Angular Spectrum' : result = band_limited_angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'TR Fresnel' : result = transfer_function_fresnel ( field , k , distance , dx , wavelength , zero_padding ) elif propagation_type == 'custom' : result = custom ( field , kernel , zero_padding ) elif propagation_type == 'Fraunhofer' : nv , nu = field . shape [ - 1 ], field . shape [ - 2 ] x = torch . linspace ( - nv * dx / 2 , nv * dx / 2 , nv , dtype = torch . float32 ) y = torch . linspace ( - nu * dx / 2 , nu * dx / 2 , nu , dtype = torch . float32 ) Y , X = torch . meshgrid ( y , x ) Z = torch . pow ( X , 2 ) + torch . pow ( Y , 2 ) c = 1. / ( 1 j * wavelength * distance ) * torch . exp ( 1 j * k * 0.5 / distance * Z ) c = c . to ( field . device ) result = c * \\ torch . fft . ifftshift ( torch . fft . fft2 ( torch . fft . fftshift ( field ))) * pow ( dx , 2 ) else : assert True == False , \"Propagation type not recognized.\" return result","title":"odak.learn.wave.propagate_beam"},{"location":"odak/learn/wave/propagate_beam/#notes","text":"We provide a sample usage of this function as below. from odak.learn.wave import propagate_beam,generate_complex_field,wavenumber from odak.learn.tools import zero_pad import torch wavelength = 0.5*pow(10,-6) pixeltom = 6*pow(10,-6) distance = 0.2 propagation_type = 'TR Fresnel' k = wavenumber(wavelength) sample_phase = torch.rand((500,500)) sample_amplitude = torch.zeros((500,500)) sample_amplitude[ 240:260, 240:260 ] = 1000 sample_field = generate_complex_field(sample_amplitude,sample_phase) sample_field = zero_pad(sample_field) reconstruction = propagate_beam( sample_field, k, distance, pixeltom, wavelength, propagation_type )","title":"Notes"},{"location":"odak/learn/wave/propagate_beam/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/quadratic_phase_function/","text":"odak.learn.wave.quadratic_phase_function \u00b6 A definition to generate 2D quadratic phase function, which is typically use to represent lenses. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required k odak.wave.wavenumber See odak.wave.wavenumber for more. required focal float Focal length of the quadratic phase function. 0.4 dx float Pixel pitch. 0.001 offset list Deviation from the center along X and Y axes. [0, 0] Returns: Type Description torch.tensor Generated quadratic phase function. Source code in odak/learn/wave/lens.py def quadratic_phase_function ( nx , ny , k , focal = 0.4 , dx = 0.001 , offset = [ 0 , 0 ]): \"\"\" A definition to generate 2D quadratic phase function, which is typically use to represent lenses. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. k : odak.wave.wavenumber See odak.wave.wavenumber for more. focal : float Focal length of the quadratic phase function. dx : float Pixel pitch. offset : list Deviation from the center along X and Y axes. Returns ------- function : torch.tensor Generated quadratic phase function. \"\"\" size = [ nx , ny ] x = torch . linspace ( - size [ 0 ] * dx / 2 , size [ 0 ] * dx / 2 , size [ 0 ]) - offset [ 1 ] * dx y = torch . linspace ( - size [ 1 ] * dx / 2 , size [ 1 ] * dx / 2 , size [ 1 ]) - offset [ 0 ] * dx X , Y = torch . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 focal = torch . tensor ([ focal ]) k = torch . tensor ([ k ]) qwf = torch . exp ( 1 j * k * 0.5 * torch . sin ( Z / focal )) return qwf Notes \u00b6 Here is a short example on how to use this function: from odak.learn.wave import wavenumber,quadratic_phase_function wavelength = 0.5*pow(10,-6) pixeltom = 6*pow(10,-6) distance = 10.0 resolution = [1080,1920] k = wavenumber(wavelength) lens_field = quadratic_phase_function( resolution[0], resolution[1], k, focal=0.3, dx=pixeltom ) See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.quadratic_phase_function"},{"location":"odak/learn/wave/quadratic_phase_function/#odaklearnwavequadratic_phase_function","text":"A definition to generate 2D quadratic phase function, which is typically use to represent lenses. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required k odak.wave.wavenumber See odak.wave.wavenumber for more. required focal float Focal length of the quadratic phase function. 0.4 dx float Pixel pitch. 0.001 offset list Deviation from the center along X and Y axes. [0, 0] Returns: Type Description torch.tensor Generated quadratic phase function. Source code in odak/learn/wave/lens.py def quadratic_phase_function ( nx , ny , k , focal = 0.4 , dx = 0.001 , offset = [ 0 , 0 ]): \"\"\" A definition to generate 2D quadratic phase function, which is typically use to represent lenses. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. k : odak.wave.wavenumber See odak.wave.wavenumber for more. focal : float Focal length of the quadratic phase function. dx : float Pixel pitch. offset : list Deviation from the center along X and Y axes. Returns ------- function : torch.tensor Generated quadratic phase function. \"\"\" size = [ nx , ny ] x = torch . linspace ( - size [ 0 ] * dx / 2 , size [ 0 ] * dx / 2 , size [ 0 ]) - offset [ 1 ] * dx y = torch . linspace ( - size [ 1 ] * dx / 2 , size [ 1 ] * dx / 2 , size [ 1 ]) - offset [ 0 ] * dx X , Y = torch . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 focal = torch . tensor ([ focal ]) k = torch . tensor ([ k ]) qwf = torch . exp ( 1 j * k * 0.5 * torch . sin ( Z / focal )) return qwf","title":"odak.learn.wave.quadratic_phase_function"},{"location":"odak/learn/wave/quadratic_phase_function/#notes","text":"Here is a short example on how to use this function: from odak.learn.wave import wavenumber,quadratic_phase_function wavelength = 0.5*pow(10,-6) pixeltom = 6*pow(10,-6) distance = 10.0 resolution = [1080,1920] k = wavenumber(wavelength) lens_field = quadratic_phase_function( resolution[0], resolution[1], k, focal=0.3, dx=pixeltom )","title":"Notes"},{"location":"odak/learn/wave/quadratic_phase_function/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/set_amplitude/","text":"odak.learn.wave.set_amplitude \u00b6 Definition to keep phase as is and change the amplitude of a given field. Parameters: Name Type Description Default field torch.cfloat Complex field. required amplitude torch.cfloat or torch.float Amplitudes. required Returns: Type Description torch.cfloat Complex field. Source code in odak/learn/wave/__init__.py def set_amplitude ( field , amplitude ): \"\"\" Definition to keep phase as is and change the amplitude of a given field. Parameters ---------- field : torch.cfloat Complex field. amplitude : torch.cfloat or torch.float Amplitudes. Returns ------- new_field : torch.cfloat Complex field. \"\"\" amplitude = calculate_amplitude ( amplitude ) phase = calculate_phase ( field ) new_field = amplitude * torch . cos ( phase ) + 1 j * amplitude * torch . sin ( phase ) return new_field Notes \u00b6 Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak . See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.set_amplitude"},{"location":"odak/learn/wave/set_amplitude/#odaklearnwaveset_amplitude","text":"Definition to keep phase as is and change the amplitude of a given field. Parameters: Name Type Description Default field torch.cfloat Complex field. required amplitude torch.cfloat or torch.float Amplitudes. required Returns: Type Description torch.cfloat Complex field. Source code in odak/learn/wave/__init__.py def set_amplitude ( field , amplitude ): \"\"\" Definition to keep phase as is and change the amplitude of a given field. Parameters ---------- field : torch.cfloat Complex field. amplitude : torch.cfloat or torch.float Amplitudes. Returns ------- new_field : torch.cfloat Complex field. \"\"\" amplitude = calculate_amplitude ( amplitude ) phase = calculate_phase ( field ) new_field = amplitude * torch . cos ( phase ) + 1 j * amplitude * torch . sin ( phase ) return new_field","title":"odak.learn.wave.set_amplitude"},{"location":"odak/learn/wave/set_amplitude/#notes","text":"Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak .","title":"Notes"},{"location":"odak/learn/wave/set_amplitude/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/learn/wave/stochastic_gradient_descent/","text":"odak.learn.wave.stochastic_gradient_descent \u00b6 Definition to generate phase and reconstruction from target image via stochastic gradient descent. Parameters: Name Type Description Default field torch.Tensor Target field amplitude. required wavelength double Set if the converted array requires gradient. required distance double Hologaram plane distance wrt SLM plane required dx float SLM pixel pitch required resolution array SLM resolution required propogation type Str Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer) required n_iteration : int Max iteratation 100 loss_function : function If none it is set to be l2 loss None cuda boolean GPU enabled False learning_rate float Learning rate. 0.1 Returns: Type Description torch.Tensor Phase only hologram as torch array Source code in odak/learn/wave/classical.py def stochastic_gradient_descent ( field , wavelength , distance , dx , resolution , propogation_type , n_iteration = 100 , loss_function = None , cuda = False , learning_rate = 0.1 ): \"\"\" Definition to generate phase and reconstruction from target image via stochastic gradient descent. Parameters ---------- field : torch.Tensor Target field amplitude. wavelength : double Set if the converted array requires gradient. distance : double Hologaram plane distance wrt SLM plane dx : float SLM pixel pitch resolution : array SLM resolution propogation type : Str Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer) n_iteration: : int Max iteratation loss_function: : function If none it is set to be l2 loss cuda : boolean GPU enabled learning_rate : float Learning rate. Returns ------- hologram : torch.Tensor Phase only hologram as torch array reconstruction_intensity: torch.Tensor Reconstruction as torch array \"\"\" torch . cuda . empty_cache () torch . manual_seed ( 0 ) device = torch . device ( \"cuda\" if cuda else \"cpu\" ) field = field . to ( device ) phase = torch . rand ( resolution [ 0 ], resolution [ 1 ]) . detach () . to ( device ) . requires_grad_ () amplitude = torch . ones ( resolution [ 0 ], resolution [ 1 ], requires_grad = False ) . to ( device ) k = wavenumber ( wavelength ) optimizer = torch . optim . Adam ([{ 'params' : [ phase ]}], lr = learning_rate ) if type ( loss_function ) == type ( None ): loss_function = torch . nn . MSELoss () . to ( device ) t = tqdm ( range ( n_iteration ), leave = False ) for i in t : optimizer . zero_grad () hologram = generate_complex_field ( amplitude , phase ) hologram_padded = zero_pad ( hologram ) reconstruction_padded = propagate_beam ( hologram_padded , k , distance , dx , wavelength , propogation_type ) reconstruction = crop_center ( reconstruction_padded ) reconstruction_intensity = calculate_amplitude ( reconstruction ) ** 2 loss = loss_function ( reconstruction_intensity , field ) description = \"Iteration: {} loss: {:.4f} \" . format ( i , loss . item ()) loss . backward ( retain_graph = True ) optimizer . step () t . set_description ( description ) print ( description ) torch . no_grad () hologram = generate_complex_field ( amplitude , phase ) hologram_padded = zero_pad ( hologram ) reconstruction_padded = propagate_beam ( hologram_padded , k , distance , dx , wavelength , propogation_type ) reconstruction = crop_center ( reconstruction_padded ) hologram = crop_center ( hologram_padded ) return hologram . detach (), reconstruction . detach () Notes \u00b6 To optimize a phase-only hologram using Gerchberg-Saxton algorithm, please follow and observe the below example: import torch from odak.learn.wave import stochastic_gradient_descent wavelength = 0.000000532 dx = 0.0000064 distance = 0.1 cuda = False resolution = [1080,1920] target_field = torch.zeros(resolution[0],resolution[1]) target_field[500::600,:] = 1 iteration_number = 5 hologram,reconstructed = stochastic_gradient_descent( target_field, wavelength, distance, dx, resolution, 'TR Fresnel', iteration_number, learning_rate=0.1, cuda=cuda ) See also \u00b6 Computer Generated-Holography odak.learn.wave.gerchberg_saxton odak.learn.wave.point_wise","title":"odak.learn.wave.stochastic_gradient_descent"},{"location":"odak/learn/wave/stochastic_gradient_descent/#odaklearnwavestochastic_gradient_descent","text":"Definition to generate phase and reconstruction from target image via stochastic gradient descent. Parameters: Name Type Description Default field torch.Tensor Target field amplitude. required wavelength double Set if the converted array requires gradient. required distance double Hologaram plane distance wrt SLM plane required dx float SLM pixel pitch required resolution array SLM resolution required propogation type Str Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer) required n_iteration : int Max iteratation 100 loss_function : function If none it is set to be l2 loss None cuda boolean GPU enabled False learning_rate float Learning rate. 0.1 Returns: Type Description torch.Tensor Phase only hologram as torch array Source code in odak/learn/wave/classical.py def stochastic_gradient_descent ( field , wavelength , distance , dx , resolution , propogation_type , n_iteration = 100 , loss_function = None , cuda = False , learning_rate = 0.1 ): \"\"\" Definition to generate phase and reconstruction from target image via stochastic gradient descent. Parameters ---------- field : torch.Tensor Target field amplitude. wavelength : double Set if the converted array requires gradient. distance : double Hologaram plane distance wrt SLM plane dx : float SLM pixel pitch resolution : array SLM resolution propogation type : Str Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer) n_iteration: : int Max iteratation loss_function: : function If none it is set to be l2 loss cuda : boolean GPU enabled learning_rate : float Learning rate. Returns ------- hologram : torch.Tensor Phase only hologram as torch array reconstruction_intensity: torch.Tensor Reconstruction as torch array \"\"\" torch . cuda . empty_cache () torch . manual_seed ( 0 ) device = torch . device ( \"cuda\" if cuda else \"cpu\" ) field = field . to ( device ) phase = torch . rand ( resolution [ 0 ], resolution [ 1 ]) . detach () . to ( device ) . requires_grad_ () amplitude = torch . ones ( resolution [ 0 ], resolution [ 1 ], requires_grad = False ) . to ( device ) k = wavenumber ( wavelength ) optimizer = torch . optim . Adam ([{ 'params' : [ phase ]}], lr = learning_rate ) if type ( loss_function ) == type ( None ): loss_function = torch . nn . MSELoss () . to ( device ) t = tqdm ( range ( n_iteration ), leave = False ) for i in t : optimizer . zero_grad () hologram = generate_complex_field ( amplitude , phase ) hologram_padded = zero_pad ( hologram ) reconstruction_padded = propagate_beam ( hologram_padded , k , distance , dx , wavelength , propogation_type ) reconstruction = crop_center ( reconstruction_padded ) reconstruction_intensity = calculate_amplitude ( reconstruction ) ** 2 loss = loss_function ( reconstruction_intensity , field ) description = \"Iteration: {} loss: {:.4f} \" . format ( i , loss . item ()) loss . backward ( retain_graph = True ) optimizer . step () t . set_description ( description ) print ( description ) torch . no_grad () hologram = generate_complex_field ( amplitude , phase ) hologram_padded = zero_pad ( hologram ) reconstruction_padded = propagate_beam ( hologram_padded , k , distance , dx , wavelength , propogation_type ) reconstruction = crop_center ( reconstruction_padded ) hologram = crop_center ( hologram_padded ) return hologram . detach (), reconstruction . detach ()","title":"odak.learn.wave.stochastic_gradient_descent"},{"location":"odak/learn/wave/stochastic_gradient_descent/#notes","text":"To optimize a phase-only hologram using Gerchberg-Saxton algorithm, please follow and observe the below example: import torch from odak.learn.wave import stochastic_gradient_descent wavelength = 0.000000532 dx = 0.0000064 distance = 0.1 cuda = False resolution = [1080,1920] target_field = torch.zeros(resolution[0],resolution[1]) target_field[500::600,:] = 1 iteration_number = 5 hologram,reconstructed = stochastic_gradient_descent( target_field, wavelength, distance, dx, resolution, 'TR Fresnel', iteration_number, learning_rate=0.1, cuda=cuda )","title":"Notes"},{"location":"odak/learn/wave/stochastic_gradient_descent/#see-also","text":"Computer Generated-Holography odak.learn.wave.gerchberg_saxton odak.learn.wave.point_wise","title":"See also"},{"location":"odak/learn/wave/transfer_function_kernel/","text":"odak.learn.wave.transfer_function_fresnel \u00b6 A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field torch.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description torch.complex Final complex field (MxN). Source code in odak/learn/wave/classical.py def transfer_function_fresnel ( field , k , distance , dx , wavelength , zero_padding = False ): \"\"\" A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters ---------- field : torch.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : torch.complex Final complex field (MxN). \"\"\" distance = torch . tensor ([ distance ]) . to ( field . device ) nv , nu = field . shape [ - 1 ], field . shape [ - 2 ] fx = torch . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nu , dtype = torch . float32 ) . to ( field . device ) fy = torch . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nv , dtype = torch . float32 ) . to ( field . device ) FY , FX = torch . meshgrid ( fx , fy ) H = torch . exp ( 1 j * k * distance * ( 1 - ( FX * wavelength ) ** 2 - ( FY * wavelength ) ** 2 ) ** 0.5 ) H = H . to ( field . device ) U1 = torch . fft . fftshift ( torch . fft . fft2 ( torch . fft . fftshift ( field ))) if zero_padding == False : U2 = H * U1 elif zero_padding == True : U2 = zero_pad ( H * U1 ) result = torch . fft . ifftshift ( torch . fft . ifft2 ( torch . fft . ifftshift ( U2 ))) return result Notes \u00b6 Unless you know what you are doing, we do not suggest you to use this function directly. Rather stick to odak.learn.wave.propagate_beam for your beam propagation code. See also \u00b6 Computer Generated-Holography odak.learn.wave.propagate_beam","title":"odak.learn.wave.transfer_function_fresnel"},{"location":"odak/learn/wave/transfer_function_kernel/#odaklearnwavetransfer_function_fresnel","text":"A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field torch.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description torch.complex Final complex field (MxN). Source code in odak/learn/wave/classical.py def transfer_function_fresnel ( field , k , distance , dx , wavelength , zero_padding = False ): \"\"\" A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters ---------- field : torch.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : torch.complex Final complex field (MxN). \"\"\" distance = torch . tensor ([ distance ]) . to ( field . device ) nv , nu = field . shape [ - 1 ], field . shape [ - 2 ] fx = torch . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nu , dtype = torch . float32 ) . to ( field . device ) fy = torch . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nv , dtype = torch . float32 ) . to ( field . device ) FY , FX = torch . meshgrid ( fx , fy ) H = torch . exp ( 1 j * k * distance * ( 1 - ( FX * wavelength ) ** 2 - ( FY * wavelength ) ** 2 ) ** 0.5 ) H = H . to ( field . device ) U1 = torch . fft . fftshift ( torch . fft . fft2 ( torch . fft . fftshift ( field ))) if zero_padding == False : U2 = H * U1 elif zero_padding == True : U2 = zero_pad ( H * U1 ) result = torch . fft . ifftshift ( torch . fft . ifft2 ( torch . fft . ifftshift ( U2 ))) return result","title":"odak.learn.wave.transfer_function_fresnel"},{"location":"odak/learn/wave/transfer_function_kernel/#notes","text":"Unless you know what you are doing, we do not suggest you to use this function directly. Rather stick to odak.learn.wave.propagate_beam for your beam propagation code.","title":"Notes"},{"location":"odak/learn/wave/transfer_function_kernel/#see-also","text":"Computer Generated-Holography odak.learn.wave.propagate_beam","title":"See also"},{"location":"odak/learn/wave/wavenumber/","text":"odak.learn.wave.wavenumber \u00b6 Definition for calculating the wavenumber of a plane wave. Parameters: Name Type Description Default wavelength float Wavelength of a wave in mm. required Returns: Type Description float Wave number for a given wavelength. Source code in odak/learn/wave/__init__.py def wavenumber ( wavelength ): \"\"\" Definition for calculating the wavenumber of a plane wave. Parameters ---------- wavelength : float Wavelength of a wave in mm. Returns ------- k : float Wave number for a given wavelength. \"\"\" k = 2 * np . pi / wavelength return k Notes \u00b6 Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak . See also \u00b6 Computer Generated-Holography","title":"odak.learn.wave.wavenumber"},{"location":"odak/learn/wave/wavenumber/#odaklearnwavewavenumber","text":"Definition for calculating the wavenumber of a plane wave. Parameters: Name Type Description Default wavelength float Wavelength of a wave in mm. required Returns: Type Description float Wave number for a given wavelength. Source code in odak/learn/wave/__init__.py def wavenumber ( wavelength ): \"\"\" Definition for calculating the wavenumber of a plane wave. Parameters ---------- wavelength : float Wavelength of a wave in mm. Returns ------- k : float Wave number for a given wavelength. \"\"\" k = 2 * np . pi / wavelength return k","title":"odak.learn.wave.wavenumber"},{"location":"odak/learn/wave/wavenumber/#notes","text":"Regarding usage of this definition, you can find use cases in the engineering notes, specifically at Optimizing holograms using Odak .","title":"Notes"},{"location":"odak/learn/wave/wavenumber/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/tools/check_directory/","text":"odak.check_directory \u00b6 check_directory(directory) Definition to check if a directory exist. If it doesn't exist, this definition will create one. Parameters: directory : str Full directory path. Returns N/A See also \u00b6 General toolkit","title":"odak.check_directory"},{"location":"odak/tools/check_directory/#odakcheck_directory","text":"check_directory(directory) Definition to check if a directory exist. If it doesn't exist, this definition will create one. Parameters: directory : str Full directory path. Returns N/A","title":"odak.check_directory"},{"location":"odak/tools/check_directory/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/convert_bytes/","text":"odak.convert_bytes \u00b6 convert_bytes(num) A definition to convert bytes to semantic scheme (MB,GB or alike). Inspired from https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python#2104083 . Parameters: num : float Size in bytes Returns num : float Size in new unit. x : str New unit bytes, KB, MB, GB or TB. See also \u00b6 General toolkit","title":"odak.convert_bytes"},{"location":"odak/tools/convert_bytes/#odakconvert_bytes","text":"convert_bytes(num) A definition to convert bytes to semantic scheme (MB,GB or alike). Inspired from https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python#2104083 . Parameters: num : float Size in bytes Returns num : float Size in new unit. x : str New unit bytes, KB, MB, GB or TB.","title":"odak.convert_bytes"},{"location":"odak/tools/convert_bytes/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/list_files/","text":"odak.list_files \u00b6 list_files(path,key='*.*',recursive=True) Definition to list files in a given path with a given key. Parameters: path : str Path to a folder. key : str Key used for scanning a path. recursive : bool If set True, scan the path recursively. Returns files_list : ndarray list of files found in a given path. See also \u00b6 General toolkit","title":"odak.list_files"},{"location":"odak/tools/list_files/#odaklist_files","text":"list_files(path,key='*.*',recursive=True) Definition to list files in a given path with a given key. Parameters: path : str Path to a folder. key : str Key used for scanning a path. recursive : bool If set True, scan the path recursively. Returns files_list : ndarray list of files found in a given path.","title":"odak.list_files"},{"location":"odak/tools/list_files/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/load_dictionary/","text":"odak.load_dictionary \u00b6 load_dictionary(filename) Definition to load a dictionary (JSON) file. Parameters: filename : str Filename. Returns settings : dict Dictionary read from the file. See also \u00b6 General toolkit","title":"odak.load_dictionary"},{"location":"odak/tools/load_dictionary/#odakload_dictionary","text":"load_dictionary(filename) Definition to load a dictionary (JSON) file. Parameters: filename : str Filename. Returns settings : dict Dictionary read from the file.","title":"odak.load_dictionary"},{"location":"odak/tools/load_dictionary/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/load_image/","text":"odak.load_image \u00b6 load_image(fn) Definition to load an image from a given location as a Numpy array. Parameters: fn : str Filename. Returns image : ndarray Image loaded as a Numpy array. See also \u00b6 General toolkit","title":"odak.load_image"},{"location":"odak/tools/load_image/#odakload_image","text":"load_image(fn) Definition to load an image from a given location as a Numpy array. Parameters: fn : str Filename. Returns image : ndarray Image loaded as a Numpy array.","title":"odak.load_image"},{"location":"odak/tools/load_image/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/resize_image/","text":"odak.resize_image \u00b6 resize_image(img,target_size) Definition to resize a given image to a target shape. Parameters: img : ndarray MxN image to be resized. Image must be normalized (0-1). target_size : list Target shape. Returns num : float Size in new unit. x : str New unit bytes, KB, MB, GB or TB. See also \u00b6 General toolkit","title":"odak.resize_image"},{"location":"odak/tools/resize_image/#odakresize_image","text":"resize_image(img,target_size) Definition to resize a given image to a target shape. Parameters: img : ndarray MxN image to be resized. Image must be normalized (0-1). target_size : list Target shape. Returns num : float Size in new unit. x : str New unit bytes, KB, MB, GB or TB.","title":"odak.resize_image"},{"location":"odak/tools/resize_image/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/save_dictionary/","text":"odak.save_dictionary \u00b6 save_dictionary(settings,filename) Definition to load a dictionary (JSON) file. Parameters: settings : dict Dictionary read from the file. filename : str Filename. Returns N/A See also \u00b6 General toolkit","title":"odak.save_dictionary"},{"location":"odak/tools/save_dictionary/#odaksave_dictionary","text":"save_dictionary(settings,filename) Definition to load a dictionary (JSON) file. Parameters: settings : dict Dictionary read from the file. filename : str Filename. Returns N/A","title":"odak.save_dictionary"},{"location":"odak/tools/save_dictionary/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/save_image/","text":"odak.save_image \u00b6 save_image(fn,img,cmin=0,cmax=255) Definition to save a Numpy array as an image. Parameters: fn : str Filename. img : ndarray A numpy array with NxMx3 or NxMx1 shapes. cmin : int Minimum value that will be interpreted as 0 level in the final image. cmax : int Maximum value that will be interpreted as 255 level in the final image. Returns bool : bool True if successful. See also \u00b6 General toolkit","title":"odak.save_image"},{"location":"odak/tools/save_image/#odaksave_image","text":"save_image(fn,img,cmin=0,cmax=255) Definition to save a Numpy array as an image. Parameters: fn : str Filename. img : ndarray A numpy array with NxMx3 or NxMx1 shapes. cmin : int Minimum value that will be interpreted as 0 level in the final image. cmax : int Maximum value that will be interpreted as 255 level in the final image. Returns bool : bool True if successful.","title":"odak.save_image"},{"location":"odak/tools/save_image/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/shell_command/","text":"odak.shell_command \u00b6 shell_command(cmd,cwd='.',timeout=None,check=True) Definition to initiate shell commands. Parameters: fn : str Filename. img : ndarray A numpy array with NxMx3 or NxMx1 shapes. cmin : int Minimum value that will be interpreted as 0 level in the final image. cmax : int Maximum value that will be interpreted as 255 level in the final image. Returns bool : bool True if successful. See also \u00b6 General toolkit","title":"odak.shell_command"},{"location":"odak/tools/shell_command/#odakshell_command","text":"shell_command(cmd,cwd='.',timeout=None,check=True) Definition to initiate shell commands. Parameters: fn : str Filename. img : ndarray A numpy array with NxMx3 or NxMx1 shapes. cmin : int Minimum value that will be interpreted as 0 level in the final image. cmax : int Maximum value that will be interpreted as 255 level in the final image. Returns bool : bool True if successful.","title":"odak.shell_command"},{"location":"odak/tools/shell_command/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/tools/size_of_a_file/","text":"odak.size_of_a_file \u00b6 size_of_a_file(file_path) A definition to get size of a file with a relevant unit. Parameters: file_path : float Path of the file. Returns a : float Size of the file. b : str Unit of the size (bytes, KB, MB, GB or TB). See also \u00b6 General toolkit","title":"odak.size_of_a_file"},{"location":"odak/tools/size_of_a_file/#odaksize_of_a_file","text":"size_of_a_file(file_path) A definition to get size of a file with a relevant unit. Parameters: file_path : float Path of the file. Returns a : float Size of the file. b : str Unit of the size (bytes, KB, MB, GB or TB).","title":"odak.size_of_a_file"},{"location":"odak/tools/size_of_a_file/#see-also","text":"General toolkit","title":"See also"},{"location":"odak/wave/adaptive_sampling_angular_spectrum/","text":"odak.wave.adaptive_sampling_angular_spectrum \u00b6 A definition to calculate adaptive sampling angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def adaptive_sampling_angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate adaptive sampling angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" iflag = - 1 eps = 10 ** ( - 12 ) nv , nu = field . shape l = nu * dx x = np . linspace ( - l / 2 , l / 2 , nu ) y = np . linspace ( - l / 2 , l / 2 , nv ) X , Y = np . meshgrid ( x , y ) fx = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nu ) fy = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nv ) FX , FY = np . meshgrid ( fx , fy ) forig = 1. / 2. / dx fc2 = 1. / 2 * ( nu / wavelength / np . abs ( distance )) ** 0.5 ss = np . abs ( fc2 ) / forig zc = nu * dx ** 2 / wavelength K = nu / 2 / np . amax ( np . abs ( fx )) m = 2 nnu2 = m * nu nnv2 = m * nv fxn = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nnu2 ) fyn = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nnv2 ) if np . abs ( distance ) > zc * 2 : fxn = fxn * ss fyn = fyn * ss FXN , FYN = np . meshgrid ( fxn , fyn ) Hn = np . exp ( 1 j * k * distance * ( 1 - ( FXN * wavelength ) ** 2 - ( FYN * wavelength ) ** 2 ) ** 0.5 ) FX = FXN / np . amax ( FXN ) * np . pi FY = FYN / np . amax ( FYN ) * np . pi t_2 = nufft2 ( field , FX * ss , FY * ss , size = [ nnv2 , nnu2 ], sign = iflag , eps = eps ) FX = FX / np . amax ( FX ) * np . pi FY = FY / np . amax ( FY ) * np . pi result = nuifft2 ( Hn * t_2 , FX * ss , FY * ss , size = [ nv , nu ], sign =- iflag , eps = eps ) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.adaptive_sampling_angular_spectrum"},{"location":"odak/wave/adaptive_sampling_angular_spectrum/#odakwaveadaptive_sampling_angular_spectrum","text":"A definition to calculate adaptive sampling angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def adaptive_sampling_angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate adaptive sampling angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" iflag = - 1 eps = 10 ** ( - 12 ) nv , nu = field . shape l = nu * dx x = np . linspace ( - l / 2 , l / 2 , nu ) y = np . linspace ( - l / 2 , l / 2 , nv ) X , Y = np . meshgrid ( x , y ) fx = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nu ) fy = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nv ) FX , FY = np . meshgrid ( fx , fy ) forig = 1. / 2. / dx fc2 = 1. / 2 * ( nu / wavelength / np . abs ( distance )) ** 0.5 ss = np . abs ( fc2 ) / forig zc = nu * dx ** 2 / wavelength K = nu / 2 / np . amax ( np . abs ( fx )) m = 2 nnu2 = m * nu nnv2 = m * nv fxn = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nnu2 ) fyn = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nnv2 ) if np . abs ( distance ) > zc * 2 : fxn = fxn * ss fyn = fyn * ss FXN , FYN = np . meshgrid ( fxn , fyn ) Hn = np . exp ( 1 j * k * distance * ( 1 - ( FXN * wavelength ) ** 2 - ( FYN * wavelength ) ** 2 ) ** 0.5 ) FX = FXN / np . amax ( FXN ) * np . pi FY = FYN / np . amax ( FYN ) * np . pi t_2 = nufft2 ( field , FX * ss , FY * ss , size = [ nnv2 , nnu2 ], sign = iflag , eps = eps ) FX = FX / np . amax ( FX ) * np . pi FY = FY / np . amax ( FY ) * np . pi result = nuifft2 ( Hn * t_2 , FX * ss , FY * ss , size = [ nv , nu ], sign =- iflag , eps = eps ) return result","title":"odak.wave.adaptive_sampling_angular_spectrum"},{"location":"odak/wave/adaptive_sampling_angular_spectrum/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/add_phase/","text":"odak.wave.add_phase \u00b6 Definition for adding a phase to a given complex field. Parameters: Name Type Description Default field np.complex64 Complex field. required new_phase np.complex64 Complex phase. required Returns: Type Description np.complex64 Complex field. Source code in odak/wave/__init__.py def add_phase ( field , new_phase ): \"\"\" Definition for adding a phase to a given complex field. Parameters ---------- field : np.complex64 Complex field. new_phase : np.complex64 Complex phase. Returns ------- new_field : np.complex64 Complex field. \"\"\" phase = calculate_phase ( field ) amplitude = calculate_amplitude ( field ) new_field = amplitude * np . cos ( phase + new_phase ) + \\ 1 j * amplitude * np . sin ( phase + new_phase ) return new_field See also \u00b6 Computer Generated-Holography","title":"odak.wave.add_phase"},{"location":"odak/wave/add_phase/#odakwaveadd_phase","text":"Definition for adding a phase to a given complex field. Parameters: Name Type Description Default field np.complex64 Complex field. required new_phase np.complex64 Complex phase. required Returns: Type Description np.complex64 Complex field. Source code in odak/wave/__init__.py def add_phase ( field , new_phase ): \"\"\" Definition for adding a phase to a given complex field. Parameters ---------- field : np.complex64 Complex field. new_phase : np.complex64 Complex phase. Returns ------- new_field : np.complex64 Complex field. \"\"\" phase = calculate_phase ( field ) amplitude = calculate_amplitude ( field ) new_field = amplitude * np . cos ( phase + new_phase ) + \\ 1 j * amplitude * np . sin ( phase + new_phase ) return new_field","title":"odak.wave.add_phase"},{"location":"odak/wave/add_phase/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/add_random_phase/","text":"odak.wave.add_random_phase \u00b6 Definition for adding a random phase to a given complex field. Parameters: Name Type Description Default field np.complex64 Complex field. required Returns: Type Description np.complex64 Complex field. Source code in odak/wave/__init__.py def add_random_phase ( field ): \"\"\" Definition for adding a random phase to a given complex field. Parameters ---------- field : np.complex64 Complex field. Returns ------- new_field : np.complex64 Complex field. \"\"\" random_phase = np . pi * np . random . random ( field . shape ) new_field = add_phase ( field , random_phase ) return new_field See also \u00b6 Computer Generated-Holography","title":"odak.wave.add_random_phase"},{"location":"odak/wave/add_random_phase/#odakwaveadd_random_phase","text":"Definition for adding a random phase to a given complex field. Parameters: Name Type Description Default field np.complex64 Complex field. required Returns: Type Description np.complex64 Complex field. Source code in odak/wave/__init__.py def add_random_phase ( field ): \"\"\" Definition for adding a random phase to a given complex field. Parameters ---------- field : np.complex64 Complex field. Returns ------- new_field : np.complex64 Complex field. \"\"\" random_phase = np . pi * np . random . random ( field . shape ) new_field = add_phase ( field , random_phase ) return new_field","title":"odak.wave.add_random_phase"},{"location":"odak/wave/add_random_phase/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/adjust_phase_only_slm_range/","text":"odak.wave.adjust_phase_only_slm_range \u00b6 Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well. Parameters: Name Type Description Default native_range float Native range of the phase only SLM in radians (i.e. two pi). required working_wavelength float Wavelength of the illumination source or some working wavelength. required native_wavelength float Wavelength which the SLM is designed for. required Returns: Type Description float Calculated phase range in radians. Source code in odak/wave/__init__.py def adjust_phase_only_slm_range ( native_range , working_wavelength , native_wavelength ): \"\"\" Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well. Parameters ---------- native_range : float Native range of the phase only SLM in radians (i.e. two pi). working_wavelength : float Wavelength of the illumination source or some working wavelength. native_wavelength : float Wavelength which the SLM is designed for. Returns ------- new_range : float Calculated phase range in radians. \"\"\" new_range = native_range / working_wavelength * native_wavelength return new_range See also \u00b6 Computer Generated-Holography","title":"odak.wave.adjust_phase_only_slm_range"},{"location":"odak/wave/adjust_phase_only_slm_range/#odakwaveadjust_phase_only_slm_range","text":"Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well. Parameters: Name Type Description Default native_range float Native range of the phase only SLM in radians (i.e. two pi). required working_wavelength float Wavelength of the illumination source or some working wavelength. required native_wavelength float Wavelength which the SLM is designed for. required Returns: Type Description float Calculated phase range in radians. Source code in odak/wave/__init__.py def adjust_phase_only_slm_range ( native_range , working_wavelength , native_wavelength ): \"\"\" Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well. Parameters ---------- native_range : float Native range of the phase only SLM in radians (i.e. two pi). working_wavelength : float Wavelength of the illumination source or some working wavelength. native_wavelength : float Wavelength which the SLM is designed for. Returns ------- new_range : float Calculated phase range in radians. \"\"\" new_range = native_range / working_wavelength * native_wavelength return new_range","title":"odak.wave.adjust_phase_only_slm_range"},{"location":"odak/wave/adjust_phase_only_slm_range/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/angular_spectrum/","text":"odak.wave.angular_spectrum \u00b6 A definition to calculate angular spectrum based beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate angular spectrum based beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape x = np . linspace ( - nu / 2 * dx , nu / 2 * dx , nu ) y = np . linspace ( - nv / 2 * dx , nv / 2 * dx , nv ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 h = 1. / ( 1 j * wavelength * distance ) * np . exp ( 1 j * k * ( distance + Z / 2 / distance )) h = np . fft . fft2 ( np . fft . fftshift ( h )) * dx ** 2 U1 = np . fft . fft2 ( np . fft . fftshift ( field )) U2 = h * U1 result = np . fft . ifftshift ( np . fft . ifft2 ( U2 )) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.angular_spectrum"},{"location":"odak/wave/angular_spectrum/#odakwaveangular_spectrum","text":"A definition to calculate angular spectrum based beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate angular spectrum based beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape x = np . linspace ( - nu / 2 * dx , nu / 2 * dx , nu ) y = np . linspace ( - nv / 2 * dx , nv / 2 * dx , nv ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 h = 1. / ( 1 j * wavelength * distance ) * np . exp ( 1 j * k * ( distance + Z / 2 / distance )) h = np . fft . fft2 ( np . fft . fftshift ( h )) * dx ** 2 U1 = np . fft . fft2 ( np . fft . fftshift ( field )) U2 = h * U1 result = np . fft . ifftshift ( np . fft . ifft2 ( U2 )) return result","title":"odak.wave.angular_spectrum"},{"location":"odak/wave/angular_spectrum/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/band_extended_angular_spectrum/","text":"odak.wave.band_extended_angular_spectrum \u00b6 A definition to calculate bandextended angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics Letters 45.6 (2020): 1543-1546. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def band_extended_angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate bandextended angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics Letters 45.6 (2020): 1543-1546. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" iflag = - 1 eps = 10 ** ( - 12 ) nv , nu = field . shape l = nu * dx x = np . linspace ( - l / 2 , l / 2 , nu ) y = np . linspace ( - l / 2 , l / 2 , nv ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 fx = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nu ) fy = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nv ) FX , FY = np . meshgrid ( fx , fy ) K = nu / 2 / np . amax ( fx ) fcn = 1. / 2 * ( nu / wavelength / np . abs ( distance )) ** 0.5 ss = np . abs ( fcn ) / np . amax ( np . abs ( fx )) zc = nu * dx ** 2 / wavelength if np . abs ( distance ) < zc : fxn = fx fyn = fy else : fxn = fx * ss fyn = fy * ss FXN , FYN = np . meshgrid ( fxn , fyn ) Hn = np . exp ( 1 j * k * distance * ( 1 - ( FXN * wavelength ) ** 2 - ( FYN * wavelength ) ** 2 ) ** 0.5 ) X = X / np . amax ( X ) * np . pi Y = Y / np . amax ( Y ) * np . pi t_asmNUFT = nufft2 ( field , X * ss , Y * ss , sign = iflag , eps = eps ) result = nuifft2 ( Hn * t_asmNUFT , X * ss , Y * ss , sign =- iflag , eps = eps ) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.band_extended_angular_spectrum"},{"location":"odak/wave/band_extended_angular_spectrum/#odakwaveband_extended_angular_spectrum","text":"A definition to calculate bandextended angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics Letters 45.6 (2020): 1543-1546. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def band_extended_angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate bandextended angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics Letters 45.6 (2020): 1543-1546. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" iflag = - 1 eps = 10 ** ( - 12 ) nv , nu = field . shape l = nu * dx x = np . linspace ( - l / 2 , l / 2 , nu ) y = np . linspace ( - l / 2 , l / 2 , nv ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 fx = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nu ) fy = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nv ) FX , FY = np . meshgrid ( fx , fy ) K = nu / 2 / np . amax ( fx ) fcn = 1. / 2 * ( nu / wavelength / np . abs ( distance )) ** 0.5 ss = np . abs ( fcn ) / np . amax ( np . abs ( fx )) zc = nu * dx ** 2 / wavelength if np . abs ( distance ) < zc : fxn = fx fyn = fy else : fxn = fx * ss fyn = fy * ss FXN , FYN = np . meshgrid ( fxn , fyn ) Hn = np . exp ( 1 j * k * distance * ( 1 - ( FXN * wavelength ) ** 2 - ( FYN * wavelength ) ** 2 ) ** 0.5 ) X = X / np . amax ( X ) * np . pi Y = Y / np . amax ( Y ) * np . pi t_asmNUFT = nufft2 ( field , X * ss , Y * ss , sign = iflag , eps = eps ) result = nuifft2 ( Hn * t_asmNUFT , X * ss , Y * ss , sign =- iflag , eps = eps ) return result","title":"odak.wave.band_extended_angular_spectrum"},{"location":"odak/wave/band_extended_angular_spectrum/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/band_limited_angular_spectrum/","text":"odak.wave.band_limited_angular_spectrum \u00b6 A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def band_limited_angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape x = np . linspace ( - nu / 2 * dx , nu / 2 * dx , nu ) y = np . linspace ( - nv / 2 * dx , nv / 2 * dx , nv ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 h = 1. / ( 1 j * wavelength * distance ) * np . exp ( 1 j * k * ( distance + Z / 2 / distance )) h = np . fft . fft2 ( np . fft . fftshift ( h )) * dx ** 2 flimx = np . ceil ( 1 / ((( 2 * distance * ( 1. / ( nu ))) ** 2 + 1 ) ** 0.5 * wavelength )) flimy = np . ceil ( 1 / ((( 2 * distance * ( 1. / ( nv ))) ** 2 + 1 ) ** 0.5 * wavelength )) mask = np . zeros (( nu , nv ), dtype = np . complex64 ) mask = ( np . abs ( X ) < flimx ) & ( np . abs ( Y ) < flimy ) mask = set_amplitude ( h , mask ) U1 = np . fft . fft2 ( np . fft . fftshift ( field )) U2 = mask * U1 result = np . fft . ifftshift ( np . fft . ifft2 ( U2 )) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.band_limited_angular_spectrum"},{"location":"odak/wave/band_limited_angular_spectrum/#odakwaveband_limited_angular_spectrum","text":"A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def band_limited_angular_spectrum ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape x = np . linspace ( - nu / 2 * dx , nu / 2 * dx , nu ) y = np . linspace ( - nv / 2 * dx , nv / 2 * dx , nv ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 h = 1. / ( 1 j * wavelength * distance ) * np . exp ( 1 j * k * ( distance + Z / 2 / distance )) h = np . fft . fft2 ( np . fft . fftshift ( h )) * dx ** 2 flimx = np . ceil ( 1 / ((( 2 * distance * ( 1. / ( nu ))) ** 2 + 1 ) ** 0.5 * wavelength )) flimy = np . ceil ( 1 / ((( 2 * distance * ( 1. / ( nv ))) ** 2 + 1 ) ** 0.5 * wavelength )) mask = np . zeros (( nu , nv ), dtype = np . complex64 ) mask = ( np . abs ( X ) < flimx ) & ( np . abs ( Y ) < flimy ) mask = set_amplitude ( h , mask ) U1 = np . fft . fft2 ( np . fft . fftshift ( field )) U2 = mask * U1 result = np . fft . ifftshift ( np . fft . ifft2 ( U2 )) return result","title":"odak.wave.band_limited_angular_spectrum"},{"location":"odak/wave/band_limited_angular_spectrum/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/calculate_amplitude/","text":"odak.wave.calculate_amplitude \u00b6 Definition to calculate amplitude of a single or multiple given electric field(s). Parameters: Name Type Description Default field ndarray.complex or complex Electric fields or an electric field. required Returns: Type Description float Amplitude or amplitudes of electric field(s). Source code in odak/wave/utils.py def calculate_amplitude ( field ): \"\"\" Definition to calculate amplitude of a single or multiple given electric field(s). Parameters ---------- field : ndarray.complex or complex Electric fields or an electric field. Returns ------- amplitude : float Amplitude or amplitudes of electric field(s). \"\"\" amplitude = np . abs ( field ) return amplitude See also \u00b6 Computer Generated-Holography","title":"odak.wave.calculate_amplitude"},{"location":"odak/wave/calculate_amplitude/#odakwavecalculate_amplitude","text":"Definition to calculate amplitude of a single or multiple given electric field(s). Parameters: Name Type Description Default field ndarray.complex or complex Electric fields or an electric field. required Returns: Type Description float Amplitude or amplitudes of electric field(s). Source code in odak/wave/utils.py def calculate_amplitude ( field ): \"\"\" Definition to calculate amplitude of a single or multiple given electric field(s). Parameters ---------- field : ndarray.complex or complex Electric fields or an electric field. Returns ------- amplitude : float Amplitude or amplitudes of electric field(s). \"\"\" amplitude = np . abs ( field ) return amplitude","title":"odak.wave.calculate_amplitude"},{"location":"odak/wave/calculate_amplitude/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/calculate_intensity/","text":"odak.wave.calculate_intensity \u00b6 Definition to calculate intensity of a single or multiple given electric field(s). Parameters: Name Type Description Default field ndarray.complex or complex Electric fields or an electric field. required Returns: Type Description float Intensity or intensities of electric field(s). Source code in odak/wave/__init__.py def calculate_intensity ( field ): \"\"\" Definition to calculate intensity of a single or multiple given electric field(s). Parameters ---------- field : ndarray.complex or complex Electric fields or an electric field. Returns ------- intensity : float Intensity or intensities of electric field(s). \"\"\" intensity = np . abs ( field ) ** 2 return intensity See also \u00b6 Computer Generated-Holography","title":"odak.wave.calculate_intensity"},{"location":"odak/wave/calculate_intensity/#odakwavecalculate_intensity","text":"Definition to calculate intensity of a single or multiple given electric field(s). Parameters: Name Type Description Default field ndarray.complex or complex Electric fields or an electric field. required Returns: Type Description float Intensity or intensities of electric field(s). Source code in odak/wave/__init__.py def calculate_intensity ( field ): \"\"\" Definition to calculate intensity of a single or multiple given electric field(s). Parameters ---------- field : ndarray.complex or complex Electric fields or an electric field. Returns ------- intensity : float Intensity or intensities of electric field(s). \"\"\" intensity = np . abs ( field ) ** 2 return intensity","title":"odak.wave.calculate_intensity"},{"location":"odak/wave/calculate_intensity/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/calculate_phase/","text":"odak.wave.calculate_phase \u00b6 Definition to calculate phase of a single or multiple given electric field(s). Parameters: Name Type Description Default field ndarray.complex or complex Electric fields or an electric field. required deg bool If set True, the angles will be returned in degrees. False Returns: Type Description float Phase or phases of electric field(s) in radians. Source code in odak/wave/utils.py def calculate_phase ( field , deg = False ): \"\"\" Definition to calculate phase of a single or multiple given electric field(s). Parameters ---------- field : ndarray.complex or complex Electric fields or an electric field. deg : bool If set True, the angles will be returned in degrees. Returns ------- phase : float Phase or phases of electric field(s) in radians. \"\"\" phase = np . angle ( field ) if deg == True : phase *= 180. / np . pi return phase See also \u00b6 Computer Generated-Holography","title":"odak.wave.calculate_phase"},{"location":"odak/wave/calculate_phase/#odakwavecalculate_phase","text":"Definition to calculate phase of a single or multiple given electric field(s). Parameters: Name Type Description Default field ndarray.complex or complex Electric fields or an electric field. required deg bool If set True, the angles will be returned in degrees. False Returns: Type Description float Phase or phases of electric field(s) in radians. Source code in odak/wave/utils.py def calculate_phase ( field , deg = False ): \"\"\" Definition to calculate phase of a single or multiple given electric field(s). Parameters ---------- field : ndarray.complex or complex Electric fields or an electric field. deg : bool If set True, the angles will be returned in degrees. Returns ------- phase : float Phase or phases of electric field(s) in radians. \"\"\" phase = np . angle ( field ) if deg == True : phase *= 180. / np . pi return phase","title":"odak.wave.calculate_phase"},{"location":"odak/wave/calculate_phase/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/double_convergence/","text":"odak.wave.double_convergence \u00b6 A definition to generate initial phase for a Gerchberg-Saxton method. For more details consult Sun, Peng, et al. \"Holographic near-eye display system based on double-convergence light Gerchberg-Saxton algorithm.\" Optics express 26.8 (2018): 10140-10151. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required k odak.wave.wavenumber See odak.wave.wavenumber for more. required r float The distance between location of a light source and an image plane. required dx float Pixel pitch. required Returns: Type Description ndarray Generated phase pattern for a Gerchberg-Saxton method. Source code in odak/wave/lens.py def double_convergence ( nx , ny , k , r , dx ): \"\"\" A definition to generate initial phase for a Gerchberg-Saxton method. For more details consult Sun, Peng, et al. \"Holographic near-eye display system based on double-convergence light Gerchberg-Saxton algorithm.\" Optics express 26.8 (2018): 10140-10151. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. k : odak.wave.wavenumber See odak.wave.wavenumber for more. r : float The distance between location of a light source and an image plane. dx : float Pixel pitch. Returns ------- function : ndarray Generated phase pattern for a Gerchberg-Saxton method. \"\"\" size = [ ny , nx ] x = np . linspace ( - size [ 0 ] * dx / 2 , size [ 0 ] * dx / 2 , size [ 0 ]) y = np . linspace ( - size [ 1 ] * dx / 2 , size [ 1 ] * dx / 2 , size [ 1 ]) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 w = np . exp ( 1 j * k * Z / r ) return w See also \u00b6 Computer Generated-Holography","title":"odak.wave.double_convergence"},{"location":"odak/wave/double_convergence/#odakwavedouble_convergence","text":"A definition to generate initial phase for a Gerchberg-Saxton method. For more details consult Sun, Peng, et al. \"Holographic near-eye display system based on double-convergence light Gerchberg-Saxton algorithm.\" Optics express 26.8 (2018): 10140-10151. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required k odak.wave.wavenumber See odak.wave.wavenumber for more. required r float The distance between location of a light source and an image plane. required dx float Pixel pitch. required Returns: Type Description ndarray Generated phase pattern for a Gerchberg-Saxton method. Source code in odak/wave/lens.py def double_convergence ( nx , ny , k , r , dx ): \"\"\" A definition to generate initial phase for a Gerchberg-Saxton method. For more details consult Sun, Peng, et al. \"Holographic near-eye display system based on double-convergence light Gerchberg-Saxton algorithm.\" Optics express 26.8 (2018): 10140-10151. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. k : odak.wave.wavenumber See odak.wave.wavenumber for more. r : float The distance between location of a light source and an image plane. dx : float Pixel pitch. Returns ------- function : ndarray Generated phase pattern for a Gerchberg-Saxton method. \"\"\" size = [ ny , nx ] x = np . linspace ( - size [ 0 ] * dx / 2 , size [ 0 ] * dx / 2 , size [ 0 ]) y = np . linspace ( - size [ 1 ] * dx / 2 , size [ 1 ] * dx / 2 , size [ 1 ]) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 w = np . exp ( 1 j * k * Z / r ) return w","title":"odak.wave.double_convergence"},{"location":"odak/wave/double_convergence/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/electric_field_per_plane_wave/","text":"odak.wave.electric_field_per_plane_wave \u00b6 Definition to return state of a plane wave at a particular distance and time. Parameters: Name Type Description Default amplitude float Amplitude of a wave. required opd float Optical path difference in mm. required k float Wave number of a wave, see odak.wave.parameters.wavenumber for more. required phase float Initial phase of a wave. 0 w float Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more. 0 t float Time in seconds. 0 Returns: Type Description complex A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)). Source code in odak/wave/vector.py def electric_field_per_plane_wave ( amplitude , opd , k , phase = 0 , w = 0 , t = 0 ): \"\"\" Definition to return state of a plane wave at a particular distance and time. Parameters ---------- amplitude : float Amplitude of a wave. opd : float Optical path difference in mm. k : float Wave number of a wave, see odak.wave.parameters.wavenumber for more. phase : float Initial phase of a wave. w : float Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more. t : float Time in seconds. Returns ------- field : complex A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)). \"\"\" field = amplitude * np . exp ( 1 j * ( - w * t + opd * k + phase )) / opd ** 2 return field See also \u00b6 Computer Generated-Holography","title":"odak.wave.electric_field_per_plane_wave"},{"location":"odak/wave/electric_field_per_plane_wave/#odakwaveelectric_field_per_plane_wave","text":"Definition to return state of a plane wave at a particular distance and time. Parameters: Name Type Description Default amplitude float Amplitude of a wave. required opd float Optical path difference in mm. required k float Wave number of a wave, see odak.wave.parameters.wavenumber for more. required phase float Initial phase of a wave. 0 w float Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more. 0 t float Time in seconds. 0 Returns: Type Description complex A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)). Source code in odak/wave/vector.py def electric_field_per_plane_wave ( amplitude , opd , k , phase = 0 , w = 0 , t = 0 ): \"\"\" Definition to return state of a plane wave at a particular distance and time. Parameters ---------- amplitude : float Amplitude of a wave. opd : float Optical path difference in mm. k : float Wave number of a wave, see odak.wave.parameters.wavenumber for more. phase : float Initial phase of a wave. w : float Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more. t : float Time in seconds. Returns ------- field : complex A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)). \"\"\" field = amplitude * np . exp ( 1 j * ( - w * t + opd * k + phase )) / opd ** 2 return field","title":"odak.wave.electric_field_per_plane_wave"},{"location":"odak/wave/electric_field_per_plane_wave/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/fraunhofer/","text":"odak.wave.fraunhofer \u00b6 fraunhofer(field,k,distance,dx,wavelength) A definition to calculate Fraunhofer based beam propagation. Parameters: field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns result : np.complex Final complex field (MxN). See also \u00b6 Computer Generated-Holography","title":"odak.wave.fraunhofer"},{"location":"odak/wave/fraunhofer/#odakwavefraunhofer","text":"fraunhofer(field,k,distance,dx,wavelength) A definition to calculate Fraunhofer based beam propagation. Parameters: field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns result : np.complex Final complex field (MxN).","title":"odak.wave.fraunhofer"},{"location":"odak/wave/fraunhofer/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/fraunhofer_equal_size_adjust/","text":"odak.wave.fraunhofer_equal_size_adjust \u00b6 A definition to match the physical size of the original field with the propagated field. Parameters: Name Type Description Default field np.complex Complex field (MxN). required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def fraunhofer_equal_size_adjust ( field , distance , dx , wavelength ): \"\"\" A definition to match the physical size of the original field with the propagated field. Parameters ---------- field : np.complex Complex field (MxN). distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- new_field : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape l1 = nu * dx l2 = wavelength * distance / dx m = l1 / l2 px = int ( m * nu ) py = int ( m * nv ) nx = int ( field . shape [ 0 ] / 2 - px / 2 ) ny = int ( field . shape [ 1 ] / 2 - py / 2 ) new_field = np . copy ( field [ nx : nx + px , ny : ny + py ]) return new_field See also \u00b6 Computer Generated-Holography","title":"odak.wave.fraunhofer_equal_size_adjust"},{"location":"odak/wave/fraunhofer_equal_size_adjust/#odakwavefraunhofer_equal_size_adjust","text":"A definition to match the physical size of the original field with the propagated field. Parameters: Name Type Description Default field np.complex Complex field (MxN). required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def fraunhofer_equal_size_adjust ( field , distance , dx , wavelength ): \"\"\" A definition to match the physical size of the original field with the propagated field. Parameters ---------- field : np.complex Complex field (MxN). distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- new_field : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape l1 = nu * dx l2 = wavelength * distance / dx m = l1 / l2 px = int ( m * nu ) py = int ( m * nv ) nx = int ( field . shape [ 0 ] / 2 - px / 2 ) ny = int ( field . shape [ 1 ] / 2 - py / 2 ) new_field = np . copy ( field [ nx : nx + px , ny : ny + py ]) return new_field","title":"odak.wave.fraunhofer_equal_size_adjust"},{"location":"odak/wave/fraunhofer_equal_size_adjust/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/fraunhofer_inverse/","text":"odak.wave.fraunhofer_inverse \u00b6 A definition to calculate Inverse Fraunhofer based beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def fraunhofer_inverse ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate Inverse Fraunhofer based beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" distance = np . abs ( distance ) nv , nu = field . shape l = nu * dx l2 = wavelength * distance / dx dx2 = wavelength * distance / l fx = np . linspace ( - l2 / 2. , l2 / 2. , nu ) fy = np . linspace ( - l2 / 2. , l2 / 2. , nv ) FX , FY = np . meshgrid ( fx , fy ) FZ = FX ** 2 + FY ** 2 c = np . exp ( 1 j * k * distance ) / ( 1 j * wavelength * distance ) * \\ np . exp ( 1 j * k / ( 2 * distance ) * FZ ) result = np . fft . fftshift ( np . fft . ifft2 ( np . fft . ifftshift ( field / dx ** 2 / c ))) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.fraunhofer_inverse"},{"location":"odak/wave/fraunhofer_inverse/#odakwavefraunhofer_inverse","text":"A definition to calculate Inverse Fraunhofer based beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def fraunhofer_inverse ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate Inverse Fraunhofer based beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" distance = np . abs ( distance ) nv , nu = field . shape l = nu * dx l2 = wavelength * distance / dx dx2 = wavelength * distance / l fx = np . linspace ( - l2 / 2. , l2 / 2. , nu ) fy = np . linspace ( - l2 / 2. , l2 / 2. , nv ) FX , FY = np . meshgrid ( fx , fy ) FZ = FX ** 2 + FY ** 2 c = np . exp ( 1 j * k * distance ) / ( 1 j * wavelength * distance ) * \\ np . exp ( 1 j * k / ( 2 * distance ) * FZ ) result = np . fft . fftshift ( np . fft . ifft2 ( np . fft . ifftshift ( field / dx ** 2 / c ))) return result","title":"odak.wave.fraunhofer_inverse"},{"location":"odak/wave/fraunhofer_inverse/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/generate_complex_field/","text":"odak.wave.generate_complex_field \u00b6 Definition to generate a complex field with a given amplitude and phase. Parameters: Name Type Description Default amplitude ndarray Amplitude of the field. required phase ndarray Phase of the field. required Returns: Type Description ndarray Complex field. Source code in odak/wave/__init__.py def generate_complex_field ( amplitude , phase ): \"\"\" Definition to generate a complex field with a given amplitude and phase. Parameters ---------- amplitude : ndarray Amplitude of the field. phase : ndarray Phase of the field. Returns ------- field : ndarray Complex field. \"\"\" field = amplitude * np . cos ( phase ) + 1 j * amplitude * np . sin ( phase ) return field See also \u00b6 Computer Generated-Holography","title":"odak.wave.generate_complex_field"},{"location":"odak/wave/generate_complex_field/#odakwavegenerate_complex_field","text":"Definition to generate a complex field with a given amplitude and phase. Parameters: Name Type Description Default amplitude ndarray Amplitude of the field. required phase ndarray Phase of the field. required Returns: Type Description ndarray Complex field. Source code in odak/wave/__init__.py def generate_complex_field ( amplitude , phase ): \"\"\" Definition to generate a complex field with a given amplitude and phase. Parameters ---------- amplitude : ndarray Amplitude of the field. phase : ndarray Phase of the field. Returns ------- field : ndarray Complex field. \"\"\" field = amplitude * np . cos ( phase ) + 1 j * amplitude * np . sin ( phase ) return field","title":"odak.wave.generate_complex_field"},{"location":"odak/wave/generate_complex_field/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/gerchberg_saxton/","text":"odak.wave.gerchberg_saxton \u00b6 Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246. Parameters: Name Type Description Default field np.complex64 Complex field (MxN). required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required slm_range float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. 6.28 propagation_type str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). 'IR Fresnel' initial_phase np.complex64 Phase to be added to the initial value. None Returns: Type Description np.complex Calculated complex hologram. Source code in odak/wave/classical.py def gerchberg_saxton ( field , n_iterations , distance , dx , wavelength , slm_range = 6.28 , propagation_type = 'IR Fresnel' , initial_phase = None ): \"\"\" Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246. Parameters ---------- field : np.complex64 Complex field (MxN). distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. slm_range : float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. propagation_type : str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). initial_phase : np.complex64 Phase to be added to the initial value. Returns ------- hologram : np.complex Calculated complex hologram. reconstruction : np.complex Calculated reconstruction using calculated hologram. \"\"\" k = wavenumber ( wavelength ) target = calculate_amplitude ( field ) hologram = generate_complex_field ( np . ones ( field . shape ), 0 ) hologram = zero_pad ( hologram ) if type ( initial_phase ) == type ( None ): hologram = add_random_phase ( hologram ) else : initial_phase = zero_pad ( initial_phase ) hologram = add_phase ( hologram , initial_phase ) center = [ int ( hologram . shape [ 0 ] / 2. ), int ( hologram . shape [ 1 ] / 2. )] orig_shape = [ int ( field . shape [ 0 ] / 2. ), int ( field . shape [ 1 ] / 2. )] for i in tqdm ( range ( n_iterations ), leave = False ): reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) new_target = calculate_amplitude ( reconstruction ) new_target [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] = target reconstruction = generate_complex_field ( new_target , calculate_phase ( reconstruction )) hologram = propagate_beam ( reconstruction , k , - distance , dx , wavelength , propagation_type ) hologram = generate_complex_field ( 1 , calculate_phase ( hologram )) hologram = hologram [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ], ] hologram = zero_pad ( hologram ) reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) hologram = hologram [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] reconstruction = reconstruction [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] return hologram , reconstruction See also \u00b6 Computer Generated-Holography","title":"odak.wave.gerchberg_saxton"},{"location":"odak/wave/gerchberg_saxton/#odakwavegerchberg_saxton","text":"Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246. Parameters: Name Type Description Default field np.complex64 Complex field (MxN). required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required slm_range float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. 6.28 propagation_type str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). 'IR Fresnel' initial_phase np.complex64 Phase to be added to the initial value. None Returns: Type Description np.complex Calculated complex hologram. Source code in odak/wave/classical.py def gerchberg_saxton ( field , n_iterations , distance , dx , wavelength , slm_range = 6.28 , propagation_type = 'IR Fresnel' , initial_phase = None ): \"\"\" Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246. Parameters ---------- field : np.complex64 Complex field (MxN). distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. slm_range : float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. propagation_type : str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). initial_phase : np.complex64 Phase to be added to the initial value. Returns ------- hologram : np.complex Calculated complex hologram. reconstruction : np.complex Calculated reconstruction using calculated hologram. \"\"\" k = wavenumber ( wavelength ) target = calculate_amplitude ( field ) hologram = generate_complex_field ( np . ones ( field . shape ), 0 ) hologram = zero_pad ( hologram ) if type ( initial_phase ) == type ( None ): hologram = add_random_phase ( hologram ) else : initial_phase = zero_pad ( initial_phase ) hologram = add_phase ( hologram , initial_phase ) center = [ int ( hologram . shape [ 0 ] / 2. ), int ( hologram . shape [ 1 ] / 2. )] orig_shape = [ int ( field . shape [ 0 ] / 2. ), int ( field . shape [ 1 ] / 2. )] for i in tqdm ( range ( n_iterations ), leave = False ): reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) new_target = calculate_amplitude ( reconstruction ) new_target [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] = target reconstruction = generate_complex_field ( new_target , calculate_phase ( reconstruction )) hologram = propagate_beam ( reconstruction , k , - distance , dx , wavelength , propagation_type ) hologram = generate_complex_field ( 1 , calculate_phase ( hologram )) hologram = hologram [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ], ] hologram = zero_pad ( hologram ) reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) hologram = hologram [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] reconstruction = reconstruction [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] return hologram , reconstruction","title":"odak.wave.gerchberg_saxton"},{"location":"odak/wave/gerchberg_saxton/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/gerchberg_saxton_3d/","text":"odak.wave.gerchberg_saxton_3d \u00b6 Definition to compute a multi plane hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Zhou, Pengcheng, et al. \"30.4: Multi\u2010plane holographic display with a uniform 3D Gerchberg\u2010Saxton algorithm.\" SID Symposium Digest of Technical Papers. Vol. 46. No. 1. 2015. Parameters: Name Type Description Default fields np.complex64 Complex fields (MxN). required distances list Propagation distances. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required slm_range float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. 6.28 propagation_type str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). 'IR Fresnel' initial_phase np.complex64 Phase to be added to the initial value. None target_type str Target type. No constraint targets the input target as is. Double constraint follows the idea in this paper, which claims to suppress speckle: Chang, Chenliang, et al. \"Speckle-suppressed phase-only holographic three-dimensional display based on double-constraint Gerchberg\u2013Saxton algorithm.\" Applied optics 54.23 (2015): 6994-7001. 'no constraint' Returns: Type Description np.complex Calculated complex hologram. Source code in odak/wave/classical.py def gerchberg_saxton_3d ( fields , n_iterations , distances , dx , wavelength , slm_range = 6.28 , propagation_type = 'IR Fresnel' , initial_phase = None , target_type = 'no constraint' , coefficients = None ): \"\"\" Definition to compute a multi plane hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Zhou, Pengcheng, et al. \"30.4: Multi\u2010plane holographic display with a uniform 3D Gerchberg\u2010Saxton algorithm.\" SID Symposium Digest of Technical Papers. Vol. 46. No. 1. 2015. Parameters ---------- fields : np.complex64 Complex fields (MxN). distances : list Propagation distances. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. slm_range : float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. propagation_type : str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). initial_phase : np.complex64 Phase to be added to the initial value. target_type : str Target type. `No constraint` targets the input target as is. `Double constraint` follows the idea in this paper, which claims to suppress speckle: Chang, Chenliang, et al. \"Speckle-suppressed phase-only holographic three-dimensional display based on double-constraint Gerchberg\u2013Saxton algorithm.\" Applied optics 54.23 (2015): 6994-7001. Returns ------- hologram : np.complex Calculated complex hologram. \"\"\" k = wavenumber ( wavelength ) targets = calculate_amplitude ( np . asarray ( fields )) . astype ( np . float ) hologram = generate_complex_field ( np . ones ( targets [ 0 ] . shape ), 0 ) hologram = zero_pad ( hologram ) if type ( initial_phase ) == type ( None ): hologram = add_random_phase ( hologram ) else : initial_phase = zero_pad ( initial_phase ) hologram = add_phase ( hologram , initial_phase ) center = [ int ( hologram . shape [ 0 ] / 2. ), int ( hologram . shape [ 1 ] / 2. )] orig_shape = [ int ( fields [ 0 ] . shape [ 0 ] / 2. ), int ( fields [ 0 ] . shape [ 1 ] / 2. )] holograms = np . zeros ( ( len ( distances ), hologram . shape [ 0 ], hologram . shape [ 1 ]), dtype = np . complex64 ) for i in tqdm ( range ( n_iterations ), leave = False ): for distance_id in tqdm ( range ( len ( distances )), leave = False ): distance = distances [ distance_id ] reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) if target_type == 'double constraint' : if type ( coefficients ) == type ( None ): raise Exception ( \"Provide coeeficients of alpha,beta and gamma for double constraint.\" ) alpha = coefficients [ 0 ] beta = coefficients [ 1 ] gamma = coefficients [ 2 ] target_current = 2 * alpha * \\ np . copy ( targets [ distance_id ]) - beta * \\ calculate_amplitude ( reconstruction ) target_current [ target_current == 0 ] = gamma * \\ np . abs ( reconstruction [ target_current == 0 ]) elif target_type == 'no constraint' : target_current = np . abs ( targets [ distance_id ]) new_target = calculate_amplitude ( reconstruction ) new_target [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] = target_current reconstruction = generate_complex_field ( new_target , calculate_phase ( reconstruction )) hologram_layer = propagate_beam ( reconstruction , k , - distance , dx , wavelength , propagation_type ) hologram_layer = generate_complex_field ( 1. , calculate_phase ( hologram_layer )) hologram_layer = hologram_layer [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] hologram_layer = zero_pad ( hologram_layer ) holograms [ distance_id ] = hologram_layer hologram = np . sum ( holograms , axis = 0 ) hologram = hologram [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] return hologram See also \u00b6 Computer Generated-Holography","title":"odak.wave.gerchberg_saxton_3d"},{"location":"odak/wave/gerchberg_saxton_3d/#odakwavegerchberg_saxton_3d","text":"Definition to compute a multi plane hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Zhou, Pengcheng, et al. \"30.4: Multi\u2010plane holographic display with a uniform 3D Gerchberg\u2010Saxton algorithm.\" SID Symposium Digest of Technical Papers. Vol. 46. No. 1. 2015. Parameters: Name Type Description Default fields np.complex64 Complex fields (MxN). required distances list Propagation distances. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required slm_range float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. 6.28 propagation_type str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). 'IR Fresnel' initial_phase np.complex64 Phase to be added to the initial value. None target_type str Target type. No constraint targets the input target as is. Double constraint follows the idea in this paper, which claims to suppress speckle: Chang, Chenliang, et al. \"Speckle-suppressed phase-only holographic three-dimensional display based on double-constraint Gerchberg\u2013Saxton algorithm.\" Applied optics 54.23 (2015): 6994-7001. 'no constraint' Returns: Type Description np.complex Calculated complex hologram. Source code in odak/wave/classical.py def gerchberg_saxton_3d ( fields , n_iterations , distances , dx , wavelength , slm_range = 6.28 , propagation_type = 'IR Fresnel' , initial_phase = None , target_type = 'no constraint' , coefficients = None ): \"\"\" Definition to compute a multi plane hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Zhou, Pengcheng, et al. \"30.4: Multi\u2010plane holographic display with a uniform 3D Gerchberg\u2010Saxton algorithm.\" SID Symposium Digest of Technical Papers. Vol. 46. No. 1. 2015. Parameters ---------- fields : np.complex64 Complex fields (MxN). distances : list Propagation distances. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. slm_range : float Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more. propagation_type : str Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer). initial_phase : np.complex64 Phase to be added to the initial value. target_type : str Target type. `No constraint` targets the input target as is. `Double constraint` follows the idea in this paper, which claims to suppress speckle: Chang, Chenliang, et al. \"Speckle-suppressed phase-only holographic three-dimensional display based on double-constraint Gerchberg\u2013Saxton algorithm.\" Applied optics 54.23 (2015): 6994-7001. Returns ------- hologram : np.complex Calculated complex hologram. \"\"\" k = wavenumber ( wavelength ) targets = calculate_amplitude ( np . asarray ( fields )) . astype ( np . float ) hologram = generate_complex_field ( np . ones ( targets [ 0 ] . shape ), 0 ) hologram = zero_pad ( hologram ) if type ( initial_phase ) == type ( None ): hologram = add_random_phase ( hologram ) else : initial_phase = zero_pad ( initial_phase ) hologram = add_phase ( hologram , initial_phase ) center = [ int ( hologram . shape [ 0 ] / 2. ), int ( hologram . shape [ 1 ] / 2. )] orig_shape = [ int ( fields [ 0 ] . shape [ 0 ] / 2. ), int ( fields [ 0 ] . shape [ 1 ] / 2. )] holograms = np . zeros ( ( len ( distances ), hologram . shape [ 0 ], hologram . shape [ 1 ]), dtype = np . complex64 ) for i in tqdm ( range ( n_iterations ), leave = False ): for distance_id in tqdm ( range ( len ( distances )), leave = False ): distance = distances [ distance_id ] reconstruction = propagate_beam ( hologram , k , distance , dx , wavelength , propagation_type ) if target_type == 'double constraint' : if type ( coefficients ) == type ( None ): raise Exception ( \"Provide coeeficients of alpha,beta and gamma for double constraint.\" ) alpha = coefficients [ 0 ] beta = coefficients [ 1 ] gamma = coefficients [ 2 ] target_current = 2 * alpha * \\ np . copy ( targets [ distance_id ]) - beta * \\ calculate_amplitude ( reconstruction ) target_current [ target_current == 0 ] = gamma * \\ np . abs ( reconstruction [ target_current == 0 ]) elif target_type == 'no constraint' : target_current = np . abs ( targets [ distance_id ]) new_target = calculate_amplitude ( reconstruction ) new_target [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] = target_current reconstruction = generate_complex_field ( new_target , calculate_phase ( reconstruction )) hologram_layer = propagate_beam ( reconstruction , k , - distance , dx , wavelength , propagation_type ) hologram_layer = generate_complex_field ( 1. , calculate_phase ( hologram_layer )) hologram_layer = hologram_layer [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] hologram_layer = zero_pad ( hologram_layer ) holograms [ distance_id ] = hologram_layer hologram = np . sum ( holograms , axis = 0 ) hologram = hologram [ center [ 0 ] - orig_shape [ 0 ]: center [ 0 ] + orig_shape [ 0 ], center [ 1 ] - orig_shape [ 1 ]: center [ 1 ] + orig_shape [ 1 ] ] return hologram","title":"odak.wave.gerchberg_saxton_3d"},{"location":"odak/wave/gerchberg_saxton_3d/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/impulse_response_fresnel/","text":"odak.wave.impulse_response_fresnel \u00b6 A definition to calculate impulse response based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def impulse_response_fresnel ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate impulse response based Fresnel approximation for beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape x = np . linspace ( - nu / 2 * dx , nu / 2 * dx , nu ) y = np . linspace ( - nv / 2 * dx , nv / 2 * dx , nv ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 h = np . exp ( 1 j * k * distance ) / ( 1 j * wavelength * distance ) * \\ np . exp ( 1 j * k / 2 / distance * Z ) h = np . fft . fft2 ( np . fft . fftshift ( h )) * dx ** 2 U1 = np . fft . fft2 ( np . fft . fftshift ( field )) U2 = h * U1 result = np . fft . ifftshift ( np . fft . ifft2 ( U2 )) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.impulse_response_fresnel"},{"location":"odak/wave/impulse_response_fresnel/#odakwaveimpulse_response_fresnel","text":"A definition to calculate impulse response based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def impulse_response_fresnel ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate impulse response based Fresnel approximation for beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape x = np . linspace ( - nu / 2 * dx , nu / 2 * dx , nu ) y = np . linspace ( - nv / 2 * dx , nv / 2 * dx , nv ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 h = np . exp ( 1 j * k * distance ) / ( 1 j * wavelength * distance ) * \\ np . exp ( 1 j * k / 2 / distance * Z ) h = np . fft . fft2 ( np . fft . fftshift ( h )) * dx ** 2 U1 = np . fft . fft2 ( np . fft . fftshift ( field )) U2 = h * U1 result = np . fft . ifftshift ( np . fft . ifft2 ( U2 )) return result","title":"odak.wave.impulse_response_fresnel"},{"location":"odak/wave/impulse_response_fresnel/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/linear_grating/","text":"odak.wave.linear_grating \u00b6 A definition to generate a linear grating. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required every int Add the add value at every given number. 2 add float Angle to be added. 3.14 axis string Axis eiter X,Y or both. 'x' Returns: Type Description ndarray Linear grating term. Source code in odak/wave/lens.py def linear_grating ( nx , ny , every = 2 , add = 3.14 , axis = 'x' ): \"\"\" A definition to generate a linear grating. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. every : int Add the add value at every given number. add : float Angle to be added. axis : string Axis eiter X,Y or both. Returns ------- field : ndarray Linear grating term. \"\"\" grating = np . zeros (( nx , ny ), dtype = np . complex64 ) if axis == 'x' : grating [:: every , :] = np . exp ( 1 j * add ) if axis == 'y' : grating [:, :: every ] = np . exp ( 1 j * add ) if axis == 'xy' : checker = np . indices (( nx , ny )) . sum ( axis = 0 ) % every checker += 1 checker = checker % 2 grating = np . exp ( 1 j * checker * add ) return grating See also \u00b6 Computer Generated-Holography","title":"odak.wave.linear_grating"},{"location":"odak/wave/linear_grating/#odakwavelinear_grating","text":"A definition to generate a linear grating. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required every int Add the add value at every given number. 2 add float Angle to be added. 3.14 axis string Axis eiter X,Y or both. 'x' Returns: Type Description ndarray Linear grating term. Source code in odak/wave/lens.py def linear_grating ( nx , ny , every = 2 , add = 3.14 , axis = 'x' ): \"\"\" A definition to generate a linear grating. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. every : int Add the add value at every given number. add : float Angle to be added. axis : string Axis eiter X,Y or both. Returns ------- field : ndarray Linear grating term. \"\"\" grating = np . zeros (( nx , ny ), dtype = np . complex64 ) if axis == 'x' : grating [:: every , :] = np . exp ( 1 j * add ) if axis == 'y' : grating [:, :: every ] = np . exp ( 1 j * add ) if axis == 'xy' : checker = np . indices (( nx , ny )) . sum ( axis = 0 ) % every checker += 1 checker = checker % 2 grating = np . exp ( 1 j * checker * add ) return grating","title":"odak.wave.linear_grating"},{"location":"odak/wave/linear_grating/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/prism_phase_function/","text":"odak.wave.prism_phase_function \u00b6 A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book for more. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required k odak.wave.wavenumber See odak.wave.wavenumber for more. required angle float Tilt angle of the prism in degrees. required dx float Pixel pitch. 0.001 axis str Axis of the prism. 'x' Returns: Type Description ndarray Generated phase function for a prism. Source code in odak/wave/lens.py def prism_phase_function ( nx , ny , k , angle , dx = 0.001 , axis = 'x' ): \"\"\" A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book for more. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. k : odak.wave.wavenumber See odak.wave.wavenumber for more. angle : float Tilt angle of the prism in degrees. dx : float Pixel pitch. axis : str Axis of the prism. Returns ------- prism : ndarray Generated phase function for a prism. \"\"\" angle = np . radians ( angle ) size = [ ny , nx ] x = np . linspace ( - size [ 0 ] * dx / 2 , size [ 0 ] * dx / 2 , size [ 0 ]) y = np . linspace ( - size [ 1 ] * dx / 2 , size [ 1 ] * dx / 2 , size [ 1 ]) X , Y = np . meshgrid ( x , y ) if axis == 'y' : prism = np . exp ( - 1 j * k * np . sin ( angle ) * Y ) elif axis == 'x' : prism = np . exp ( - 1 j * k * np . sin ( angle ) * X ) return prism See also \u00b6 Computer Generated-Holography","title":"odak.wave.prism_phase_function"},{"location":"odak/wave/prism_phase_function/#odakwaveprism_phase_function","text":"A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book for more. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required k odak.wave.wavenumber See odak.wave.wavenumber for more. required angle float Tilt angle of the prism in degrees. required dx float Pixel pitch. 0.001 axis str Axis of the prism. 'x' Returns: Type Description ndarray Generated phase function for a prism. Source code in odak/wave/lens.py def prism_phase_function ( nx , ny , k , angle , dx = 0.001 , axis = 'x' ): \"\"\" A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book for more. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. k : odak.wave.wavenumber See odak.wave.wavenumber for more. angle : float Tilt angle of the prism in degrees. dx : float Pixel pitch. axis : str Axis of the prism. Returns ------- prism : ndarray Generated phase function for a prism. \"\"\" angle = np . radians ( angle ) size = [ ny , nx ] x = np . linspace ( - size [ 0 ] * dx / 2 , size [ 0 ] * dx / 2 , size [ 0 ]) y = np . linspace ( - size [ 1 ] * dx / 2 , size [ 1 ] * dx / 2 , size [ 1 ]) X , Y = np . meshgrid ( x , y ) if axis == 'y' : prism = np . exp ( - 1 j * k * np . sin ( angle ) * Y ) elif axis == 'x' : prism = np . exp ( - 1 j * k * np . sin ( angle ) * X ) return prism","title":"odak.wave.prism_phase_function"},{"location":"odak/wave/prism_phase_function/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/produce_phase_only_slm_pattern/","text":"odak.wave.produce_phase_only_slm_pattern \u00b6 Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field. Parameters: Name Type Description Default hologram np.complex64 Input holographic field. required slm_range float Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more. required filename str Optional variable, if provided the patterns will be save to given location. None bits int Quantization bits. 8 default_range float Default range of phase only SLM. 6.28 illumination np.ndarray Spatial illumination distribution. None Returns: Type Description np.complex64 Adjusted phase only pattern. Source code in odak/wave/__init__.py def produce_phase_only_slm_pattern ( hologram , slm_range , filename = None , bits = 8 , default_range = 6.28 , illumination = None ): \"\"\" Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field. Parameters ---------- hologram : np.complex64 Input holographic field. slm_range : float Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more. filename : str Optional variable, if provided the patterns will be save to given location. bits : int Quantization bits. default_range : float Default range of phase only SLM. illumination : np.ndarray Spatial illumination distribution. Returns ------- pattern : np.complex64 Adjusted phase only pattern. hologram_digital : np.int Digital representation of the hologram. \"\"\" #hologram_phase = calculate_phase(hologram) % default_range hologram_phase = calculate_phase ( hologram ) hologram_phase = hologram_phase % slm_range hologram_phase /= slm_range hologram_phase *= 2 ** bits hologram_phase = hologram_phase . astype ( np . int ) hologram_digital = np . copy ( hologram_phase ) if type ( filename ) != type ( None ): save_image ( filename , hologram_phase , cmin = 0 , cmax = 2 ** bits ) hologram_phase = hologram_phase . astype ( np . float ) hologram_phase *= slm_range / 2 ** bits if type ( illumination ) == type ( None ): A = 1. else : A = illumination return A * np . cos ( hologram_phase ) + A * 1 j * np . sin ( hologram_phase ), hologram_digital See also \u00b6 Computer Generated-Holography","title":"odak.wave.produce_phase_only_slm_pattern"},{"location":"odak/wave/produce_phase_only_slm_pattern/#odakwaveproduce_phase_only_slm_pattern","text":"Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field. Parameters: Name Type Description Default hologram np.complex64 Input holographic field. required slm_range float Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more. required filename str Optional variable, if provided the patterns will be save to given location. None bits int Quantization bits. 8 default_range float Default range of phase only SLM. 6.28 illumination np.ndarray Spatial illumination distribution. None Returns: Type Description np.complex64 Adjusted phase only pattern. Source code in odak/wave/__init__.py def produce_phase_only_slm_pattern ( hologram , slm_range , filename = None , bits = 8 , default_range = 6.28 , illumination = None ): \"\"\" Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field. Parameters ---------- hologram : np.complex64 Input holographic field. slm_range : float Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more. filename : str Optional variable, if provided the patterns will be save to given location. bits : int Quantization bits. default_range : float Default range of phase only SLM. illumination : np.ndarray Spatial illumination distribution. Returns ------- pattern : np.complex64 Adjusted phase only pattern. hologram_digital : np.int Digital representation of the hologram. \"\"\" #hologram_phase = calculate_phase(hologram) % default_range hologram_phase = calculate_phase ( hologram ) hologram_phase = hologram_phase % slm_range hologram_phase /= slm_range hologram_phase *= 2 ** bits hologram_phase = hologram_phase . astype ( np . int ) hologram_digital = np . copy ( hologram_phase ) if type ( filename ) != type ( None ): save_image ( filename , hologram_phase , cmin = 0 , cmax = 2 ** bits ) hologram_phase = hologram_phase . astype ( np . float ) hologram_phase *= slm_range / 2 ** bits if type ( illumination ) == type ( None ): A = 1. else : A = illumination return A * np . cos ( hologram_phase ) + A * 1 j * np . sin ( hologram_phase ), hologram_digital","title":"odak.wave.produce_phase_only_slm_pattern"},{"location":"odak/wave/produce_phase_only_slm_pattern/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/propagate_beam/","text":"odak.wave.propagate_beam \u00b6 Definitions for Fresnel Impulse Response (IR), Angular Spectrum (AS), Bandlimited Angular Spectrum (BAS), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. For more on Bandlimited Fresnel impulse response also known as Bandlimited Angular Spectrum method see \"Band-limited Angular Spectrum Method for Numerical Simulation of Free-Space Propagation in Far and Near Fields\". Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required propagation_type str Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer). 'IR Fresnel' Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def propagate_beam ( field , k , distance , dx , wavelength , propagation_type = 'IR Fresnel' ): \"\"\" Definitions for Fresnel Impulse Response (IR), Angular Spectrum (AS), Bandlimited Angular Spectrum (BAS), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. For more on Bandlimited Fresnel impulse response also known as Bandlimited Angular Spectrum method see \"Band-limited Angular Spectrum Method for Numerical Simulation of Free-Space Propagation in Far and Near Fields\". Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. propagation_type : str Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer). Returns ------- result : np.complex Final complex field (MxN). \"\"\" if propagation_type == 'Rayleigh-Sommerfeld' : result = rayleigh_sommerfeld ( field , k , distance , dx , wavelength ) elif propagation_type == 'Angular Spectrum' : result = angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'IR Fresnel' : result = impulse_response_fresnel ( field , k , distance , dx , wavelength ) elif propagation_type == 'Bandlimited Angular Spectrum' : result = band_limited_angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'Bandextended Angular Spectrum' : result = band_extended_angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'Adaptive Sampling Angular Spectrum' : result = adaptive_sampling_angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'TR Fresnel' : result = transfer_function_fresnel ( field , k , distance , dx , wavelength ) elif propagation_type == 'Fraunhofer' : result = fraunhofer ( field , k , distance , dx , wavelength ) elif propagation_type == 'Fraunhofer Inverse' : result = fraunhofer_inverse ( field , k , distance , dx , wavelength ) else : raise Exception ( \"Unknown propagation type selected.\" ) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.propagate_beam"},{"location":"odak/wave/propagate_beam/#odakwavepropagate_beam","text":"Definitions for Fresnel Impulse Response (IR), Angular Spectrum (AS), Bandlimited Angular Spectrum (BAS), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. For more on Bandlimited Fresnel impulse response also known as Bandlimited Angular Spectrum method see \"Band-limited Angular Spectrum Method for Numerical Simulation of Free-Space Propagation in Far and Near Fields\". Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required propagation_type str Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer). 'IR Fresnel' Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def propagate_beam ( field , k , distance , dx , wavelength , propagation_type = 'IR Fresnel' ): \"\"\" Definitions for Fresnel Impulse Response (IR), Angular Spectrum (AS), Bandlimited Angular Spectrum (BAS), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. For more on Bandlimited Fresnel impulse response also known as Bandlimited Angular Spectrum method see \"Band-limited Angular Spectrum Method for Numerical Simulation of Free-Space Propagation in Far and Near Fields\". Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. propagation_type : str Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer). Returns ------- result : np.complex Final complex field (MxN). \"\"\" if propagation_type == 'Rayleigh-Sommerfeld' : result = rayleigh_sommerfeld ( field , k , distance , dx , wavelength ) elif propagation_type == 'Angular Spectrum' : result = angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'IR Fresnel' : result = impulse_response_fresnel ( field , k , distance , dx , wavelength ) elif propagation_type == 'Bandlimited Angular Spectrum' : result = band_limited_angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'Bandextended Angular Spectrum' : result = band_extended_angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'Adaptive Sampling Angular Spectrum' : result = adaptive_sampling_angular_spectrum ( field , k , distance , dx , wavelength ) elif propagation_type == 'TR Fresnel' : result = transfer_function_fresnel ( field , k , distance , dx , wavelength ) elif propagation_type == 'Fraunhofer' : result = fraunhofer ( field , k , distance , dx , wavelength ) elif propagation_type == 'Fraunhofer Inverse' : result = fraunhofer_inverse ( field , k , distance , dx , wavelength ) else : raise Exception ( \"Unknown propagation type selected.\" ) return result","title":"odak.wave.propagate_beam"},{"location":"odak/wave/propagate_beam/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/propagate_field/","text":"odak.wave.propagate_field \u00b6 Definition to propagate a field from points to an another points in space: propagate a given array of spherical sources to given set of points in space. Parameters: Name Type Description Default points0 ndarray Start points (i.e. odak.tools.grid_sample). required points1 ndarray End points (ie. odak.tools.grid_sample). required field0 ndarray Field for given starting points. required wave_number float Wave number of a wave, see odak.wave.wavenumber for more. required direction float For propagating in forward direction set as 1, otherwise -1. 1 Returns: Type Description ndarray Field for given end points. Source code in odak/wave/vector.py def propagate_field ( points0 , points1 , field0 , wave_number , direction = 1 ): \"\"\" Definition to propagate a field from points to an another points in space: propagate a given array of spherical sources to given set of points in space. Parameters ---------- points0 : ndarray Start points (i.e. odak.tools.grid_sample). points1 : ndarray End points (ie. odak.tools.grid_sample). field0 : ndarray Field for given starting points. wave_number : float Wave number of a wave, see odak.wave.wavenumber for more. direction : float For propagating in forward direction set as 1, otherwise -1. Returns ------- field1 : ndarray Field for given end points. \"\"\" field1 = np . zeros ( points1 . shape [ 0 ], dtype = np . complex64 ) for point_id in range ( points0 . shape [ 0 ]): point = points0 [ point_id ] distances = distance_between_two_points ( point , points1 ) field1 += electric_field_per_plane_wave ( calculate_amplitude ( field0 [ point_id ]), distances * direction , wave_number , phase = calculate_phase ( field0 [ point_id ]) ) return field1 See also \u00b6 Computer Generated-Holography","title":"odak.wave.propagate_field"},{"location":"odak/wave/propagate_field/#odakwavepropagate_field","text":"Definition to propagate a field from points to an another points in space: propagate a given array of spherical sources to given set of points in space. Parameters: Name Type Description Default points0 ndarray Start points (i.e. odak.tools.grid_sample). required points1 ndarray End points (ie. odak.tools.grid_sample). required field0 ndarray Field for given starting points. required wave_number float Wave number of a wave, see odak.wave.wavenumber for more. required direction float For propagating in forward direction set as 1, otherwise -1. 1 Returns: Type Description ndarray Field for given end points. Source code in odak/wave/vector.py def propagate_field ( points0 , points1 , field0 , wave_number , direction = 1 ): \"\"\" Definition to propagate a field from points to an another points in space: propagate a given array of spherical sources to given set of points in space. Parameters ---------- points0 : ndarray Start points (i.e. odak.tools.grid_sample). points1 : ndarray End points (ie. odak.tools.grid_sample). field0 : ndarray Field for given starting points. wave_number : float Wave number of a wave, see odak.wave.wavenumber for more. direction : float For propagating in forward direction set as 1, otherwise -1. Returns ------- field1 : ndarray Field for given end points. \"\"\" field1 = np . zeros ( points1 . shape [ 0 ], dtype = np . complex64 ) for point_id in range ( points0 . shape [ 0 ]): point = points0 [ point_id ] distances = distance_between_two_points ( point , points1 ) field1 += electric_field_per_plane_wave ( calculate_amplitude ( field0 [ point_id ]), distances * direction , wave_number , phase = calculate_phase ( field0 [ point_id ]) ) return field1","title":"odak.wave.propagate_field"},{"location":"odak/wave/propagate_field/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/propagate_plane_waves/","text":"odak.wave.propagate_plane_waves \u00b6 Definition to propagate a field representing a plane wave at a particular distance and time. Parameters: Name Type Description Default field complex Complex field. required opd float Optical path difference in mm. required k float Wave number of a wave, see odak.wave.parameters.wavenumber for more. required w float Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more. 0 t float Time in seconds. 0 Returns: Type Description complex A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)). Source code in odak/wave/vector.py def propagate_plane_waves ( field , opd , k , w = 0 , t = 0 ): \"\"\" Definition to propagate a field representing a plane wave at a particular distance and time. Parameters ---------- field : complex Complex field. opd : float Optical path difference in mm. k : float Wave number of a wave, see odak.wave.parameters.wavenumber for more. w : float Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more. t : float Time in seconds. Returns ------- new_field : complex A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)). \"\"\" new_field = field * np . exp ( 1 j * ( - w * t + opd * k )) / opd ** 2 return new_field See also \u00b6 Computer Generated-Holography","title":"odak.wave.propagate_plane_waves"},{"location":"odak/wave/propagate_plane_waves/#odakwavepropagate_plane_waves","text":"Definition to propagate a field representing a plane wave at a particular distance and time. Parameters: Name Type Description Default field complex Complex field. required opd float Optical path difference in mm. required k float Wave number of a wave, see odak.wave.parameters.wavenumber for more. required w float Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more. 0 t float Time in seconds. 0 Returns: Type Description complex A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)). Source code in odak/wave/vector.py def propagate_plane_waves ( field , opd , k , w = 0 , t = 0 ): \"\"\" Definition to propagate a field representing a plane wave at a particular distance and time. Parameters ---------- field : complex Complex field. opd : float Optical path difference in mm. k : float Wave number of a wave, see odak.wave.parameters.wavenumber for more. w : float Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more. t : float Time in seconds. Returns ------- new_field : complex A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)). \"\"\" new_field = field * np . exp ( 1 j * ( - w * t + opd * k )) / opd ** 2 return new_field","title":"odak.wave.propagate_plane_waves"},{"location":"odak/wave/propagate_plane_waves/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/quadratic_phase_function/","text":"odak.wave.quadratic_phase_function \u00b6 A definition to generate 2D quadratic phase function, which is typically use to represent lenses. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required k odak.wave.wavenumber See odak.wave.wavenumber for more. required focal float Focal length of the quadratic phase function. 0.4 dx float Pixel pitch. 0.001 offset list Deviation from the center along X and Y axes. [0, 0] Returns: Type Description ndarray Generated quadratic phase function. Source code in odak/wave/lens.py def quadratic_phase_function ( nx , ny , k , focal = 0.4 , dx = 0.001 , offset = [ 0 , 0 ]): \"\"\" A definition to generate 2D quadratic phase function, which is typically use to represent lenses. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. k : odak.wave.wavenumber See odak.wave.wavenumber for more. focal : float Focal length of the quadratic phase function. dx : float Pixel pitch. offset : list Deviation from the center along X and Y axes. Returns ------- function : ndarray Generated quadratic phase function. \"\"\" size = [ nx , ny ] x = np . linspace ( - size [ 0 ] * dx / 2 , size [ 0 ] * dx / 2 , size [ 0 ]) - offset [ 1 ] * dx y = np . linspace ( - size [ 1 ] * dx / 2 , size [ 1 ] * dx / 2 , size [ 1 ]) - offset [ 0 ] * dx X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 qwf = np . exp ( 1 j * k * 0.5 * np . sin ( Z / focal )) return qwf See also \u00b6 Computer Generated-Holography","title":"odak.wave.quadratic_phase_function"},{"location":"odak/wave/quadratic_phase_function/#odakwavequadratic_phase_function","text":"A definition to generate 2D quadratic phase function, which is typically use to represent lenses. Parameters: Name Type Description Default nx int Size of the output along X. required ny int Size of the output along Y. required k odak.wave.wavenumber See odak.wave.wavenumber for more. required focal float Focal length of the quadratic phase function. 0.4 dx float Pixel pitch. 0.001 offset list Deviation from the center along X and Y axes. [0, 0] Returns: Type Description ndarray Generated quadratic phase function. Source code in odak/wave/lens.py def quadratic_phase_function ( nx , ny , k , focal = 0.4 , dx = 0.001 , offset = [ 0 , 0 ]): \"\"\" A definition to generate 2D quadratic phase function, which is typically use to represent lenses. Parameters ---------- nx : int Size of the output along X. ny : int Size of the output along Y. k : odak.wave.wavenumber See odak.wave.wavenumber for more. focal : float Focal length of the quadratic phase function. dx : float Pixel pitch. offset : list Deviation from the center along X and Y axes. Returns ------- function : ndarray Generated quadratic phase function. \"\"\" size = [ nx , ny ] x = np . linspace ( - size [ 0 ] * dx / 2 , size [ 0 ] * dx / 2 , size [ 0 ]) - offset [ 1 ] * dx y = np . linspace ( - size [ 1 ] * dx / 2 , size [ 1 ] * dx / 2 , size [ 1 ]) - offset [ 0 ] * dx X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 qwf = np . exp ( 1 j * k * 0.5 * np . sin ( Z / focal )) return qwf","title":"odak.wave.quadratic_phase_function"},{"location":"odak/wave/quadratic_phase_function/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/rayleigh_resolution/","text":"odak.wave.rayleigh_resolution \u00b6 Definition to calculate rayleigh resolution limit of a lens with a certain focal length and an aperture. Lens is assumed to be focusing a plane wave at a focal distance. Parameter \u00b6 diameter : float Diameter of a lens. focal : float Focal length of a lens, when focal length is provided, spatial resolution is provided at the focal plane. When focal length isn't provided angular resolution is provided. wavelength : float Wavelength of light. Returns \u00b6 resolution : float Resolvable angular or spatial spot size, see focal in parameters to know what to expect. Source code in odak/wave/__init__.py def rayleigh_resolution ( diameter , focal = None , wavelength = 0.0005 ): \"\"\" Definition to calculate rayleigh resolution limit of a lens with a certain focal length and an aperture. Lens is assumed to be focusing a plane wave at a focal distance. Parameter --------- diameter : float Diameter of a lens. focal : float Focal length of a lens, when focal length is provided, spatial resolution is provided at the focal plane. When focal length isn't provided angular resolution is provided. wavelength : float Wavelength of light. Returns -------- resolution : float Resolvable angular or spatial spot size, see focal in parameters to know what to expect. \"\"\" resolution = 1.22 * wavelength / diameter if type ( focal ) != type ( None ): resolution *= focal return resolution See also \u00b6 Computer Generated-Holography","title":"odak.wave.rayleigh_resolution"},{"location":"odak/wave/rayleigh_resolution/#odakwaverayleigh_resolution","text":"Definition to calculate rayleigh resolution limit of a lens with a certain focal length and an aperture. Lens is assumed to be focusing a plane wave at a focal distance.","title":"odak.wave.rayleigh_resolution"},{"location":"odak/wave/rayleigh_resolution/#odak.wave.rayleigh_resolution--parameter","text":"diameter : float Diameter of a lens. focal : float Focal length of a lens, when focal length is provided, spatial resolution is provided at the focal plane. When focal length isn't provided angular resolution is provided. wavelength : float Wavelength of light.","title":"Parameter"},{"location":"odak/wave/rayleigh_resolution/#odak.wave.rayleigh_resolution--returns","text":"resolution : float Resolvable angular or spatial spot size, see focal in parameters to know what to expect. Source code in odak/wave/__init__.py def rayleigh_resolution ( diameter , focal = None , wavelength = 0.0005 ): \"\"\" Definition to calculate rayleigh resolution limit of a lens with a certain focal length and an aperture. Lens is assumed to be focusing a plane wave at a focal distance. Parameter --------- diameter : float Diameter of a lens. focal : float Focal length of a lens, when focal length is provided, spatial resolution is provided at the focal plane. When focal length isn't provided angular resolution is provided. wavelength : float Wavelength of light. Returns -------- resolution : float Resolvable angular or spatial spot size, see focal in parameters to know what to expect. \"\"\" resolution = 1.22 * wavelength / diameter if type ( focal ) != type ( None ): resolution *= focal return resolution","title":"Returns"},{"location":"odak/wave/rayleigh_resolution/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/rayleigh_sommerfeld/","text":"odak.wave.rayleigh_sommerfeld \u00b6 Definition to compute beam propagation using Rayleigh-Sommerfeld's diffraction formula (Huygens-Fresnel Principle). For more see Section 3.5.2 in Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company Publishers, 2005. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def rayleigh_sommerfeld ( field , k , distance , dx , wavelength ): \"\"\" Definition to compute beam propagation using Rayleigh-Sommerfeld's diffraction formula (Huygens-Fresnel Principle). For more see Section 3.5.2 in Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company Publishers, 2005. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape x = np . linspace ( - nv * dx / 2 , nv * dx / 2 , nv ) y = np . linspace ( - nu * dx / 2 , nu * dx / 2 , nu ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 result = np . zeros ( field . shape , dtype = np . complex64 ) direction = int ( distance / np . abs ( distance )) for i in range ( nu ): for j in range ( nv ): if field [ i , j ] != 0 : r01 = np . sqrt ( distance ** 2 + ( X - X [ i , j ]) ** 2 + ( Y - Y [ i , j ]) ** 2 ) * direction cosnr01 = np . cos ( distance / r01 ) result += field [ i , j ] * np . exp ( 1 j * k * r01 ) / r01 * cosnr01 result *= 1. / ( 1 j * wavelength ) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.rayleigh_sommerfeld"},{"location":"odak/wave/rayleigh_sommerfeld/#odakwaverayleigh_sommerfeld","text":"Definition to compute beam propagation using Rayleigh-Sommerfeld's diffraction formula (Huygens-Fresnel Principle). For more see Section 3.5.2 in Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company Publishers, 2005. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def rayleigh_sommerfeld ( field , k , distance , dx , wavelength ): \"\"\" Definition to compute beam propagation using Rayleigh-Sommerfeld's diffraction formula (Huygens-Fresnel Principle). For more see Section 3.5.2 in Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company Publishers, 2005. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape x = np . linspace ( - nv * dx / 2 , nv * dx / 2 , nv ) y = np . linspace ( - nu * dx / 2 , nu * dx / 2 , nu ) X , Y = np . meshgrid ( x , y ) Z = X ** 2 + Y ** 2 result = np . zeros ( field . shape , dtype = np . complex64 ) direction = int ( distance / np . abs ( distance )) for i in range ( nu ): for j in range ( nv ): if field [ i , j ] != 0 : r01 = np . sqrt ( distance ** 2 + ( X - X [ i , j ]) ** 2 + ( Y - Y [ i , j ]) ** 2 ) * direction cosnr01 = np . cos ( distance / r01 ) result += field [ i , j ] * np . exp ( 1 j * k * r01 ) / r01 * cosnr01 result *= 1. / ( 1 j * wavelength ) return result","title":"odak.wave.rayleigh_sommerfeld"},{"location":"odak/wave/rayleigh_sommerfeld/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/rotationspeed/","text":"odak.wave.rotationspeed \u00b6 Definition for calculating rotation speed of a wave (w in A*e^(j(wt+phi))). Parameters: Name Type Description Default wavelength float Wavelength of a wave in mm. required c float Speed of wave in mm/seconds. Default is the speed of light in the void! 300000000000 Source code in odak/wave/__init__.py def rotationspeed ( wavelength , c = 3 * 10 ** 11 ): \"\"\" Definition for calculating rotation speed of a wave (w in A*e^(j(wt+phi))). Parameters ---------- wavelength : float Wavelength of a wave in mm. c : float Speed of wave in mm/seconds. Default is the speed of light in the void! Returns ------- w : float \"\"\" f = c * wavelength w = 2 * np . pi * f return w See also \u00b6 Computer Generated-Holography","title":"odak.wave.rotationspeed"},{"location":"odak/wave/rotationspeed/#odakwaverotationspeed","text":"Definition for calculating rotation speed of a wave (w in A*e^(j(wt+phi))). Parameters: Name Type Description Default wavelength float Wavelength of a wave in mm. required c float Speed of wave in mm/seconds. Default is the speed of light in the void! 300000000000 Source code in odak/wave/__init__.py def rotationspeed ( wavelength , c = 3 * 10 ** 11 ): \"\"\" Definition for calculating rotation speed of a wave (w in A*e^(j(wt+phi))). Parameters ---------- wavelength : float Wavelength of a wave in mm. c : float Speed of wave in mm/seconds. Default is the speed of light in the void! Returns ------- w : float \"\"\" f = c * wavelength w = 2 * np . pi * f return w","title":"odak.wave.rotationspeed"},{"location":"odak/wave/rotationspeed/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/set_amplitude/","text":"odak.wave.set_amplitude \u00b6 Definition to keep phase as is and change the amplitude of a given field. Parameters: Name Type Description Default field np.complex64 Complex field. required amplitude np.array or np.complex64 Amplitudes. required Returns: Type Description np.complex64 Complex field. Source code in odak/wave/__init__.py def set_amplitude ( field , amplitude ): \"\"\" Definition to keep phase as is and change the amplitude of a given field. Parameters ---------- field : np.complex64 Complex field. amplitude : np.array or np.complex64 Amplitudes. Returns ------- new_field : np.complex64 Complex field. \"\"\" amplitude = calculate_amplitude ( amplitude ) phase = calculate_phase ( field ) new_field = amplitude * np . cos ( phase ) + 1 j * amplitude * np . sin ( phase ) return new_field See also \u00b6 Computer Generated-Holography","title":"odak.wave.set_amplitude"},{"location":"odak/wave/set_amplitude/#odakwaveset_amplitude","text":"Definition to keep phase as is and change the amplitude of a given field. Parameters: Name Type Description Default field np.complex64 Complex field. required amplitude np.array or np.complex64 Amplitudes. required Returns: Type Description np.complex64 Complex field. Source code in odak/wave/__init__.py def set_amplitude ( field , amplitude ): \"\"\" Definition to keep phase as is and change the amplitude of a given field. Parameters ---------- field : np.complex64 Complex field. amplitude : np.array or np.complex64 Amplitudes. Returns ------- new_field : np.complex64 Complex field. \"\"\" amplitude = calculate_amplitude ( amplitude ) phase = calculate_phase ( field ) new_field = amplitude * np . cos ( phase ) + 1 j * amplitude * np . sin ( phase ) return new_field","title":"odak.wave.set_amplitude"},{"location":"odak/wave/set_amplitude/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/transfer_function_kernel/","text":"odak.wave.transfer_function_fresnel \u00b6 A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def transfer_function_fresnel ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape fx = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nu ) fy = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nv ) FX , FY = np . meshgrid ( fx , fy ) H = np . exp ( 1 j * k * distance * ( 1 - ( FX * wavelength ) ** 2 - ( FY * wavelength ) ** 2 ) ** 0.5 ) U1 = np . fft . fftshift ( np . fft . fft2 ( np . fft . fftshift ( field ))) U2 = H * U1 result = np . fft . ifftshift ( np . fft . ifft2 ( np . fft . ifftshift ( U2 ))) return result See also \u00b6 Computer Generated-Holography","title":"odak.wave.transfer_function_fresnel"},{"location":"odak/wave/transfer_function_kernel/#odakwavetransfer_function_fresnel","text":"A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters: Name Type Description Default field np.complex Complex field (MxN). required k odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. required distance float Propagation distance. required dx float Size of one single pixel in the field grid (in meters). required wavelength float Wavelength of the electric field. required Returns: Type Description np.complex Final complex field (MxN). Source code in odak/wave/classical.py def transfer_function_fresnel ( field , k , distance , dx , wavelength ): \"\"\" A definition to calculate convolution based Fresnel approximation for beam propagation. Parameters ---------- field : np.complex Complex field (MxN). k : odak.wave.wavenumber Wave number of a wave, see odak.wave.wavenumber for more. distance : float Propagation distance. dx : float Size of one single pixel in the field grid (in meters). wavelength : float Wavelength of the electric field. Returns ------- result : np.complex Final complex field (MxN). \"\"\" nv , nu = field . shape fx = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nu ) fy = np . linspace ( - 1. / 2. / dx , 1. / 2. / dx , nv ) FX , FY = np . meshgrid ( fx , fy ) H = np . exp ( 1 j * k * distance * ( 1 - ( FX * wavelength ) ** 2 - ( FY * wavelength ) ** 2 ) ** 0.5 ) U1 = np . fft . fftshift ( np . fft . fft2 ( np . fft . fftshift ( field ))) U2 = H * U1 result = np . fft . ifftshift ( np . fft . ifft2 ( np . fft . ifftshift ( U2 ))) return result","title":"odak.wave.transfer_function_fresnel"},{"location":"odak/wave/transfer_function_kernel/#see-also","text":"Computer Generated-Holography","title":"See also"},{"location":"odak/wave/wavenumber/","text":"odak.wave.wavenumber \u00b6 Definition for calculating the wavenumber of a plane wave. Parameters: Name Type Description Default wavelength float Wavelength of a wave in mm. required Returns: Type Description float Wave number for a given wavelength. Source code in odak/wave/__init__.py def wavenumber ( wavelength ): \"\"\" Definition for calculating the wavenumber of a plane wave. Parameters ---------- wavelength : float Wavelength of a wave in mm. Returns ------- k : float Wave number for a given wavelength. \"\"\" k = 2 * np . pi / wavelength return k See also \u00b6 Computer Generated-Holography","title":"odak.wave.wavenumber"},{"location":"odak/wave/wavenumber/#odakwavewavenumber","text":"Definition for calculating the wavenumber of a plane wave. Parameters: Name Type Description Default wavelength float Wavelength of a wave in mm. required Returns: Type Description float Wave number for a given wavelength. Source code in odak/wave/__init__.py def wavenumber ( wavelength ): \"\"\" Definition for calculating the wavenumber of a plane wave. Parameters ---------- wavelength : float Wavelength of a wave in mm. Returns ------- k : float Wave number for a given wavelength. \"\"\" k = 2 * np . pi / wavelength return k","title":"odak.wave.wavenumber"},{"location":"odak/wave/wavenumber/#see-also","text":"Computer Generated-Holography","title":"See also"}]}